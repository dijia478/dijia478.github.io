<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[SpringBoot的使用]]></title>
    <url>%2Fblog%2F2018%2F10%2F06%2F2018-10-06-SpringBoot%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[来新公司半年多了，在这里第一次接触到Spring Boot，期间从各种网上资料，同事的指教，还有书籍中，积累了一些使用的心得，后面陆陆续续会整理一下，温故而知新。（我每次都会使用最新的Spring Boot稳定版作为演示，目前是2.0.5） 在刚开始学习Spring这个框架的时候，印象最深刻的就是其繁杂的配置。而Spring Boot最大的特点呢，就是自动配置。Spring Boot让我们的Spring应用变的更轻量化。比如：你可以仅仅依靠一个Java注解来自动配置一项功能。你也可以打包你的应用为jar，并通过使用java -jar来运行你的Spring Web应用。 快速入门废话不多说，直接上实践，在这里，我来搭建一个基础的Spring Boot项目，并实现简单的Http请求处理，通过这个例子对Spring Boot有一个初步的了解，并体验其结构简单、开发快速的特性。 构建项目你可以使用IDEA自带的Spring Boot模板创建，但我这里推荐使用Spring官方提供的快速构建方式 打开https://start.spring.io/ 在里面选择你要构建的项目信息（maven项目，使用java语言，Spring Boot版本是2.0.5） 填写项目名称（cn.dijia478.hello） 选择需要的依赖（web） 创建 基本就是这个样子： 会自动下载下来一个项目包，解压，导入到IDEA里，静静的等maven把Spring Boot的相关依赖下载好，第一次构建会有些慢 其实到这里，项目，已经构建好了，并且可以运行 通过上面步骤完成了Spring Boot基础项目的创建，如上图所示，Spring Boot的基础结构共三个文件（具体路径根据用户生成项目时填写的Group所有差异）： src/main/java下的程序入口：HelloApplication src/main/resources下的配置文件：application.properties src/test/下的测试入口：HelloApplicationTests 是不是很不可思议！！一个Spring项目，居然1分钟就搭建好了！！！ 引入web模块这一步，其实在上面构建Spring Boot的时候，已经在第4步勾选过了，生成的项目里，自动就会引入web模块的依赖。上面第4部没有勾选的，在这里手动添加一下 这下，所有的web相关的配置，Sping Boot已经全部自动配置好了！！哪里还用管什么web.xml的配置啊，通通不需要了。神奇不？ 编写接口创建目录cn.dijia478.hello.controller 这里说一下，为了spring boot的自动扫描注解，请将HelloApplication启动类，放置在所有类的最顶层目录。也就是说，你创建所有的类，都请在HelloApplication启动类的同级目录或子目录下创建。否则你需要在启动类上使用@ComponentScan()声明注解扫描路径。 在该目录下创建类HelloWorld 123456789@RestControllerpublic class HelloWorld &#123; @RequestMapping("/hello") public String hello() &#123; return "Hello World"; &#125;&#125; 启动主程序，打开浏览器访问http://localhost:8080/hello，就可以看到页面输出Hello World 至此已通过Maven构建了一个空白的Spring Boot项目，再通过引入web模块实现了一个简单的请求处理。是不是很省事，全程没有一个配置，就构建好了一个Spring框架下的web项目。]]></content>
      <categories>
        <category>Spring Boot</category>
      </categories>
      <tags>
        <tag>Spring Boot</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL8.0安装步骤]]></title>
    <url>%2Fblog%2F2018%2F09%2F20%2F2018-09-20-MySQL8-0%E5%AE%89%E8%A3%85%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[半年没有更新了，因为谈了个恋爱吧，女友把电脑借走了，又不想在公司电脑更新博客。现在分手后就比较闲啦。在新公司里学了好多新知识，慢慢的会把新学的一些知识都整理一番。 这次先说说mysql，话说从5.7，直接升8.0，不用惊讶，其实5.6版本就相当于6.0版本，5.7就相当于7.0版本。Oracle 公司这次没有用5.8，而是直接命名为了8.0，所以以为自己没用过6.0和7.0的同学们不用介意啦。下面直接说正题，如何在windows上安装mysql8.0 一、下载MySQL这个没啥好说的了吧，去官网下好，下下来解压 二、设置环境变量不知道怎么设置环境变量的话，去百度吧。我是win10的系统，设置好之后是这样的 三、在解压目录下，添加my.ini文件12345678910111213141516[mysql]#设置mysql客户端默认字符集default-character-set=utf8[mysqld]#设置3306端口port = 3306 #设置mysql的安装目录basedir=D:\develop\mysql-8.0.12-winx64#设置mysql数据库的数据的存放目录datadir=D:\develop\mysql-8.0.12-winx64\data#允许最大连接数max_connections=200#服务端使用的字符集默认为8比特编码的latin1字符集character-set-server=utf8#创建新表时将使用的默认存储引擎default-storage-engine=INNODB 四、数据库初始化按键盘的win+R，输入cmd，打开命令窗口 切换到刚才解压mysql的bin目录下（别问我怎么切换。。） 输入命令，自动生成root用户，这个地方会有点慢，大概两三分钟 1mysqld --initialize-insecure 五、安装还在上面的目录下安装服务： 1mysqld -install 报错了，是因为没有用管理员权限启动cmd命令窗口 重新用管理员身份打开cmd，运行安装命令（怎么用管理员什么打开？自行百度） 启动服务： 1net start mysql 连接mysql 1mysql -u root OK！到这里就已经启动好了 停止服务 1net stop mysql 注意一下，如果需要卸载服务，重新安装，需要将mysql解压目录里的data文件夹里的东西清空 然后输入 1mysqld -remove 六、更换密码刚才安装完，进入mysql是不需要密码的，实际中肯定是不行的，现在要登录mysql后设置密码 1ALTER USER 'root'@'localhost' IDENTIFIED WITH mysql_native_password BY '123456'; #更新一下用户的密码为123456 以后登录就需要输入密码了]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[session共享问题-下篇]]></title>
    <url>%2Fblog%2F2018%2F03%2F18%2F2018-03-18-session%E5%85%B1%E4%BA%AB%E9%97%AE%E9%A2%98-%E4%B8%8B%E7%AF%87%2F</url>
    <content type="text"><![CDATA[过了个年，换了份工作，来到一家通讯行业的公司，之前的技术感觉好多都有点用不上了，需要学习很多新的东西，而且还有了女朋友，所以最近跟新可能会很慢，大家见谅。现在继续更新年前写的那篇博客。本文是第二篇。 由于nginx是随机分配请求，假设一个用户登录时访问网站登录时被分配到192.168.25.128:8080上，然后进行了登录操作，此时该服务器上就会有该用户登录的session信息，然后登陆后重定向到网站首页或个人中心时，此时如果被分配到192.168.25.128:8081上，那么这台服务器上没有该用户session信息，于是又会变成未登录状态，所以由于nginx的负载均衡会导致session共享的问题。 解决方法：1.nginx提供了ip_hash的分配策略（还有轮询策略和权重策略），可以保持用户ip进行hash值计算固定分配到某台服务器上，然后只要是该ip则会保持分配到该服务器上，保证用户访问的是同一台服务器，那么session问题就不存在了。这也是解决session共享的一种方式，也称为黏性session。但是假设一台tomcat服务器挂了的话，那么session也会丢失。所以比较好的方案是抽取session。 2.将session存到memcache或者redis中，以这种方式来同步session，把session抽取出来，放到内存级数据库里面，解决了session共享问题，同时读取速度也是非常之快。 Redis解决session共享：在redis服务器192.168.25.128上搭建redis（我这里还是在同一台上搭建），redis默认端口为6379 redis的部署步骤我这里就不详细讲了，可以去看我之前写的《Redis在Linux上的部署和Jedis简单使用》 不过我之前写的那个，没有指定配置文件启动，会采用redis默认的配置文件。这个配置文件其实是可以指定的，启动： 1./redis-server ./redis.conf 这种启动方式叫做前端启动，必须保持在当前窗口，如果ctrl + c 退出，那么redis也就退出了，实际工作中不建议使用 那么后端启动： 首先修改redis的配置文件redis.conf中daemonize的值，打开可以看到默认是no，修改为daemonize yes，启动即可。也可以在该配置文件中修改redis默认端口6379为其他值。 关闭redis： 1./redis-cli shutdown 至此，redis服务器搭建完成。 tomcat与redis集成实现session共享： 在所有需要共享session的服务器的tomcat的lib目录中添加如下ar包： 这些jar包，主要起作用的是第4个，tomcat-cluster-redis-session-manager-2.0.4。 这些jar包都可以在这里下载到，下载zip压缩包的那个，里面会有需要的所有依赖和文件（一共需要5个）： https://github.com/ran-jit/tomcat-cluster-redis-session-manager/wiki 接下来，在所有需要共享session的服务器的tomcat的conf目录下，添加redis-data-cache.properties文件，这个文件也在刚才下的压缩包里有。 修改每个redis-data-cache.properties文件如下： 这个应该很好看懂吧，我只用了一台redis，所以就配一个地址，如果有redis集群，那就配多个。redis如果设置了密码，下面第二个参数把密码也要设好。 然后，需要在两个tomcat/conf/context.xml文件中增加以下两行： 12&lt;Valve className="tomcat.request.session.redis.SessionHandlerValve" /&gt;&lt;Manager className="tomcat.request.session.redis.SessionManager" /&gt; 设置tomcat/conf/web.xml中，session有效期（这个不配的话，也是默认有的，30分钟，可以省略此步骤） 123&lt;session-config&gt; &lt;session-timeout&gt;60&lt;session-timeout&gt;&lt;session-config&gt; 如果项目里也配置了session有效期，则以项目中为准。 先启动redis服务，再重新启动所有tomcat，再启动nginx，刷新nginx页面,两台tomcat页面可以看到sessionid值不变，关闭某台tomcat，nginx中sessionid不变，说明session是共享的。 问题： 有可能此时访问会报错，redis无法访问，这是由于redis的安全机制，默认只有127.0.0.1才能访问，在redis.conf中可以找到bind 127.0.0.1，你可以将此ip改为访问者ip， 如果有多个访问者，也可以把bind 127.0.0.1注释掉，然后在配置文件中找到protected-mode，修改protected-mode yes改为protected-mode no 关闭redis保护模式即可 注意：按照如上配置，使用redis数据库，放入session中的对象必须要实现java.io.Serializable接口，而使用memcache实现session共享的可以不用实现Serializable接口 原因是：因为tomcat里使用的将session放置redis使用的工具类：是使用的jdk序列化模式存储的，这一点也是很容易理解的，session.setAttribute(String key, Object value)，存储Object类型 object放入redis中又要能取出来，只能是序列化进行存储了，然后取出的时候进行反序列化。 所以我们在session中存储的任何对象，都必须实现序列化接口。]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>session共享</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx反向代理和负载均衡实现-上篇]]></title>
    <url>%2Fblog%2F2018%2F02%2F12%2F2018-02-12-Nginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E5%92%8C%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%E5%AE%9E%E7%8E%B0-%E4%B8%8A%E7%AF%87%2F</url>
    <content type="text"><![CDATA[准备纯干货，分上、下，两篇来写。总结一下Nginx反向代理和负载均衡实现（上篇），redis解决session共享（下篇），但只是停留在使用的层面上，没有深入原理。使用的资源： nginx主服务器一台，nginx备服务器一台，使用keepalived进行宕机切换，实现高可用。 tomcat服务器两台，由nginx进行反向代理和负载均衡，此处可搭建服务器集群。 redis服务器一台，用于session的分离共享。 因为我的小电脑配置不高，没法开那么多台服务器，所以就全部安装在同一个服务器（192.168.25.128）里了啊，通过不同的端口来区分。实际应用中应该是安装在5台服务器里，ip地址不同。 注意访问时需要配置防火墙规则，或者关闭防火墙 环境： VMware12 Pro CentOS-6.7-i386 jdk-8u151-linux-i586 nginx-1.12.2 apache-tomcat-8.5.24 redis-4.0.8 架构图 此时需要用到三台服务器，一台nginx服务器，两台正式部署项目的服务器 因为条件限制，我就用一台服务器（192.168.25.128），来部署三个服务了。 一、安装tomcat这个简单，不多说，上传解压即可使用，tomcat/bin目录下 startup.sh启动，shutdown.sh关闭 复制两份出来，放在/usr/local/nginx下 在tomcat/conf/server.xml中，修改端口，这里演示tomcat01里的 用/port命令搜索port字符串，按n搜索下一个，有三个地方要改，原本是8005,8080,8009，依次更改为 tomcat02里也是修改这三个位置，分别修改为8205,8280,8209。 修改端口是因为我这里是在同一台服务器上部署了两个tomcat。如果按照原本的架构，是在两台服务器部署，ip不同，那么是不用修改端口的，可省略上面修改端口这步。 二、修改防火墙规则配置防火墙拦截规则：vim /etc/sysconfig/iptables 开放8180和8280端口，80端口等一些常用端口，当然后边有用到一些端口都是需要配置开放的，不建议关闭防火墙 编辑好后 service iptables restart 命令，重新加载防火墙配置 如果是自己测试嫌配置麻烦，可以关闭防火墙： service iptables stop 此命令在重启服务器后，防火墙还会打开，即在此次开机状态下有效。完全关闭防火墙： chkconfig iptables off ，即使重启服务器后防火墙也不会打开。 注意有时候服务都起了，但访问就是出错，可能就是防火墙问题 启动tomcat访问：192.168.25.128:8180，192.168.25.128:8280，打开tomcat首页即成功。 三、部署项目编写测试项目，IDEA新建maven项目，项目名为testproject，在webapp目录下新建一个jsp页面为index.jsp,添加如下内容 1234567891011&lt;%@ page contentType="text/html;charset=UTF-8" language="java" %&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Title&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;h1&gt;测试页面&lt;h1/&gt; &lt;br&gt; &lt;h1&gt;SessionId:&lt;%=session.getId() %&gt;&lt;/h1&gt;&lt;/body&gt;&lt;/html&gt; 将项目中web.xml中的访问顺序&lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt;上移到第一个访问 123456789101112131415&lt;?xml version="1.0" encoding="UTF-8"?&gt;&lt;web-app xmlns="http://xmlns.jcp.org/xml/ns/javaee"xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"xsi:schemaLocation="http://xmlns.jcp.org/xml/ns/javaee http://xmlns.jcp.org/xml/ns/javaee/web-app_3_1.xsd"version="3.1"&gt;&lt;display-name&gt;testproject&lt;/display-name&gt;&lt;welcome-file-list&gt; &lt;welcome-file&gt;index.jsp&lt;/welcome-file&gt; &lt;welcome-file&gt;index.html&lt;/welcome-file&gt; &lt;welcome-file&gt;index.htm&lt;/welcome-file&gt; &lt;welcome-file&gt;default.html&lt;/welcome-file&gt; &lt;welcome-file&gt;default.htm&lt;/welcome-file&gt; &lt;welcome-file&gt;default.jsp&lt;/welcome-file&gt;&lt;/welcome-file-list&gt;&lt;/web-app&gt; 在pom.xml中添加&lt;packaging&gt;war&lt;/packaging&gt;标签，然后利用IDEA最右侧maven工具将项目打成war包，testproject.war 将该war包上传到两台服务器的tomcat的webapps中 然后修改tomcat的server.xml文件，在tomcat/conf目录中 我这次使用的是notepad++的插件NppFTP直接连上的linux，然后使用notepad++修改文件的，修改后保存记得使用UTF-8无BOM格式 修改Engine标签，添加jvmRoute属性，用于标识nginx访问的是哪个服务器tomcat，tomcat01服务器标识为8180Server1，tomcat02服务器标识为8280Server2。我这里只演示tomcat01 设置tomcat的虚拟目录，在两台tomcat的server.xml文件，Host标签中添加：&lt;Context path=&quot;&quot; docBase=&quot;testproject&quot;/&gt;，path表示访问路径，为空的话访问地址就不用输入项目名，docBase为项目路径，我这是相对路径。这里还是只演示tomcat01 此时，重新启动两个tomcat，访问两个项目，两台服务器访问显示如下： 至此，两台tomcat服务搭建完成。 四、安装Nginx先使用yum命令安装gcc的环境，然后安装pcre，zlib，openssl第三方的开发包，依次执行命令： 1234yum install gcc-c++yum install -y pcre pcre-develyum install -y zlib zlib-develyum install -y openssl openssl-devel 上传nginx-1.12.2.tar.gz，解压 1tar zxf nginx-1.12.2.tar.gz 进入解压后的目录，依次执行命令： 123./configuremakemkae install 此时nginx安装完毕，默认安装目录是/usr/local/nginx，nginx默认占用80端口 哎呀，和之前的两个tomcat放在同一目录下了。。。算了，不影响啥，就放这吧。那两个脚本文件是我用来启动和关闭两个tomcat的，懒得进目录去一个一个打开关闭。之前的步骤里没说这两个脚本。其实在上面输入./configure 命令时，可以给后面加上--prefix=安装路径 来指定安装位置的。 继续说Nginx，其中，sbin目录为nginx执行命令，conf目录下的nginx.conf为默认加载的配置文件 启动nginx： 1./sbin/nginx 关闭nginx： 1./sbin/nginx -s quit 重启nginx： 123先关闭后启动。刷新配置文件：./sbin/nginx -s reload 启动nginx后访问192.168.25.128:80即可访问nginx：显示nginx欢迎页（注意防火墙是否添加80端口） 至此，nginx安装完毕。 五、nginx反向代理现有两台tomcat服务器，一台为192.168.25.128:8180，一台为192.168.25.128:8280。有一台Nginx在192.168.25.128:80。 现在要配置nginx，当访问192.168.25.128:80时，即可访问192.168.25.128:8180，192.168.25.128:8280中随机一台。此时192.168.25.128:80被nginx监听，当有请求时，代理到192.168.25.128:8180，192.168.25.128:8280随机一台即可，即为nginx反向代理功能。 同时，此时可以通过nginx将请求进行转发，保证了一个入口，将所有请求转发到两台服务器上，也减轻了任何一台的负载压力，当有大量请求时，可以搭建大量服务器，在入口代理服务器上使用nginx进行转发，即是负载均衡功能。、 配置即是配置nginx安装目录中conf目录下的nginx.conf文件即可：具体配置如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101#Nginx所用用户和组#user niumd niumd;#工作的子进程数量（通常等于CPU数量或者2倍于CPU）worker_processes 2;#错误日志存放路径#error_log logs/error.log;#error_log logs/error.log notice;error_log logs/error.log info;#指定pid存放文件pid logs/nginx.pid;events &#123; #使用网络IO模型linux建议epoll，FreeBSD建议采用kqueue #use epoll; #允许最大连接数 worker_connections 1024;&#125;http &#123; include mime.types; default_type application/octet-stream; #定义日志格式 #log_format main '$remote_addr - $remote_user [$time_local] $request ' # '"$status" $body_bytes_sent "$http_referer" ' # '"$http_user_agent" "$http_x_forwarded_for"'; #access_log off; access_log logs/access.log; client_header_timeout 3m; client_body_timeout 3m; send_timeout 3m; client_header_buffer_size 1k; large_client_header_buffers 4 4k; sendfile on; tcp_nopush on; tcp_nodelay on; #fastcgi_intercept_errors on; error_page 404 /404.html; #keepalive_timeout 75 20; gzip on; gzip_min_length 1000; gzip_types text/plain text/css application/x-javascript; #配置被代理的服务器，blank这个名称可以改 upstream blank &#123; #ip_hash; server 192.168.25.128:8180; server 192.168.25.139:8280; &#125; #一个server节点就是一个虚拟主机，可以配置多个server，来配置多个虚拟主机 server &#123; #nginx监听80端口，请求该端口时转发到真实目标 listen 80; #配置访问域名 server_name localhost; location / &#123; #这里配置代理是指上面定义的两个被代理目标，blank名字必须一致 proxy_pass http://blank; #proxy_redirect off; #如果是非80端口，配置为Host $host：端口号，目的是将代理服务器收到的用户的信息传到真实服务器上 proxy_set_header Host $host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; client_max_body_size 10m; client_body_buffer_size 128k; proxy_connect_timeout 300; proxy_send_timeout 300; proxy_read_timeout 300; proxy_buffer_size 4k; proxy_buffers 4 32k; proxy_busy_buffers_size 64k; proxy_temp_file_write_size 64k; add_header Access-Control-Allow-Origin *; &#125; #此处定义500 502 503 504的错误页面 error_page 500 502 503 504 /50x.html; #错误页面位置 location = /50x.html &#123; #root表示路径 html为nginx安装目录中的html文件夹 #位于/usr/local/nginx/html/下 root html; &#125; &#125;&#125; 重点是红色部分 还有注意一个小问题，我这里的server节点，是我复制配置文件里原有的，并不是修改原有的。在文件下面还有个server节点，它的监听端口原本是80，因为我复制的server节点要监听这个端口，所以我将配置文件里原有的server节点改成了监听81端口。不然会冲突。 启动两台tomcat，重新启动nginx，刷新配置文件。 访问192.168.25.128:80将会访问192.168.25.128:8180，192.168.25.128:8280中随机一台。 注意：现在每次刷新nginx服务器地址，SessionId会变，存在session不能共享的问题。这个问题会留到我的下一篇博客《redis解决session共享（中篇）》中去解决 至此，nginx的反向代理功能已经实现。session共享问题放到下一篇博客去解决。 六、 nginx负载均衡nginx负载均衡到多台服务器上时，默认采用轮询策略 常见策略： 1、轮询 每个请求按时间顺序逐一分配到不同的后端服务器，如果后端服务器down掉，能自动剔除。 2、weight指定轮询几率，weight和访问比率成正比，用于后端服务器性能不均的情况，数字越大命中率越高。例如：轮询几率是2:1，只需在nginx.conf中这么配置： 1234upstream blank &#123; server 192.168.25.128:8180 weight=2; server 192.168.25.128:8280 weight=1;&#125; 3、ip_hash每个请求按访问ip的hash结果分配，这样每个访客固定访问一个后端服务器，可以解决session的问题。例如： 12345upstream blank &#123; ip_hash; server 192.168.25.128:8180; server 192.168.25.128:8280;&#125; 其他策略可以自行查询学习，nginx还有很多其他可配置项，静态资源缓存，重定向等，想深入的童鞋请自行学习 nginx配置详解：http://blog.csdn.net/tjcyjd/article/details/50695922 《Nginx反向代理和负载均衡实现（上篇）》就先写到这里，过几天会再整理出来《redis解决session共享（中篇）》。快过年了，祝大家新年快乐！]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
        <tag>反向代理</tag>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Tomcat在Linux服务器上的BIO、NIO、APR模式设置]]></title>
    <url>%2Fblog%2F2018%2F01%2F04%2F2018-01-04-Tomcat%E5%9C%A8Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84BIO%E3%80%81NIO%E3%80%81APR%E6%A8%A1%E5%BC%8F%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[一、BIO、NIO、AIO先了解四个概念同步： 自己亲自出马持银行卡到银行取钱（使用同步IO时，Java自己处理IO读写）。 异步： 委托一小弟拿银行卡到银行取钱，然后给你（使用异步IO时，Java将IO读写委托给OS处理，需要将数据缓冲区地址和大小传给OS(银行卡和密码)，OS需要支持异步IO操作API）。阻塞： ATM排队取款，你只能等待（使用阻塞IO时，Java调用会一直阻塞到读写完成才返回）。 非阻塞：柜台取款，取个号，然后坐在椅子上做其它事，等号广播会通知你办理，没到号你就不能去，你可以不断问大堂经理排到了没有，大堂经理如果说还没到你就不能去（使用非阻塞IO时，如果不能读写Java调用会马上返回，当IO事件分发器会通知可读写时再继续进行读写，不断循环直到读写完成）。 Java对BIO、NIO、AIO的支持Java BIO： 同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。 Java NIO： 同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。 Java AIO(NIO.2)： 异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理 BIO、NIO、AIO适用场景分析BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。 NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。 AIO方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。 二、tomcat三种模式简介BIOBIO(blocking I/O)，顾名思义，即阻塞式I/O操作，表示Tomcat使用的是传统的Java I/O操作(即java.io包及其子包)。Tomcat在默认情况下，就是以bio模式运行的。遗憾的是，就一般而言，bio模式是三种运行模式中性能最低的一种。我们可以通过Tomcat Manager来查看服务器的当前状态。 NIO是Java SE 1.4及后续版本提供的一种新的I/O操作方式(即java.nio包及其子包)。Java nio是一个基于缓冲区、并能提供非阻塞I/O操作的Java API，因此nio也被看成是non-blocking I/O的缩写。它拥有比传统I/O操作(bio)更好的并发运行性能。 APR(Apache Portable Runtime/Apache可移植运行库)，是Apache HTTP服务器的支持库。你可以简单地理解为，Tomcat将以JNI的形式调用Apache HTTP服务器的核心动态链接库来处理文件读取或网络传输操作，从而大大地提高Tomcat对静态文件的处理性能。 Tomcat apr也是在Tomcat上运行高并发应用的首选模式。 三、tomcat三种模式性能比较这里我引用了网友给出的测试结果 四、tomcat模式设置我这里演示的是tomcat7，默认是BIO模式的。而tomcat8是默认NIO模式的。 BIO模式tomcat7默认就是。如果你是tomcat8或9想设置成BIO模式的，那么在tomcat目录里的conf目录里的server.xml文件中修改。找到设置端口号8080的那个标签，主要是修改protocol属性为HTTP/1.1，重启tomcat就会使用BIO模式。tomcat7默认就是这个样子的 123&lt;Connector port="8080" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt; NIO模式在和设置BIO模式同样的位置上，修改protocol属性为org.apache.coyote.http11.Http11NioProtocol，重启tomcat就会使用NIO模式。tomcat8以上默认就是这个样子的 123&lt;Connector port="8080" protocol="org.apache.coyote.http11.Http11NioProtocol" connectionTimeout="20000" redirectPort="8443" /&gt; APR模式启用这种模式稍微麻烦一些，除了需要改配置文件，还需要安装一些依赖库，以下就是安装所需的条件： 1. 最新的apr 2. 最新的apr-util 3. tomcat-native.tar.gz(在tomcat/bin/下有相应的安装tar包) 前两个依赖库的官方下载地址：http://apr.apache.org/download.cgi 先改配置文件，和改BIO,NIO模式的位置一样，修改protocol属性为org.apache.coyote.http11.Http11AprProtocol，注意和NIO的很像，但不一样 123&lt;Connector port="8080" protocol="org.apache.coyote.http11.Http11AprProtocol" connectionTimeout="20000" redirectPort="8443" /&gt; 现在先安装apr，在解压好的apr目录下，执行下面的命令，指定apr安装目录： 1./configure --prefix=/usr/local/apr &amp;&amp; make &amp;&amp; make install 再安装apr-util，在解压好的apr-util目录下，执行下面的命令，指定apr目录和apr-util安装目录： 1./configure --with-apr=/usr/local/apr/ --prefix=/usr/local/apr-util &amp;&amp; make &amp;&amp; make install 再安装tomcat-native，这个不用下载，在tomcat/bin/下有相应的安装tar包，在解压好的tomcat-native目录下，执行下面的命令，指定指定apr目录和JAVA_HOME目录： 1./native/configure --with-apr=/usr/local/apr --with-java-home=/usr/share/jdk1.8 &amp;&amp; make &amp;&amp; make install 安装完后记得在 /etc/profile 文件中的JAVA_HOME环境变量后面多加一条APR的环境变量（注意你自己安装的apr目录） 1export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/apr/lib 用source /etc/profile命令让环境变量配置立即生效，启动tomcat，就是APR模式了 五、如何确定自己当前的模式启动tomcat后，可以在tomcat/logs目录下，执行如下命令： 1tail -f catalina.out 我这里就是成功启动APR模式后显示的样子]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat设置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solr集群在Linux上搭建详细步骤]]></title>
    <url>%2Fblog%2F2017%2F12%2F24%2F2017-12-24-Solr%E9%9B%86%E7%BE%A4%E5%9C%A8Linux%E4%B8%8A%E6%90%AD%E5%BB%BA%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[一、Solr集群的系统架构SolrCloud(solr 云)是Solr提供的分布式搜索方案，当你需要大规模，容错，分布式索引和检索能力时使用 SolrCloud。当一个系统的索引数据量少的时候是不需要使用SolrCloud的，当索引量很大，搜索请求并发很高，这时需要使用SolrCloud来满足这些需求。 SolrCloud是基于Solr和Zookeeper的分布式搜索方案，它的主要思想是使用Zookeeper作为集群的配置信息中心。它有几个特色功能： 1）集中式的配置信息 2）自动容错 3）近实时搜索 4）查询时自动负载均衡 1.物理结构三个Solr实例（ 每个实例包括两个Core），组成一个SolrCloud。 2.逻辑结构索引集合包括两个Shard（shard1和shard2），shard1和shard2分别由三个Core组成，其中一个Leader两个Replication，Leader是由zookeeper选举产生，zookeeper控制每个shard上三个Core的索引数据一致，解决高可用问题。 用户发起索引请求分别从shard1和shard2上获取，解决高并发问题。 2.1. collectionCollection在SolrCloud集群中是一个逻辑意义上的完整的索引结构。它常常被划分为一个或多个Shard（分片），它们使用相同的配置信息。 比如：针对商品信息搜索可以创建一个collection。 collection=shard1+shard2+….+shardX 2.2. Core每个Core是Solr中一个独立运行单位，提供 索引和搜索服务。一个shard需要由一个Core或多个Core组成。由于collection由多个shard组成所以collection一般由多个core组成。 2.3. Master或SlaveMaster是master-slave结构中的主结点（通常说主服务器），Slave是master-slave结构中的从结点（通常说从服务器或备服务器）。同一个Shard下master和slave存储的数据是一致的，这是为了达到高可用目的。 2.4. ShardCollection的逻辑分片。每个Shard被化成一个或者多个replication，通过选举确定哪个是Leader。 3.本次演示实现的solr集群架构 Zookeeper作为集群的管理工具。 1、集群管理：容错、负载均衡。 2、配置文件的集中管理 3、集群的入口 需要实现zookeeper 高可用。需要搭建集群。建议是奇数节点。需要三个zookeeper服务器。 搭建solr集群至少需要7台服务器。 这里因环境限制，演示的是搭建伪分布式（在一台虚拟机上，建议内存至少1G）： 需要三个zookeeper节点 需要四个tomcat节点。 本文使用tomcat进行部署，而不使用solr自带的jetty 4.系统环境CentOS-6.7-i386-bin-DVD1 jdk-8u151-linux-i586 apache-tomcat-8.5.24 zookeeper-3.4.10 solr-7.1.0 注意：solr6.0以上版本，官方建议使用jdk8，tomcat8，搭建集群步骤和solr6以下略微有区别。搭建solr集群前，要先关闭iptables防火墙服务 二、 先搭建Zookeeper集群第一步：上传，解压zookeeper12rztar zxf zookeeper-3.4.10.tar.gz 第二步：在zookeeper目录下创建一个data目录1mkdir ./zookeeper-3.4.10/data 第三步：把zookeeper目录下的conf目录下的zoo_sample.cfg文件改名为zoo.cfg12cd ./zookeeper-3.4.10/confmv zoo_sample.cfg zoo.cfg 第四步：把zookeeper目录复制三份先创建目录/usr/local/solr-cloud，这里就是后面集群放置的目录了 1234mkdir /usr/local/solr-cloudcp -r ./zookeeper-3.4.10 /usr/local/solr-cloud/zookeeper01cp -r ./zookeeper-3.4.10 /usr/local/solr-cloud/zookeeper02cp -r ./zookeeper-3.4.10 /usr/local/solr-cloud/zookeeper03 第五步：创建三个myid文件在第四步复制的三个zookeeper目录里的data目录下，分别创建一个myid文件，文件名都叫做“myid”。内容就是每个实例的id。例如1、2、3 123echo 1 &gt;&gt; ./zookeeper01/data/myidecho 2 &gt;&gt; ./zookeeper02/data/myidecho 3 &gt;&gt; ./zookeeper03/data/myid 第六步：修改三个zoo.cfg配置文件就是上面第三步改的文件，第四步复制了三份。这里只演示一个，但三份都要改。前两个红色框框里的（目录和端口号）内容三份配置文件是不一样的，目录就是各自对应的目录，我的端口号分别改为了2181，2182，2183。而最后那个红框里的内容三份配置文件是一样的。 server.1的这个1，就是第五步myid文件的内容。在实际工作中每个zookeeper实例在不同的服务器上，所以后面的ip应该是不同的，我这里是在一台虚拟机上演示，所以ip相同。 第七步：启动每个zookeeper实例这里一个个进目录里启动实在麻烦，我这里在solr-cloud目录下写个脚本来执行 1vi start-zookeeper-all.sh 脚本内容： 123456cd /usr/local/solr-cloud/zookeeper01/bin/./zkServer.sh startcd /usr/local/solr-cloud/zookeeper02/bin/./zkServer.sh startcd /usr/local/solr-cloud/zookeeper03/bin/./zkServer.sh start 写完后保存退出，发现脚本没有执行权限，添加权限： 1chmod u+x start-zookeeper-all.sh 执行脚本： 为了验证，去三个zookeeper的实例里bin目录下分别查看每个实例的状态 （我刚开始是把查看状态的命令写在脚本里的，让它一启动就查看，可每次状态都显示not running，后来想了想，应该是因为脚本执行太快，启动命令执行了但还没启动起来，就去查看状态，所以会显示没有运行） 如果你显示的是这样子的一个领导两个部下（leader和follower不一定是谁，随机的），那么就代表zookeeper集群已经搭建完成 zookeeper集群搭建完成了，下来搭建solr集群 三、solr集群搭建注意，在搭建solr集群前，建议最好有一个solr服务是已经搭建好的，可以简化大量重复的配置操作。 单机solr服务搭建过程参看我的这篇文章： Solr服务在Linux上搭建详细步骤 这个单机solr服务在solr集群搭建第二步和第三步里需要，搭建好一个solr服务后，就可以进行集群搭建了。 第一步：创建四个tomcat实例。每个tomcat运行在不同的端口。8180、8280、8380、8480在搭建单机solr服务第二步的时候，tomcat8已经上传解压好了，所以直接复制用就行，复制到搭建zookeeper集群第四步时创建的/usr/local/solr-cloud/目录下。 1234cp -r apache-tomcat-8.5.24 /usr/local/solr-cloud/tomcat01cp -r apache-tomcat-8.5.24 /usr/local/solr-cloud/tomcat02cp -r apache-tomcat-8.5.24 /usr/local/solr-cloud/tomcat03cp -r apache-tomcat-8.5.24 /usr/local/solr-cloud/tomcat04 在复制过来的四个tomcat目录下的conf里的server.xml，修改每个tomcat的端口号，这里只演示第一个目录的，另外三个都要改，端口要互不冲突 1vi ./tomcat01/conf/server.xml 用/port命令搜索port字符串，按n搜索下一个，有三个地方要改，依次更改为 其他三个tomcat目录里的server.xml配置的端口按照234的顺序全改了啊，这里我就不截图了。 一共要改四个文件，每个文件里改三个地方，这12个端口号要互不冲突。 123vi ./tomcat02/conf/server.xmlvi ./tomcat03/conf/server.xmlvi ./tomcat04/conf/server.xml 第二步：把单机版的solr工程复制到集群中的tomcat里在搭建好的单机solr服务里（这里是另一篇教程，上面说过了），复制solr工程到第一步的4个tomcat目录里，一共是复制4份 1234cp -r /usr/local/solr/tomcat/webapps/solr /usr/local/solr-cloud/tomcat01/webapps/cp -r /usr/local/solr/tomcat/webapps/solr /usr/local/solr-cloud/tomcat02/webapps/cp -r /usr/local/solr/tomcat/webapps/solr /usr/local/solr-cloud/tomcat03/webapps/cp -r /usr/local/solr/tomcat/webapps/solr /usr/local/solr-cloud/tomcat04/webapps/ 第三步：为每个solr实例创建一个对应的solrhome一样在之前搭建的单机solr服务里，把solrhome复制4份出来到solr-cloud目录里 1234cp -r /usr/local/solr/solrhome /usr/local/solr-cloud/solrhome01cp -r /usr/local/solr/solrhome /usr/local/solr-cloud/solrhome02cp -r /usr/local/solr/solrhome /usr/local/solr-cloud/solrhome03cp -r /usr/local/solr/solrhome /usr/local/solr-cloud/solrhome04 现在solr-cloud目录里是有这些目录，检查一下有没有复制错地方的： 123456789101112solrhome01solrhome02solrhome03solrhome04start-zookeeper-all.shtomcat01tomcat02tomcat03tomcat04zookeeper01zookeeper02zookeeper03 第四步：配置solrCloud相关的配置第三步复制好的四个solrhome下，都有一个solr.xml，把其中的ip及端口号配置好。搭建单机版solr服务的时候没有动过solrhome里的这个文件，现在搭建集群了，需要进行修改。需要修改4个solr.xml，我这里还是只演示第一个，另外三个目录里的类比着改，一样的，就是1234的顺序 1vi solrhome01/solr.xml 找到这个地方，第一个红框代表当前节点的ip地址，实际工作中就是会部署4个服务器，一个服务器是一个节点，部署一个solr服务。第二个红框代表当前solr服务实例的端口号，就是所在的tomcat的端口号，就是第一步在tomcat01的server.xml里配置的8180 我的配置完后是这样的： 12&lt;str name="host"&gt;192.168.25.128&lt;/str&gt;&lt;int name="hostPort"&gt;8180&lt;/int&gt; 其他三个solr.xml照着改，由于我是在一台虚拟机搭建的四个solr实例，所以肯定四个实例的ip是一样的，但端口分别是8180，8280，8380，8480 123vi solrhome02/solr.xmlvi solrhome03/solr.xmlvi solrhome04/solr.xml 第五步：修改solr服务的web.xml文件。把solrhome关联起来修改这个文件，和单机版的solr配置是一样的 要注意的是，tomcat01这里的solr服务，是上面第二步复制过来的对吧，是我之前用solr7搭建的单机solr服务，里面的配置和solr4不太一样，具体还是去上面看我提供的单机solr服务搭建教程链接 1vi tomcat01/webapps/solr/WEB-INF/web.xml 找到这个，这里是我之前搭建单机solr服务时，配置的solrhome路径 现在改成集群的solrhome01目录，使他们关联起来 1&lt;env-entry-value&gt;/usr/local/solr-cloud/solrhome01&lt;/env-entry-value&gt; 其他三个tomcat里的web.xml都对应着一改，solrhome02，03，04 123vi tomcat02/webapps/solr/WEB-INF/web.xmlvi tomcat03/webapps/solr/WEB-INF/web.xmlvi tomcat04/webapps/solr/WEB-INF/web.xml 第六步：让zookeeper统一管理配置文件。需要把/conf目录上传到zookeeper现在我们每一个solr都有了自己的solrhome，现在我们要让每一个solr实例的配置文件都一样，这个配置文件需要集中管理，这个时候我们使用zookeeper来统一管理配置文件。所以要将配置文件上传到zookeeper中。那么上传哪些配置文件呢？ 这里注意下managed-schema文件，网上有很多低版本solr，会提到一个collection1/conf下的schema.xml，但是本人并没有找到。好像是从5.0版本开始不使用schema.xml的，这俩其实内容都一样，搞不懂为啥要换个名字，而且内容格式是xml，但是文件名却没有.xml的后辍，这里直接上传整个conf目录就行。 1/usr/local/solr-cloud/solrhome01/configsets/sample_techproducts_configs/conf 知道了要上传什么，那么怎么上传呢？打开最早solr解压出来的原始文件（如果删了那就重新上传解压吧） 在这个目录下有个zkcli.sh脚本文件，执行这个脚本就可以将配置文件上传到zookeeper了，有点难找，我是用find命令给搜出来的。 1/home/dijia478/solr-7.1.0/server/scripts/cloud-scripts 要注意，在执行脚本上传配置文件前，必须先去启动zookeeper集群 12cd /usr/local/solr-cloud/./start-zookeeper-all.sh 然后回来执行脚本，这里脚本的执行命令有点长，主要是参数多，建议复制出来改好后再粘贴上去 12cd /home/dijia478/solr-7.1.0/server/scripts/cloud-scripts./zkcli.sh -zkhost 192.168.25.128:2181,192.168.25.128:2182,192.168.25.128:2183 -cmd upconfig -confdir /usr/local/solr-cloud/solrhome01/configsets/sample_techproducts_configs/conf -confname myconf 我解释下各个参数的含义： 红色框代表zookeeper集群的ip和端口号列表（搭建zookeeper集群的时候配置过的） 绿色框代表要执行的是上传配置文件操作 黄色框代表的是要上传的配置文件目录（低版本不太一样，具体以那两个主要的配置文件所在目录为准，不知到在哪就find命令搜吧） 紫色框代表的是你给上传的配置起的名字，可以改 现在上传完了，那么我们怎么确定是否上传成功呢？去zookeeper集群的一个目录找到bin里zookeeper的客户端脚本 12cd /usr/local/solr-cloud/zookeeper01/bin/./zkCli.sh 运行后，里面会出现一大堆内容，如果你不指定参数，他会默认访问localhost:2181 在最下面执行ls /，查看在根目录下有什么，发现一个configs，再看它里面有什么？ 这就是我们刚才上传的配置了，名字一样的，代表上传成功了 然后用quit命令退出 如果你以后需要修改solr配置的话，只用在刚才那个solrhome01/…/conf目录里改好，改好后再上传一次就行了，就会覆盖原来的配置文件 第七步：修改tomcat/bin目录下的catalina.sh 文件，关联solr和zookeeper现在上传好配置文件了，可是solr和zookeeper还没有建立任何关系，他们也不知道对方在哪里，这个时候需要修改4个tomcat的配置文件，这里只演示tomcat01，其他三个完全相同照着改 12cd tomcat01/binvi catalina.sh 用/JAVA_OPTS搜索红色框框里的这句话（因为低版本加的位置长的不太一样，但这句话的例子是不变的），在这句话下面的位置加（注意位置，不要弄错了）： 在图上位置加上JAVA_OPTS的值（zookeeper集群的ip列表）： 1JAVA_OPTS="-DzkHost=192.168.25.128:2181,192.168.25.128:2182,192.168.25.128:2183" 然后把其他三个tomcat也一改，改的位置和内容是一样的，不用变 123vi tomcat02/bin/catalina.shvi tomcat03/bin/catalina.shvi tomcat04/bin/catalina.sh 这样每个solr实例就通过这个参数和zookeeper集群建立了联系，solr会将自己的状态发送给zookeeper，比如ip地址啊，端口号啊，zookeeper就可以连接到solr了，建立了通信关系 第八步：启动每个tomcat实例。要包装zookeeper集群是启动状态现在需要启动每个tomcat，当然了，这个和启动zookeeper集群一样，要一个一个进去启动，太麻烦了，还是在solr-cloud目录下写个批处理脚本来运行 1vi start-tomcat-all.sh 脚本内容： 1234/usr/local/solr-cloud/tomcat01/bin/startup.sh/usr/local/solr-cloud/tomcat02/bin/startup.sh/usr/local/solr-cloud/tomcat03/bin/startup.sh/usr/local/solr-cloud/tomcat04/bin/startup.sh 保存，退出，还是要为脚本添加执行权限，不然执行不了。添加完后运行脚本，tomcat集群启动有点慢 12chmod u+x start-tomcat-all.sh./start-tomcat-all.sh 如果你想看启动起来没有，可以复制一个SSH渠道（我用的xshell5），去看看tomcat的日志信息（相当于看控制台打印信息），在另一个会话窗口里运行下面的命令： 1tail -f /usr/local/solr-cloud/tomcat01/logs/catalina.out 这里说个小知识点，tomcat8开始，默认启动的是NIO模式，7默认启动的是BIO模式，还可以通过配置设置APR模式启动，至于APR，NIO和BIO的区别，是和tomcat并发性能有关的，高并发的系统应该将tomcat的模式设置成APR模式，会大幅度的提高服务器的处理和响应性能。感兴趣的可以自己百度下。当然这个不用在意，跟本文集群搭建没关系，就是想到了说一下。过几天我再写个配置tomcat三种模式的博客吧。 第九步：访问集群然后用自己的电脑访问下集群，之前单机版是没有红框框出来的两个东西的，出现这个就是OK了，但还没完 我这里还没创建collections，所以什么都没有： 注意下访问地址的输入，要写全。访问地址写成这样会404 第十步：创建新的Collection进行分片处理点击页面的Collections按钮，然后就能添加了（高版本的solr才有，低版本的需要通过地址栏传递参数去设置，比较麻烦） 我这里选择的是名字叫mycollection1，用自己上传的myconf配置文件，有2片shard，每个shard有2个备份节点一主一备 然后回去那个第九步空的页面看下，这样solr集群就搭建ok了！ 第十一步：删除不用的Collection或core删除collection，点这里，然后输入你要删除的collection名称就行 删除core在右边，完了如果要添加下面有add replica（如果工作中你的哪个备份机挂了，就这样删掉挂的服务器，再添加一个好的就行，当然了，在这里添加前，肯定是需要在服务器上部署好solr服务，然后连接zookeeper集群才行的）]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>solr集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solr服务在Linux上搭建详细步骤]]></title>
    <url>%2Fblog%2F2017%2F12%2F20%2F2017-12-20-Solr%E6%9C%8D%E5%8A%A1%E5%9C%A8Linux%E4%B8%8A%E6%90%AD%E5%BB%BA%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[一、系统环境CentOS-6.7-i386-bin-DVD1 jdk-8u151-linux-i586 apache-tomcat-8.5.24.tar solr-7.1.0 注意：solr6.0以上，官方建议使用jdk8，tomcat8。和solr4部署流程有略微差别。部署solr服务前，要先关闭iptables防火墙服务 二、搭建步骤第一步：上传并解压solr7.1这里解压出来的文件夹叫solr-7.1.0，后面有很多步骤需要这个目录，以下简称：solr目录 12rz tar zxf solr-7.1.0.tgz 第二步：上传并解压tomcat8.5，复制一份出来1234rztar zxf apache-tomcat-8.5.24.tar.gzmkdir /usr/local/solrcp -r apache-tomcat-8.5.24 /usr/local/solr/tomcat 第三步：把solr部署到tomcat下注意，这里因为我用的是solr7.1最新版，所以跟网上很多solr4的版本要拷贝*.war文件，然后再启动tomcat解压的操作是不一样的。（这里直接就是解压好的） 复制并重命名solr目录里的server/solr-webapp/webapp文件夹到/usr/local/solr/tomcat/webapps/solr 第四步：把solr目录里的server/lib/目录下的部分jar包，添加到第三步部署的solr工程中。solr目录里的server/lib/ext/下的所有jar包，都复制到 /usr/local/solr/tomcat/webapps/solr/WEB-INF/lib/下，都是些日志相关的jar包 12cd server/lib/ext/cp * /usr/local/solr/tomcat/webapps/solr/WEB-INF/lib/ solr目录里的server/lib/metrics* 开头的5个jar包，复制到 /usr/local/solr/tomcat/webapps/solr/WEB-INF/lib/下（solr4部署没有这个） 12cd server/lib/cp met* /usr/local/solr/tomcat/webapps/solr/WEB-INF/lib/ 第五步：把solr目录里的server/resources/目录下的log4j.properties，添加到第三步部署的solr工程中注意要创建一个classes的目录（solr4部署没有第五步） 123mkdir /usr/local/solr/tomcat/webapps/solr/WEB-INF/classes/cd server/resources/cp log4j.properties /usr/local/solr/tomcat/webapps/solr/WEB-INF/classes/ 第六步：创建一个solrhome将solr目录里的servier/solr文件夹，复制到/usr/local/solr/下，重命名为solrhome，现在/usr/local/solr/目录下就有两个文件夹了，分别是第二步复制过来的tomcat文件夹，和第六步复制过来的solrhome文件夹 12cd servercp -r solr /usr/local/solr/solrhome 第七步：关联已部署的solr和solrhome需要修改第二步复制出来的tomcat目录里，solr工程的web.xml文件 12cd tomcat/webapps/solr/WEB-INF/vi web.xml 找到这个，是被注释的，需要修改value那项 打开注释，修改为上面自己创建的solrhome目录 然后到最下方，将这一段注释掉，不然会报403错误，完成后保存退出（solr4部署不用注释这个） 第八步：启动tomcat12cd tomcat/bin./startup.sh 去自己的电脑上访问下服务器的solr服务 注意下访问地址，直接访问(根据你自己的服务器ip写)192.168.25.128:8080/solr会报404，需要在后面加上/index.xml 如果出现下面的页面，就是solr服务搭建ok了 如果要关闭solr服务，直接关闭tomcat就可以了 12cd tomcat/bin./shutdown.sh 三、关于集群搭建这篇只是单机solr服务的搭建过程，如果需要搭建solr集群，请参考我的这篇博客： Solr集群在Linux上搭建详细步骤]]></content>
      <categories>
        <category>Solr</category>
      </categories>
      <tags>
        <tag>单机solr服务搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux服务器上安装JDK小白教程]]></title>
    <url>%2Fblog%2F2017%2F09%2F07%2F2017-09-07-Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E5%AE%89%E8%A3%85JDK%E5%B0%8F%E7%99%BD%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、环境VMware12 Pro CentOS-6.7-i386-bin-DVD1 jdk-8u151-linux-i586 二、详细安装步骤前提：需要卸载已经装过的jdk rpm -qa | grep jdk 会显示你所有包含jdk这个字符串的安装包 rpm -e –nodeps 对应的每个包名 会卸载对应的包 之后如果输入java -version命令后显示command not found，就是卸载完了。我之前是装过jdk1.7的，所以我需要卸载 ，从来没装过jdk的直接从下面开始 1. 去官网下载JDK http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 需要选中那个图中红色框起来的小圆点，才能下在，意思就是你接收许可协议 因为我的虚拟系统是32位的，所以我下32位JDK，根据你的情况具体看。x86代表32位系统，x64代表64位系统。 不知道服务器是多少位系统的，直接用这个命令： 1getconf LONG_BIT 2. 上传，解压JDK有人可能不知到怎么上传文件到服务器上，这里我推荐个软件，叫lrzsz 这是一个简单的linux服务器上传下载工具，如果你没装过的话，可以装一下，挺好用的 安装前先检查有没有安装lrzsz： 1rpm –q lrzsz 没安装可以使用下列命令进行安装： 1yum -y install lrzsz 之后你就可以用rz命令上传本地文件到服务器的当前目录下了，sz命令后面跟上指定文件目录，可以将其从服务器上下载到本地 好了，用rz命令上传好jdk的tar包后，需要解压，执行如下命令，可能你用的版本和我不一样，注意后面是你自己上传的jdk的包名： 1tar zxf jdk-8u151-linux-i586.tar.gz 3. 移动下位置为了以后方便管理，我把jdk的目录换个位置，重命名一下 1mv jdk1.8.0_151 /usr/share/jdk1.8 4. 配置环境变量用vim打开/etc/profile文件 1vim /etc/profile 在文件最下面添加下面的语句，保存退出（按ESC，然后输入，英文冒号wq英文感叹号（:wq!），按回车） 1234JAVA_HOME=/usr/share/jdk1.8CLASSPATH=$JAVA_HOME/lib/PATH=$PATH:$JAVA_HOME/binexport PATH JAVA_HOME CLASSPATH 执行下面的命令让配置立即生效 1source /etc/profile 现在输入java -version命令，如果显示java version &quot;1.8.0_151&quot;，就是ok了]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>JDK安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring框架学习笔记整理-cache篇]]></title>
    <url>%2Fblog%2F2017%2F08%2F28%2F2017-08-28-Spring%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-cache%E7%AF%87%2F</url>
    <content type="text"><![CDATA[关于缓存缓存是实际工作中非常常用的一种提高性能的方法。而在java中，所谓缓存，就是将程序或系统经常要调用的对象存在内存中，再次调用时可以快速从内存中获取对象，不必再去创建新的重复的实例。这样做可以减少系统开销，提高系统效率。在增删改查中，数据库查询占据了数据库操作的80%以上，而非常频繁的磁盘I/O读取操作，会导致数据库性能极度低下。而数据库的重要性就不言而喻了： 数据库通常是企业应用系统最核心的部分 数据库保存的数据量通常非常庞大 数据库查询操作通常很频繁，有时还很复杂 在系统架构的不同层级之间，为了加快访问速度，都可以存在缓存 Spring cache特性与缺憾现在市场上主流的缓存框架有ehcache、redis、memcached。spring cache可以通过简单的配置就可以搭配使用起来。其中使用注解方式是最简单的。 Cache注解 从以上的注解中可以看出，虽然使用注解的确方便，但是缺少灵活的缓存策略， 缓存策略： TTL（Time To Live ） 存活期，即从缓存中创建时间点开始直到它到期的一个时间段（不管在这个时间段内有没有访问都将过期） TTI（Time To Idle） 空闲期，即一个数据多久没被访问将从缓存中移除的时间 项目中可能有很多缓存的TTL不相同，这时候就需要编码式使用编写缓存。 条件缓存根据运行流程，如下@Cacheable将在执行方法之前( #result还拿不到返回值)判断condition，如果返回true，则查缓存； 12@Cacheable(value = "user", key = "#id", condition = "#id lt 10") public User conditionFindById(final Long id) 如下@CachePut将在执行完方法后（#result就能拿到返回值了）判断condition，如果返回true，则放入缓存 12@CachePut(value = "user", key = "#id", condition = "#result.username ne 'liu'") public User conditionSave(final User user) 如下@CachePut将在执行完方法后（#result就能拿到返回值了）判断unless，如果返回false，则放入缓存；（即跟condition相反） 12@CachePut(value = "user", key = "#user.id", unless = "#result.username eq 'liu'") public User conditionSave2(final User user) 如下@CacheEvict， beforeInvocation=false表示在方法执行之后调用（#result能拿到返回值了）；且判断condition，如果返回true，则移除缓存； 12@CacheEvict(value = "user", key = "#user.id", beforeInvocation = false, condition = "#result.username ne 'liu'") public User conditionDelete(final User user) 小试牛刀，综合运用： 12345678910111213141516171819202122232425262728293031323334@CachePut(value = "user", key = "#user.id")public User save(User user) &#123; users.add(user); return user;&#125;@CachePut(value = "user", key = "#user.id")public User update(User user) &#123; users.remove(user); users.add(user); return user;&#125;@CacheEvict(value = "user", key = "#user.id")public User delete(User user) &#123; users.remove(user); return user;&#125;@CacheEvict(value = "user", allEntries = true)public void deleteAll() &#123; users.clear();&#125;@Cacheable(value = "user", key = "#id")public User findById(final Long id) &#123; System.out.println("cache miss, invoke find by id, id:" + id); for (User user : users) &#123; if (user.getId().equals(id)) &#123; return user; &#125; &#125; return null;&#125; 配置ehcache与redis spring cache集成ehcache，spring-ehcache.xml主要内容： 12345&lt;dependency&gt; &lt;groupId&gt;net.sf.ehcache&lt;/groupId&gt; &lt;artifactId&gt;ehcache-core&lt;/artifactId&gt; &lt;version&gt;$&#123;ehcache.version&#125;&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314&lt;!-- Spring提供的基于的Ehcache实现的缓存管理器 --&gt; &lt;!-- 如果有多个ehcacheManager要在bean加上p:shared="true" --&gt;&lt;bean id="ehcacheManager" class="org.springframework.cache.ehcache.EhCacheManagerFactoryBean"&gt; &lt;property name="configLocation" value="classpath:xml/ehcache.xml"/&gt;&lt;/bean&gt; &lt;bean id="cacheManager" class="org.springframework.cache.ehcache.EhCacheCacheManager"&gt; &lt;property name="cacheManager" ref="ehcacheManager"/&gt; &lt;property name="transactionAware" value="true"/&gt;&lt;/bean&gt; &lt;!-- cache注解，和spring-redis.xml中的只能使用一个 --&gt;&lt;cache:annotation-driven cache-manager="cacheManager" proxy-target-class="true"/&gt; spring cache集成redis，spring-redis.xml主要内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt; &lt;version&gt;1.8.1.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 注意需要添加Spring Data Redis等jar包 --&gt;&lt;description&gt;redis配置&lt;/description&gt;&lt;bean id="jedisPoolConfig" class="redis.clients.jedis.JedisPoolConfig"&gt; &lt;property name="maxIdle" value="$&#123;redis.pool.maxIdle&#125;"/&gt; &lt;property name="maxTotal" value="$&#123;redis.pool.maxActive&#125;"/&gt; &lt;property name="maxWaitMillis" value="$&#123;redis.pool.maxWait&#125;"/&gt; &lt;property name="testOnBorrow" value="$&#123;redis.pool.testOnBorrow&#125;"/&gt; &lt;property name="testOnReturn" value="$&#123;redis.pool.testOnReturn&#125;"/&gt;&lt;/bean&gt;&lt;!-- JedisConnectionFactory --&gt;&lt;bean id="jedisConnectionFactory" class="org.springframework.data.redis.connection.jedis.JedisConnectionFactory"&gt; &lt;property name="hostName" value="$&#123;redis.master.ip&#125;"/&gt; &lt;property name="port" value="$&#123;redis.master.port&#125;"/&gt; &lt;property name="poolConfig" ref="jedisPoolConfig"/&gt;&lt;/bean&gt;&lt;bean id="redisTemplate" class="org.springframework.data.redis.core.RedisTemplate" p:connectionFactory-ref="jedisConnectionFactory"&gt; &lt;property name="keySerializer"&gt; &lt;bean class="org.springframework.data.redis.serializer.JdkSerializationRedisSerializer"&gt;&lt;/bean&gt; &lt;/property&gt; &lt;property name="valueSerializer"&gt; &lt;bean class="org.springframework.data.redis.serializer.JdkSerializationRedisSerializer"/&gt; &lt;/property&gt; &lt;property name="hashKeySerializer"&gt; &lt;bean class="org.springframework.data.redis.serializer.JdkSerializationRedisSerializer"/&gt; &lt;/property&gt; &lt;property name="hashValueSerializer"&gt; &lt;bean class="org.springframework.data.redis.serializer.JdkSerializationRedisSerializer"/&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!--spring cache--&gt;&lt;bean id="cacheManager" class="org.springframework.data.redis.cache.RedisCacheManager" c:redisOperations-ref="redisTemplate"&gt; &lt;!-- 默认缓存10分钟 --&gt; &lt;property name="defaultExpiration" value="600"/&gt; &lt;property name="usePrefix" value="true"/&gt; &lt;!-- cacheName 缓存超时配置，半小时，一小时，一天 --&gt; &lt;property name="expires"&gt; &lt;map key-type="java.lang.String" value-type="java.lang.Long"&gt; &lt;entry key="halfHour" value="1800"/&gt; &lt;entry key="hour" value="3600"/&gt; &lt;entry key="oneDay" value="86400"/&gt; &lt;!-- shiro cache keys --&gt; &lt;entry key="authorizationCache" value="1800"/&gt; &lt;entry key="authenticationCache" value="1800"/&gt; &lt;entry key="activeSessionCache" value="1800"/&gt; &lt;/map&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- cache注解，和spring-ehcache.xml中的只能使用一个 --&gt;&lt;cache:annotation-driven cache-manager="cacheManager" proxy-target-class="true"/&gt; 项目中注解缓存只能配置一个，所以可以通过以下引入哪个配置文件来决定使用哪个缓存。 12&lt;import resource="classpath:spring/spring-ehcache.xml"/&gt;&lt;!-- &lt;import resource="classpath:spring/spring-redis.xml"/&gt;--&gt; 当然，可以通过其他配置搭配使用两个缓存机制。比如ecache做一级缓存，redis做二级缓存。 好了所有spring的知识点总结就到这里结束了。]]></content>
      <categories>
        <category>Spring</category>
        <category>cache</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Spring-cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring框架学习笔记整理-aop篇]]></title>
    <url>%2Fblog%2F2017%2F08%2F07%2F2017-08-07-Spring%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-aop%E7%AF%87%2F</url>
    <content type="text"><![CDATA[什么是aopAOP（Aspect-OrientedProgramming，面向方面编程），可以说是OOP（Object-Oriented Programing，面向对象编程）的补充和完善。OOP允许你定义从上到下的关系，但并不适合定义从左到右的关系。例如日志功能。日志代码往往水平地散布在所有对象层次中，而与它所散布到的对象的核心功能毫无关系。这种散布在各处的无关的代码被称为横切（cross-cutting）代码，在OOP设计中，它导致了大量代码的重复，而不利于各个模块的重用。而AOP技术则恰恰相反，它利用一种称为“横切”的技术，剖解开封装的对象内部，并将那些影响了多个类的公共行为封装到一个可重用模块，并将其名为“Aspect”，即方面。所谓“方面”，简单地说，就是将那些与业务无关，却为业务模块所共同调用的逻辑或责任封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可操作性和可维护性。 aop使用场景aop框架种类 AspectJ JBoss AOP Spring AOP 使用aop可以做的事情有很多。 性能监控，在方法调用前后记录调用时间，方法执行太长或超时报警。 缓存代理，缓存某方法的返回值，下次执行该方法时，直接从缓存里获取。 软件破解，使用AOP修改软件的验证类的判断逻辑。 记录日志，在方法执行前后记录系统日志。 工作流系统，工作流系统需要将业务代码和流程引擎代码混合在一起执行，那么我们可以使用AOP将其分离，并动态挂接业务。 权限验证，方法执行前验证是否有权限执行当前方法，没有则抛出没有权限执行异常，由业务代码捕捉。 观察一下传统编码方式与使用aop的区别： 核心概念描述AOP常用的一些术语有通知(Adivce)、切点（Pointcut）、连接点（Join point）、切面（Aspect）、引入（Introduction）、织入（Weaving）、通知（Advice）等。 简单例子相比xml配置，基于注解的方式更加简洁方便。 12345678910111213141516171819202122232425@Aspectpublic class TransactionDemo &#123; @Pointcut(value="execution(* cn.dijia478.core.service.*.*.*(..))") public void point()&#123; &#125; @Before(value="point()") public void before()&#123; System.out.println("transaction begin"); &#125; @AfterReturning(value = "point()") public void after()&#123; System.out.println("transaction commit"); &#125; @Around("point()") public void around(ProceedingJoinPoint joinPoint) throws Throwable&#123; System.out.println("transaction begin"); joinPoint.proceed(); System.out.println("transaction commit"); &#125;&#125; 在applicationContext.xml中配置。 12&lt;aop:aspectj-autoproxy /&gt;&lt;bean id = "transactionDemo" class = "cn.dijia478.core.transaction.TransactionDemo" /&gt; Spring aop原理通过前面介绍可以知道：AOP 代理其实是由 AOP 框架动态生成的一个对象，该对象可作为目标对象使用。AOP 代理包含了目标对象的全部方法，但 AOP 代理中的方法与目标对象的方法存在差异：AOP 方法在特定切入点添加了增强处理，并回调了目标对象的方法。 Spring 的 AOP 代理由 Spring 的 IoC 容器负责生成、管理，其依赖关系也由 IoC 容器负责管理。因此，AOP 代理可以直接使用容器中的其他 Bean 实例作为目标，这种关系可由 IoC 容器的依赖注入提供。 aop开发时，其中需要程序员参与的只有 3 个部分： 定义普通业务组件。 定义切入点，一个切入点可能横切多个业务组件。 定义增强处理，增强处理就是在 AOP 框架为普通业务组件织入的处理动作。 为了理清关系，先来个类关系图(找不到更清晰的了)。 两种动态代理方式Spring默认采取的动态代理机制实现AOP，当动态代理不可用时（代理类无接口）会使用CGlib机制。 Spring提供了两种方式来生成代理对象: JDKProxy和Cglib，具体使用哪种方式生成由AopProxyFactory根据AdvisedSupport对象的配置来决定。默认的策略是如果目标类是接口，则使用JDK动态代理技术，否则使用Cglib来生成代理。 JDK动态代理 JDK动态代理主要涉及到java.lang.reflect包中的两个类：Proxy和InvocationHandler。InvocationHandler是一个接口，通过实现该接口定义横切逻辑，并通过反射机制调用目标类的代码，动态将横切逻辑和业务逻辑编制在一起。 Proxy利用InvocationHandler动态创建一个符合某一接口的实例，生成目标类的代理对象。 CGLib动态代理 CGLib全称为Code Generation Library，是一个强大的高性能，高质量的代码生成类库，可以在运行期扩展Java类与实现Java接口，CGLib封装了asm，可以再运行期动态生成新的class。和JDK动态代理相比较：JDK创建代理有一个限制，就是只能为接口创建代理实例，而对于没有通过接口定义业务方法的类，则可以通过CGLib创建动态代理。 知识拓展通过上面的分析，大家是否有种熟悉的感觉，似乎和拦截器、过滤器的功能相似。那么问题来了，aop与拦截器、过滤器是什么关系。 先来回顾一下拦截器与过滤器。如下图一网友的测试，在web.xml中注册了TestFilter1和TestFilter2。然后在spring的配置文件中配置了BaseInterceptor和TestInterceptor。得到的结果如下图所示。从图中可以看出，拦截器和过滤器都横切了业务方法，看似符合aop的思想。 Filter过滤器：拦截web访问url地址。 Interceptor拦截器：拦截以 .action结尾的url，拦截Action的访问。 Spring AOP拦截器：只能拦截Spring管理Bean的访问（业务层Service） aop篇就写到这里，下篇博客将会写Spring cache的内容]]></content>
      <categories>
        <category>Spring</category>
        <category>aop</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Spring-aop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring框架学习笔记整理-mvc篇]]></title>
    <url>%2Fblog%2F2017%2F07%2F17%2F2017-07-17-Spring%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-mvc%E7%AF%87%2F</url>
    <content type="text"><![CDATA[Spring MVC简介与运行原理Spring的模型-视图-控制器（MVC）框架是围绕一个DispatcherServlet来设计的，这个Servlet会把请求分发给各个处理器，并支持可配置的处理器映射、视图渲染、本地化、时区与主题渲染等，甚至还能支持文件上传。 (1) Http请求：客户端请求提交到DispatcherServlet。 (2) 寻找处理器：由DispatcherServlet控制器查询一个或多个HandlerMapping，找到处理请求的Controller。 (3) 调用处理器：DispatcherServlet将请求提交到Controller。 (4)(5)调用业务处理和返回结果：Controller调用业务逻辑处理后，返回ModelAndView。 (6)(7)处理视图映射并返回模型： DispatcherServlet查询一个或多个ViewResoler视图解析器，找到ModelAndView指定的视图。 (8) Http响应：视图负责将结果显示到客户端 Spring MVC的主要注解 ContextLoaderListener在讲ContextLoaderListener之前，首先来了解一下web.xml的作用。 一个web项目中可以没有web.xml文件，也就是说，web.xml文件并不是web工程必须的。web.xml文件是用来初始化配置信息：比如Welcome页面、servlet、servlet-mapping、filter、listener、启动加载级别等。当你的web工程没用到这些时，你可以不用web.xml文件来配置你的Application。 当要启动某个web项目时，服务器软件或容器如（tomcat）会第一步加载项目中的web.xml文件，通过其中的各种配置来启动项目，只有其中配置的各项均无误时，项目才能正确启动。web.xml有多项标签，在其加载的过程中顺序依次为：context-param &gt;&gt; listener &gt;&gt; fileter &gt;&gt; servlet。（同类多个节点以出现顺序依次加载） 而Spring MVC启动过程大致分为两个过程： ContextLoaderListener初始化，实例化IOC容器，并将此容器实例注册到ServletContext中。 DispatcherServlet初始化。 其中ContextLoaderListener监听器它实现了ServletContextListener这个接口，在web.xml配置这个监听器，启动容器时，就会默认执行它实现的方法。在ContextLoaderListener中关联了ContextLoader这个类，所以整个加载配置过程由ContextLoader来完成。 ContextLoaderListener在web.xml中的配置： 12345678910&lt;!-- 配置contextConfigLocation初始化参数 --&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/applicationContext.xml&lt;/param-value&gt;&lt;/context-param&gt;&lt;!-- 配置ContextLoaderListerner --&gt;&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt; ServletContextListener 接口有两个方法：contextInitialized和contextDestroyed DispatcherServletSpring MVC框架，与其他很多web的MVC框架一样：请求驱动；所有设计都围绕着一个中央Servlet来展开，它负责把所有请求分发到控制器；同时提供其他web应用开发所需要的功能。不过Spring的中央处理器，DispatcherServlet，能做的比这更多。 下图展示了Spring Web MVC的DispatcherServlet处理请求的工作流。熟悉设计模式的朋友会发现，DispatcherServlet应用的其实就是一个“前端控制器”的设计模式（其他很多优秀的web框架也都使用了这个设计模式）。 流程图 DispatcherServlet在web.xml中的配置 1234567891011&lt;!-- servlet定义 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 其中 load-on-startup：表示启动容器时初始化该Servlet； url-pattern：表示哪些请求交给Spring Web MVC处理， / 是用来定义默认servlet映射的。也可以如*.html表示拦截所有以html为扩展名的请求。 在Spring MVC中，每个DispatcherServlet都持有一个自己的上下文对象WebApplicationContext，它又继承了根（root）WebApplicationContext对象中已经定义的所有bean。这些继承的bean可以在具体的Servlet实例中被重载，在每个Servlet实例中你也可以定义其scope下的新bean。 WebApplicationContext继承自ApplicationContext，它提供了一些web应用经常需要用到的特性。它与普通的ApplicationContext不同的地方在于，它支持主题的解析，并且知道它关联到的是哪个servlet（它持有一个该ServletContext的引用） Spring MVC同时提供了很多特殊的注解，用于处理请求和渲染视图等。DispatcherServlet初始化的过程中会默认使用这些特殊bean进行配置。如果你想指定使用哪个特定的bean，你可以在web应用上下文WebApplicationContext中简单地配置它们。 其中，常用的ViewResolver的配置。以jsp作为视图为例 12345&lt;!-- 对模型视图名称的解析,即在模型视图名称添加前后缀 --&gt;&lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="prefix" value="/WEB-INF/jsp/" /&gt; &lt;property name="suffix" value=".jsp" /&gt;&lt;/bean&gt; 配置上传文件限制MultipartResolver 12345&lt;!-- 上传限制 --&gt;&lt;bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver"&gt; &lt;!-- 上传文件大小限制为31M，31*1024*1024 --&gt; &lt;property name="maxUploadSize" value="32505856"/&gt;&lt;/bean&gt; applicationContext.xml配置文件中的标签 文件上传前面说到DispatcherServlet中有个特殊的Bean叫MultipartResolver，可用于限制文件的上传大小等。当解析器MultipartResolver完成处理时，请求便会像其他请求一样被正常流程处理。 表单 12345&lt;form method="post" action="/form" enctype="multipart/form-data"&gt; &lt;input type="text" name="name"/&gt; &lt;input type="file" name="file"/&gt; &lt;input type="submit"/&gt;&lt;/form&gt; 控制器 1234567891011@RequestMapping(path = "/form", method = RequestMethod.POST) public String handleFormUpload(@RequestParam("name") String name, @RequestParam("file") MultipartFile file) &#123; if (!file.isEmpty()) &#123; byte[] bytes = file.getBytes(); // store the bytes somewhere return "redirect:uploadSuccess"; &#125; return "redirect:uploadFailure";&#125; 异常处理先来说下常见的异常处理有几种方式，如下图： Spring的处理器异常解析器HandlerExceptionResolver接口的实现负责处理各类控制器执行过程中出现的异常。也是上面提到的，是DispatcherServlet中的特殊bean，可以自定义配置处理。 某种程度上讲，HandlerExceptionResolver与你在web应用描述符web.xml文件中能定义的异常映射（exception mapping）很相像，不过它比后者提供了更灵活的方式。比如它能提供异常被抛出时正在执行的是哪个处理器这样的信息。 HandlerExceptionResolver 提供resolveException接口 1234public interface HandlerExceptionResolver &#123; ModelAndView resolveException( HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex); &#125; 在BaseController中使用 @ExceptionHandler注解处理异常 123456789101112131415161718192021222324252627282930313233343536373839@ExceptionHandler(Exception.class)public Object exceptionHandler(Exception ex, HttpServletResponse response, HttpServletRequest request) throws IOException &#123; String url = ""; String msg = ex.getMessage(); Object resultModel = null; try &#123; if (ex.getClass() == HttpRequestMethodNotSupportedException.class) &#123; url = "admin/common/500"; System.out.println("--------没有找到对应方法---------"); &#125; else if (ex.getClass() == ParameterException.class) &#123;// 自定义的异常 &#125; else if (ex.getClass() == UnauthorizedException.class) &#123; url = "admin/common/unauth"; System.out.println("--------没有权限---------"); &#125; String header = req.getHeader("X-Requested-With"); boolean isAjax = "XMLHttpRequest".equalsIgnoreCase(header); String method = req.getMethod(); boolean isPost = "POST".equalsIgnoreCase(method); if (isAjax || isPost) &#123; return Message.error(msg); &#125; else &#123; ModelAndView view = new ModelAndView(url); view.addObject("error", msg); view.addObject("class", ex.getClass()); view.addObject("method", request.getRequestURI()); return view; &#125; &#125; catch (Exception exception) &#123; logger.error(exception.getMessage(), exception); return resultModel; &#125; finally &#123; logger.error(msg, ex); ex.printStackTrace(); &#125;&#125; 在web.xml中处理异常 12345678910111213141516171819202122232425262728&lt;!-- 默认的错误处理页面 --&gt;&lt;error-page&gt; &lt;error-code&gt;403&lt;/error-code&gt; &lt;location&gt;/403.html&lt;/location&gt;&lt;/error-page&gt;&lt;error-page&gt; &lt;error-code&gt;404&lt;/error-code&gt; &lt;location&gt;/404.html&lt;/location&gt;&lt;/error-page&gt;&lt;!-- 仅仅在调试的时候注视掉,在正式部署的时候不能注释 --&gt;&lt;!-- 这样配置也是可以的，表示发生500错误的时候，转到500.jsp页面处理。 --&gt;&lt;error-page&gt; &lt;error-code&gt;500&lt;/error-code&gt; &lt;location&gt;/500.html&lt;/location&gt; &lt;/error-page&gt; &lt;!-- 这样的配置表示如果jsp页面或者servlet发生java.lang.Exception类型（当然包含子类）的异常就会转到500.jsp页面处理。 --&gt;&lt;error-page&gt; &lt;exception-type&gt;java.lang.Exception&lt;/exception-type&gt; &lt;location&gt;/500.jsp&lt;/location&gt; &lt;/error-page&gt; &lt;error-page&gt; &lt;exception-type&gt;java.lang.Throwable&lt;/exception-type&gt; &lt;location&gt;/500.jsp&lt;/location&gt; &lt;/error-page&gt;&lt;!-- 当error-code和exception-type都配置时，exception-type配置的页面优先级高及出现500错误，发生异常Exception时会跳转到500.jsp--&gt; 来一个问题：HandlerExceptionResolver和web.xml中配置的error-page会有冲突吗？ 解答：如果resolveException返回了ModelAndView，会优先根据返回值中的页面来显示。不过，resolveException可以返回null，此时则展示web.xml中的error-page的500状态码配置的页面。 当web.xml中有相应的error-page配置，则可以在实现resolveException方法时返回null。 API文档中对返回值的解释：return a corresponding ModelAndView to forward to, or null for default processing. OK，关于Spring MVC就先到这里，下篇博客将会整理Spring aop的内容]]></content>
      <categories>
        <category>Spring</category>
        <category>SpringMVC</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring框架学习笔记整理-ioc篇]]></title>
    <url>%2Fblog%2F2017%2F05%2F18%2F2017-05-18-Spring%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-ioc%E7%AF%87%2F</url>
    <content type="text"><![CDATA[学过java的都知道：所有的对象都必须创建；或者说：使用对象之前必须先创建。而使用IoC之后，你就可以不再手动创建对象，而是从IoC容器中直接获取对象。 就好像我们无需考虑对象的销毁回收一样，因为java垃圾回收机制帮助我们实现了这个过程；而IoC则是让我们无需考虑对象的创建过程，由IoC容器帮我们实现对象的创建、注入等过程。 控制反转 Spring IoC容器在Spring框架中的核心组件只有三个：Core、Context和Bean。它们构建起了整个Spring的骨骼架构，没有它们就不可能有AOP、Web等特性功能 如果说在三个核心中再选出一个核心，那就非Bean莫属了。可以说，Spring就是面向Bean的编程，Bean在Spring中才是真正的主角。 Spring为何如此流行？你会发现Spring解决了一个非常关键的问题，它可以让你对对象之间的关系转而用配置文件来管理，或者注解，也就是它的依赖注入机制。而这个注入关系在一个叫IoC的容器中管理。IoC容器就是被Bean包裹的对象。Spring正是通过把对象包装在Bean中从而达到管理这些对象及做一些列额外操作的目的。 三大核心组件协同工作BeanSpring的bean实例。身份是演员。 ContextSpring的上下文。身份是导演。 我们知道Bean包装的是Object，而Object必然有数据，如何给这些数据提供生存环境就是Context要解决的问题，对Context来说，他就是要发现每个Bean之间的关系，为他们建立这种关系并且要维护好这种关系。所以Context就是一个Bean关系的集合，这个关系集合又叫IoC容器，一旦建立起这个IoC容器后，Spring就可以为你工作了。 CoreSpring的核心工具包。身份是道具。 建立和维护每个Bean之间的关系所需要的一系列核心工具包。其实就相当于Util包。 BeanFactory与ApplacationContext的区别IoC中最核心的接口是Beanfactory提供IoC的高级服务，而ApplicationContext是建立在BeanFactory基础之上提供抽象的面向应用的服务。 三种注入方式在Spring框架中，依赖注入(DI)的设计模式是用来定义对象彼此间的依赖。使用xml配置bean的情况下，它主要有两种类型： Setter方法注入 构造器注入 当然，有了注解之后，使用注解的方式更加方便快捷。即自动装配功能实现属性自动注入（@Autowire）。 写到这里，让我想起了最近在牛客网上看的一道关于spring的选择题了： 下面有关spring的依赖注入，说法错误的是？A.依赖注入通常有如下两种：设置注入和构造注入B.构造注入可以在构造器中决定依赖关系的注入顺序，优先依赖的优先注入C.当设值注入与构造注入同时存在时，先执行设值注入，再执行构造注入D.设值注入是指IoC容器使用属性的setter方法来注入被依赖的实例。这种注入方式比较简单、直观 牛客网给出的答案是选C，应该是先执行构造注入，后执行设置注入。查看网友评论及答案 Spring原理解析Spring的代码还真是不好读，分得太细了，文字也是难以描述出来，看了别人有关的博客，贴了好多代码，画了好多ER图来描述关键接口或类之间的关系。这么一篇这么长文章下来，大家也未必会认真读代码，看ER图，干脆也不跟风了。就贴了一点在我看来比较关键的代码，嘿嘿。 context的初始化过程当运行ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); 时，构造方法ClassPathXmlApplicationContext(String configLocation)调用了this(new String[] {configLocation}, true, null);， 该构造方法具体代码如下。 从时序图来看启动上述初始化 好了，IoC容器篇就简单总结到这里，下一篇会整理下Spring MVC的相关内容]]></content>
      <categories>
        <category>Spring</category>
        <category>IoC</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Spring-IoC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring框架学习笔记整理-概述]]></title>
    <url>%2Fblog%2F2017%2F05%2F04%2F2017-05-04-Spring%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[关于SpringSpring 是一个开源的Java／Java EE全功能栈的应用程序框架，他解决的是业务逻辑层和其他各层的松耦合问题，因此它将面向接口的编程思想贯穿整个系统应用。它的主要优势之一就是其分层架构，分层架构允许您选择使用哪一个组件，同时为 Java EE 应用程序开发提供集成的框架。 特点： Sping架构Spring框架是分模块存在，除了最核心的Spring Core Container(即Spring容器)是必要模块之外，其他模块都是可选，视需要而定。大约有20多个模块。 Spring3与Spring4是有区别的，4.0主要是对Java 8的新函数式语法进行支持，还有加强了对网络各种新技术比如http-streaming, websocket的更好的支持。 一般来说，Spring主要分为7个模块： Spring的主要jar包 Spring的常用注解bean注入与装配的的方式有很多种，可以通过xml，getset方式，构造函数或者注解等。简单易用的方式就是使用Spring的注解了，Spring提供了大量的注解方式，让项目阅读和开发起来更加方便。 常用注解 装配注解比较 第三方框架集成Spring框架的开发不是为了替代现有的优秀第三方框架，而是通过集成的方式把它们都连接起来。下面总结了一些常集成的优秀框架。 最后本篇博客简单总结了下Spring是什么，没涉及到原理的东西。后面还会整理4篇笔记出来。分为ioc篇，mvc篇，aop篇和cache篇。]]></content>
      <categories>
        <category>Spring</category>
        <category>概述</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Spring概述</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis框架学习笔记整理-下篇]]></title>
    <url>%2Fblog%2F2017%2F04%2F21%2F2017-04-21-MyBatis%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E4%B8%8B%E7%AF%87%2F</url>
    <content type="text"><![CDATA[上一篇博客写了MyBatis的基本原理和配置文件的基本使用，这一篇写MyBatis的使用，主要包括与Sping集成、动态sql、还有mapper的xml文件一下复杂配置等。值得注意的是，思维导图”resultMap子元素”中讲解的时候可能讲得不是很清楚，应该需要大量的代码来讲解使用方法，思维导图恰恰不适合这种表现方法。所以需要根据给出的链接去读一些好的博客文章，这样才能更好理解。毕竟是技术性的东西，实践出真理。 MyBatis简介MyBatis 是支持普通 SQL查询，存储过程和高级映射的优秀持久层框架。MyBatis 消除了几乎所有的JDBC代码和参数的手工设置以及结果集的检索。MyBatis 使用简单的 XML或注解用于配置和原始映射，将接口和 Java 的POJOs（Plain Old Java Objects，普通的 Java对象）映射成数据库中的记录。 MyBatis的优缺点 与Spring整合 mapper的xml文件 官方参考文档 属性值可以用于包含的refid属性或者包含的字句里面的属性值1234567891011121314151617&lt;sql id="sometable"&gt; $&#123;prefix&#125;Table&lt;/sql&gt;&lt;sql id="someinclude"&gt; from &lt;include refid="$&#123;include_target&#125;"/&gt;&lt;/sql&gt;&lt;select id="select" resultType="map"&gt; select field1, field2, field3 &lt;include refid="someinclude"&gt; &lt;property name="prefix" value="Some"/&gt; &lt;property name="include_target" value="sometable"/&gt; &lt;/include&gt;&lt;/select&gt; 高级结果映射MyBatis 创建的一个想法:数据库不用永远是你想要的或需要它们是什么样的。而我们 最喜欢的数据库最好是第三范式或 BCNF 模式,但它们有时不是。如果可能有一个单独的 数据库映射,所有应用程序都可以使用它,这是非常好的,但有时也不是。结果映射就是 MyBatis 提供处理这个问题的答案。 比如,我们如何映射下面这个语句? 12345678910111213141516171819202122232425262728293031323334&lt;!-- Very Complex Statement --&gt;&lt;select id="selectBlogDetails" resultMap="detailedBlogResultMap"&gt; select B.id as blog_id, B.title as blog_title, B.author_id as blog_author_id, A.id as author_id, A.username as author_username, A.password as author_password, A.email as author_email, A.bio as author_bio, A.favourite_section as author_favourite_section, P.id as post_id, P.blog_id as post_blog_id, P.author_id as post_author_id, P.created_on as post_created_on, P.section as post_section, P.subject as post_subject, P.draft as draft, P.body as post_body, C.id as comment_id, C.post_id as comment_post_id, C.name as comment_name, C.comment as comment_text, T.id as tag_id, T.name as tag_name from Blog B left outer join Author A on B.author_id = A.id left outer join Post P on B.id = P.blog_id left outer join Comment C on P.id = C.post_id left outer join Post_Tag PT on PT.post_id = P.id left outer join Tag T on PT.tag_id = T.id where B.id = #&#123;id&#125;&lt;/select&gt; 你可能想把它映射到一个智能的对象模型,包含一个作者写的博客,有很多的博文,每 篇博文有零条或多条的评论和标签。 下面是一个完整的复杂结果映射例子 (假设作者, 博客, 博文, 评论和标签都是类型的别名) 我们来看看, 。 但是不用紧张, 我们会一步一步来说明。 当天最初它看起来令人生畏,但实际上非常简单。 1234567891011121314151617181920212223242526272829&lt;!-- Very Complex Result Map --&gt;&lt;resultMap id="detailedBlogResultMap" type="Blog"&gt; &lt;constructor&gt; &lt;idArg column="blog_id" javaType="int"/&gt; &lt;/constructor&gt; &lt;result property="title" column="blog_title"/&gt; &lt;association property="author" javaType="Author"&gt; &lt;id property="id" column="author_id"/&gt; &lt;result property="username" column="author_username"/&gt; &lt;result property="password" column="author_password"/&gt; &lt;result property="email" column="author_email"/&gt; &lt;result property="bio" column="author_bio"/&gt; &lt;result property="favouriteSection" column="author_favourite_section"/&gt; &lt;/association&gt; &lt;collection property="posts" ofType="Post"&gt; &lt;id property="id" column="post_id"/&gt; &lt;result property="subject" column="post_subject"/&gt; &lt;association property="author" javaType="Author"/&gt; &lt;collection property="comments" ofType="Comment"&gt; &lt;id property="id" column="comment_id"/&gt; &lt;/collection&gt; &lt;collection property="tags" ofType="Tag" &gt; &lt;id property="id" column="tag_id"/&gt; &lt;/collection&gt; &lt;discriminator javaType="int" column="draft"&gt; &lt;case value="1" resultType="DraftPost"/&gt; &lt;/discriminator&gt; &lt;/collection&gt;&lt;/resultMap&gt; resultMap子元素概念视图1234567891011121314151617181920212223242526272829&lt;!-- Very Complex Result Map --&gt;&lt;resultMap id="detailedBlogResultMap" type="Blog"&gt; &lt;constructor&gt; &lt;idArg column="blog_id" javaType="int"/&gt; &lt;/constructor&gt; &lt;result property="title" column="blog_title"/&gt; &lt;association property="author" javaType="Author"&gt; &lt;id property="id" column="author_id"/&gt; &lt;result property="username" column="author_username"/&gt; &lt;result property="password" column="author_password"/&gt; &lt;result property="email" column="author_email"/&gt; &lt;result property="bio" column="author_bio"/&gt; &lt;result property="favouriteSection" column="author_favourite_section"/&gt; &lt;/association&gt; &lt;collection property="posts" ofType="Post"&gt; &lt;id property="id" column="post_id"/&gt; &lt;result property="subject" column="post_subject"/&gt; &lt;association property="author" javaType="Author"/&gt; &lt;collection property="comments" ofType="Comment"&gt; &lt;id property="id" column="comment_id"/&gt; &lt;/collection&gt; &lt;collection property="tags" ofType="Tag" &gt; &lt;id property="id" column="tag_id"/&gt; &lt;/collection&gt; &lt;discriminator javaType="int" column="draft"&gt; &lt;case value="1" resultType="DraftPost"/&gt; &lt;/discriminator&gt; &lt;/collection&gt;&lt;/resultMap&gt; resultMap子元素 官方参考文档 关联的嵌套查询我们有两个查询语句:一个来加载博客,另外一个来加载作者,而且博客的结果映射描 述了“selectAuthor”语句应该被用来加载它的 author 属性。 其他所有的属性将会被自动加载,假设它们的列和属性名相匹配。 这种方式很简单, 但是对于大型数据集合和列表将不会表现很好。 问题就是我们熟知的 “N+1 查询问题”。概括地讲,N+1 查询问题可以是这样引起的: 你执行了一个单独的 SQL 语句来获取结果列表(就是“+1”)。对返回的每条记录,你执行了一个查询语句来为每个加载细节(就是“N”)。这个问题会导致成百上千的 SQL 语句被执行。这通常不是期望的。 MyBatis 能延迟加载这样的查询就是一个好处,因此你可以分散这些语句同时运行的消 耗。然而,如果你加载一个列表,之后迅速迭代来访问嵌套的数据,你会调用所有的延迟加 载,这样的行为可能是很糟糕的。 元素集合collection 鉴别器discriminator 例子1234567891011121314151617181920212223&lt;resultMap id="vehicleResult" type="Vehicle"&gt; &lt;id property="id" column="id" /&gt; &lt;result property="vin" column="vin"/&gt; &lt;result property="year" column="year"/&gt; &lt;result property="make" column="make"/&gt; &lt;result property="model" column="model"/&gt; &lt;result property="color" column="color"/&gt; &lt;discriminator javaType="int" column="vehicle_type"&gt; &lt;case value="1" resultType="carResult"&gt; &lt;result property="doorCount" column="door_count" /&gt; &lt;/case&gt; &lt;case value="2" resultType="truckResult"&gt; &lt;result property="boxSize" column="box_size" /&gt; &lt;result property="extendedCab" column="extended_cab" /&gt; &lt;/case&gt; &lt;case value="3" resultType="vanResult"&gt; &lt;result property="powerSlidingDoor" column="power_sliding_door" /&gt; &lt;/case&gt; &lt;case value="4" resultType="suvResult"&gt; &lt;result property="allWheelDrive" column="all_wheel_drive" /&gt; &lt;/case&gt; &lt;/discriminator&gt;&lt;/resultMap&gt; 动态 SQL #{}和${}的区别]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis框架学习笔记整理-上篇]]></title>
    <url>%2Fblog%2F2017%2F04%2F16%2F2017-04-16-MyBatis%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E4%B8%8A%E7%AF%87%2F</url>
    <content type="text"><![CDATA[前言与Hibernate相比，我无疑更喜欢MyBatis，就因为我觉得它真的好用，哈哈。它简单上手和掌握；sql语句和代码分开，方便统一管理和优化；当然缺点也有：sql工作量很大，尤其是字段多、关联表多时，更是如此。而且sql依赖于数据库，导致数据库移植性差。 选用一个框架之前最好先了解它的优缺点，对项目最好用，效率最高的才是一个你最好的选择。本次决定使用XMind思维导图来总结，会更为直观。如果图片看不清，可以右键在新标签页打开或者保存到本地。一共分为上下两篇来总结MyBatis的相关知识点。本篇为上篇，主要写一些MyBatis的原理和配置文件的基本实用，后一篇会写MyBatis的使用方法。 MyBaits简介MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生信息，将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录。 官方参考文档 与Hibernate的对比 两者的缓存机制异同 相同点 Hibernate和MyBatis的二级缓存除了采用系统默认的缓存机制外，都可以通过实现你自己的缓存或为其他第三方缓存方案，创建适配器来完全覆盖缓存行为。 不同点 Hibernate的二级缓存配置在SessionFactory生成的配置文件中进行详细配置，然后再在具体的表-对象映射中配置是那种缓存。 MyBatis的二级缓存配置都是在每个具体的表-对象映射中进行详细配置，这样针对不同的表可以自定义不同的缓存机制。并且MyBatis可以在命名空间中共享相同的缓存配置和实例，通过Cache-ref来实现。 两者比较 因为Hibernate对查询对象有着良好的管理机制，用户无需关心SQL。所以在使用二级缓存时如果出现脏数据，系统会报出错误并提示。 而MyBatis在这一方面，使用二级缓存时需要特别小心。如果不能完全确定数据更新操作的波及范围，避免Cache的盲目使用。否则，脏数据的出现会给系统的正常运行带来很大的隐患。 Hibernate的内置和外置缓存内置缓存中存放的是SessionFactory对象的一些集合属性包含的数据(映射元素据及预定SQL语句等),对于应用程序来说,它是只读的。 外置缓存中存放的是数据库数据的副本,其作用和一级缓存类似.二级缓存除了以内存作为存储介质外,还可以选用硬盘等外部存储设备。二级缓存称为进程级缓存或SessionFactory级缓存，它可以被所有session共享，它的生命周期伴随着SessionFactory的生命周期存在和消亡。 MyBatis的几个关键类 MyBatis的执行浅析 参考文章： MyBatis原理深入解析 MyBatis框架整体设计 MyBatis初始化与执行sql过程 MyBatis源码的几个主要部件 XML 映射配置文件 官方参考文档 自定义类型处理器使用这个的类型处理器将会覆盖已经存在的处理 Java 的 String 类型属性和 VARCHAR 参数及结果的类型处理器。 要注意 MyBatis 不会窥探数据库元信息来决定使用哪种类型，所以你必须在参数和结果映射中指明那是 VARCHAR 类型的字段， 以使其能够绑定到正确的类型处理器上。 这是因为：MyBatis 直到语句被执行才清楚数据类型。 通过类型处理器的泛型，MyBatis 可以得知该类型处理器处理的 Java 类型，不过这种行为可以通过两种方法改变： 在类型处理器的配置元素（typeHandler element）上增加一个 javaType 属性（比如：javaType=”String”）；在类型处理器的类上（TypeHandler class）增加一个 @MappedTypes 注解来指定与其关联的 Java 类型列表。 如果在 javaType 属性中也同时指定，则注解方式将被忽略。可以通过两种方式来指定被关联的 JDBC 类型： 在类型处理器的配置元素上增加一个 jdbcType 属性（比如：jdbcType=”VARCHAR”）； 在类型处理器的类上（TypeHandler class）增加一个 @MappedJdbcTypes 注解来指定与其关联的 JDBC 类型列表。 如果在 jdbcType 属性中也同时指定，则注解方式将被忽略。 当决定在ResultMap中使用某一TypeHandler时，此时java类型是已知的（从结果类型中获得），但是JDBC类型是未知的。 因此Mybatis使用javaType=[TheJavaType], jdbcType=null的组合来选择一个TypeHandler。 这意味着使用@MappedJdbcTypes注解可以限制TypeHandler的范围，同时除非显示的设置，否则TypeHandler在ResultMap中将是无效的。 如果希望在ResultMap中使用TypeHandler，那么设置@MappedJdbcTypes注解的includeNullJdbcType=true即可。 然而从Mybatis 3.4.0开始，如果只有一个注册的TypeHandler来处理Java类型，那么它将是ResultMap使用Java类型时的默认值（即使没有includeNullJdbcType=true）。 最后，可以让 MyBatis 为你查找类型处理器： 1234&lt;!-- mybatis-config.xml --&gt;&lt;typeHandlers&gt; &lt;package name="org.mybatis.example"/&gt;&lt;/typeHandlers&gt; 注意在使用自动检索（autodiscovery）功能的时候，只能通过注解方式来指定 JDBC 的类型。 你能创建一个泛型类型处理器，它可以处理多于一个类。为达到此目的， 需要增加一个接收该类作为参数的构造器，这样在构造一个类型处理器的时候 MyBatis 就会传入一个具体的类。 12345678910// GenericTypeHandler.javapublic class GenericTypeHandler&lt;E extends MyObject&gt; extends BaseTypeHandler&lt;E&gt; &#123; private Class&lt;E&gt; type; public GenericTypeHandler(Class&lt;E&gt; type) &#123; if (type == null) throw new IllegalArgumentException("Type argument cannot be null"); this.type = type; &#125; ... 处理枚举类型若想映射枚举类型 Enum，则需要从 EnumTypeHandler 或者 EnumOrdinalTypeHandler 中选一个来使用。 比如说我们想存储取近似值时用到的舍入模式。默认情况下，MyBatis 会利用 EnumTypeHandler 来把 Enum 值转换成对应的名字。 注意 EnumTypeHandler 在某种意义上来说是比较特别的，其他的处理器只针对某个特定的类，而它不同，它会处理任意继承了 Enum 的类。不过，我们可能不想存储名字，相反我们的 DBA 会坚持使用整形值代码。那也一样轻而易举： 在配置文件中把 EnumOrdinalTypeHandler 加到 typeHandlers 中即可， 这样每个 RoundingMode 将通过他们的序数值来映射成对应的整形。 1234&lt;!-- mybatis-config.xml --&gt;&lt;typeHandlers&gt; &lt;typeHandler handler="org.apache.ibatis.type.EnumOrdinalTypeHandler" javaType="java.math.RoundingMode"/&gt;&lt;/typeHandlers&gt; 但是怎样能将同样的 Enum 既映射成字符串又映射成整形呢？ 自动映射器（auto-mapper）会自动地选用 EnumOrdinalTypeHandler 来处理， 所以如果我们想用普通的 EnumTypeHandler，就非要为那些 SQL 语句显式地设置要用到的类型处理器不可。 databaseIdProviderMyBatis 可以根据不同的数据库厂商执行不同的语句，这种多厂商的支持是基于映射语句中的 databaseId 属性。 MyBatis 会加载不带 databaseId 属性和带有匹配当前数据库 databaseId 属性的所有语句。 如果同时找到带有 databaseId 和不带 databaseId 的相同语句，则后者会被舍弃。 为支持多厂商特性只要像下面这样在 mybatis-config.xml 文件中加入 databaseIdProvider 即可： 1&lt;databaseIdProvider type="DB_VENDOR" /&gt; 这里的 DB_VENDOR 会通过 DatabaseMetaData#getDatabaseProductName() 返回的字符串进行设置。 由于通常情况下这个字符串都非常长而且相同产品的不同版本会返回不同的值，所以最好通过设置属性别名来使其变短，如下： 12345&lt;databaseIdProvider type="DB_VENDOR"&gt; &lt;property name="SQL Server" value="sqlserver"/&gt; &lt;property name="DB2" value="db2"/&gt; &lt;property name="Oracle" value="oracle" /&gt;&lt;/databaseIdProvider&gt; 在有 properties 时，DB_VENDOR databaseIdProvider 的将被设置为第一个能匹配数据库产品名称的属性键对应的值，如果没有匹配的属性将会设置为 “null”。 在这个例子中，如果 getDatabaseProductName() 返回“Oracle (DataDirect)”，databaseId 将被设置为“oracle”。 你可以通过实现接口 org.apache.ibatis.mapping.DatabaseIdProvider 并在 mybatis-config.xml 中注册来构建自己的 DatabaseIdProvider： 1234public interface DatabaseIdProvider &#123; void setProperties(Properties p); String getDatabaseId(DataSource dataSource) throws SQLException;&#125; dataSource数据源（dataSource） dataSource 元素使用标准的 JDBC 数据源接口来配置 JDBC 连接对象的资源。 许多 MyBatis 的应用程序将会按示例中的例子来配置数据源。然而它并不是必须的。要知道为了方便使用延迟加载，数据源才是必须的。有三种内建的数据源类型（也就是 type=”[UNPOOLED|POOLED|JNDI]”）： UNPOOLED– 这个数据源的实现只是每次被请求时打开和关闭连接。虽然一点慢，它对在及时可用连接方面没有性能要求的简单应用程序是一个很好的选择。 不同的数据库在这方面表现也是不一样的，所以对某些数据库来说使用连接池并不重要，这个配置也是理想的。UNPOOLED 类型的数据源仅仅需要配置以下 5 种属性： driver – 这是 JDBC 驱动的 Java 类的完全限定名（并不是JDBC驱动中可能包含的数据源类）。 url – 这是数据库的 JDBC URL 地址。 username – 登录数据库的用户名。 password – 登录数据库的密码。 defaultTransactionIsolationLevel – 默认的连接事务隔离级别。 作为可选项，你也可以传递属性给数据库驱动。要这样做，属性的前缀为“driver.”，例如： 1driver.encoding=UTF8 这将通过DriverManager.getConnection(url,driverProperties)方法传递值为 UTF8 的 encoding 属性给数据库驱动。 POOLED– 这种数据源的实现利用“池”的概念将 JDBC 连接对象组织起来，避免了创建新的连接实例时所必需的初始化和认证时间。 这是一种使得并发 Web 应用快速响应请求的流行处理方式。 除了上述提到 UNPOOLED 下的属性外，会有更多属性用来配置 POOLED 的数据源： poolMaximumActiveConnections – 在任意时间可以存在的活动（也就是正在使用）连接数量，默认值：10poolMaximumIdleConnections – 任意时间可能存在的空闲连接数。poolMaximumCheckoutTime – 在被强制返回之前，池中连接被检出（checked out）时间，默认值：20000 毫秒（即 20 秒）poolTimeToWait – 这是一个底层设置，如果获取连接花费的相当长的时间，它会给连接池打印状态日志并重新尝试获取一个连接（避免在误配置的情况下一直安静的失败），默认值：20000 毫秒（即 20 秒）。poolPingQuery – 发送到数据库的侦测查询，用来检验连接是否处在正常工作秩序中并准备接受请求。默认是“NO PING QUERY SET”，这会导致多数数据库驱动失败时带有一个恰当的错误消息。poolPingEnabled – 是否启用侦测查询。若开启，也必须使用一个可执行的 SQL 语句设置 poolPingQuery 属性（最好是一个非常快的 SQL），默认值：false。poolPingConnectionsNotUsedFor – 配置 poolPingQuery 的使用频度。这可以被设置成匹配具体的数据库连接超时时间，来避免不必要的侦测，默认值：0（即所有连接每一时刻都被侦测 — 当然仅当 poolPingEnabled 为 true 时适用）。JNDI– 这个数据源的实现是为了能在如 EJB 或应用服务器这类容器中使用，容器可以集中或在外部配置数据源，然后放置一个 JNDI 上下文的引用。这种数据源配置只需要两个属性： initial_context – 这个属性用来在 InitialContext 中寻找上下文（即，initialContext.lookup(initial_context)）。这是个可选属性，如果忽略，那么 data_source 属性将会直接从 InitialContext 中寻找。data_source – 这是引用数据源实例位置的上下文的路径。提供了 initial_context 配置时会在其返回的上下文中进行查找，没有提供时则直接在 InitialContext 中查找。和其他数据源配置类似，可以通过添加前缀“env.”直接把属性传递给初始上下文。比如： env.encoding=UTF8这就会在初始上下文（InitialContext）实例化时往它的构造方法传递值为 UTF8 的 encoding 属性。 通过需要实现接口 org.apache.ibatis.datasource.DataSourceFactory ， 也可使用任何第三方数据源，： 1234public interface DataSourceFactory &#123; void setProperties(Properties props); DataSource getDataSource();&#125; org.apache.ibatis.datasource.unpooled.UnpooledDataSourceFactory 可被用作父类来构建新的数据源适配器，比如下面这段插入 C3P0 数据源所必需的代码： 12345678import org.apache.ibatis.datasource.unpooled.UnpooledDataSourceFactory;import com.mchange.v2.c3p0.ComboPooledDataSource; public class C3P0DataSourceFactory extends UnpooledDataSourceFactory &#123; public C3P0DataSourceFactory() &#123; this.dataSource = new ComboPooledDataSource(); &#125;&#125; 为了令其工作，为每个需要 MyBatis 调用的 setter 方法中增加一个属性。下面是一个可以连接至 PostgreSQL 数据库的例子： 123456&lt;dataSource type="org.myproject.C3P0DataSourceFactory"&gt; &lt;property name="driver" value="org.postgresql.Driver"/&gt; &lt;property name="url" value="jdbc:postgresql:mydb"/&gt; &lt;property name="username" value="postgres"/&gt; &lt;property name="password" value="root"/&gt;&lt;/dataSource&gt;]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis在Linux上的部署和Jedis简单使用]]></title>
    <url>%2Fblog%2F2017%2F03%2F18%2F2017-03-18-Redis%E5%9C%A8Linux%E4%B8%8A%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8CJedis%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、redis的安装这里演示的版本是Redis4.0，Linux系统是CentOS6.7，Jdk1.7，Jedis2.8 这是官方文档介绍的安装方式 下载，解压，编译: 1234$ wget http://download.redis.io/releases/redis-4.0.0.tar.gz$ tar xzf redis-4.0.0.tar.gz$ cd redis-4.0.0$ make 二进制文件是编译完成后在src目录下，通过下面的命令启动Redis服务： 1$ src/redis-server 你可以使用内置的客户端命令redis-cli进行使用： 12345$ src/redis-cliredis&gt; set foo barOKredis&gt; get foo"bar" 本人不建议直接使用源码文件中的服务，make编译完成后，可以安装到指定目录： 1make PREFIX=/usr/local/redis install 现在去刚刚tar包解压出来的源码目录中，拷贝一个redis.conf配置文件，放到/usr/local/redis/bin/目录下，以后在这个目录下使用就好了。 启动服务端（暂时不使用自己刚才复制过来的redis.conf配置文件，配置文件是可以指定的） 1./redis-server 复制一个会话窗口，启动客户端（暂时不设置ip，端口号和密码） 1./redis-cli 测试下： 123127.0.0.1:6379&gt; pingPONG# 客户端启动成功 二、Java程序中jedis操作redis上面的方式只是一种小练习，我们现在通过Java程序用jedis来操作Linux服务器上的redis。 用maven来引入jedis： 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.8.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Java代码： 1234567public static void main(String[] args) &#123; // 服务器ip，redis默认端口号 Jedis jedis = new Jedis("192.168.133.128", 6379); jedis.set("key01", "zhangsan"); jedis.set("key02", "lisi"); System.out.println(jedis.get("key01"));&#125; 注意上面的代码是有问题的！ 三、redis配置文件上面的代码运行后，会报错 redis.clients.jedis.exceptions.JedisConnectionException: java.net.ConnectException: Connection refused: connect 连接超被拒绝了，这是因为，redis的访问ip默认是127.0.0.1 你需要在自己拷贝的redis.conf配置文件中修改： 把绑定的主机ip添加进去，之后启动redis服务的时候，需要手动加载配置文件 我的配置文件放在了和server服务的同一个目录里，所以启动服务时输入： 1./redis-server redis.conf 注意啊：如果不输入后面的配置文件目录，那么该配置文件不起作用，会提示说启动默认的配置文件。 之后再次运行Java代码 又报错！！ redis.clients.jedis.exceptions.JedisDataException: DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command &#39;CONFIG SET protected-mode no&#39; from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to &#39;no&#39;, and then restarting the server. 3) If you started the server manually just for testing, restart it with the &#39;--protected-mode no&#39; option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside. 这错报的好长。。 好心的博主帮你谷歌翻译了一下。 简单来说呢？就是给你提供了几个解决方案 1）只需禁用保护模式，即可通过从同一主机连接到Redis，从回送接口发送命令“CONFIG SET protected-mode no”正在运行，但是如果您这样做，请勿使用互联网公开访问互联网。使用CONFIG REWRITE使此更改永久。 2）或者，您可以通过编辑Redis配置文件并将protected mode选项设置为“no”来禁用保护模式，然后重新启动服务器。 3）如果您只是为了测试而手动启动服务器，请使用“ –protected-mode no”选项重新启动服务器。 4）设置绑定地址或认证密码。 这是redis4.0版本的新特性，redis3不会报错。 在这里我选择设置redis密码，同样打开redis.conf配置文件，设置密码为123456，保存退出 然后启动服务器 1./redis-server redis.conf 以后如果你要想在Linux里用命令打开redis客户端，就需要输入一些参数才行 1./redis-cli -h 192.168.133.128 -p 6379 -a 123456 很显然，-h是redis服务绑定的主机ip，-p是redis服务的端口号，-a是redis服务的密码，都可以在redis.conf里更改的 然后就好了 这个时候，Java代码中的问题还没解决完，运行代码还会报错的，没有访问权限 redis.clients.jedis.exceptions.JedisDataException: NOAUTH Authentication required. 你还需要在Java代码中增加一条密码设置 123456789public static void main(String[] args) &#123; // 服务器ip，redis默认端口号 Jedis jedis = new Jedis("192.168.133.128", 6379); // redis访问密码 jedis.auth("123456"); jedis.set("key01", "zhangsan"); jedis.set("key02", "lisi"); System.out.println(jedis.get("key01"));&#125; OK，运行正常 四、其他分享一个redis详细学习教程的网址： http://www.runoob.com/redis/redis-intro.html redis.conf 配置项说明如下： 1.Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程，可以后台启动，用ps -ef | grep redis查看redis进程 daemonize no 2.当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定 pidfile /var/run/redis.pid 3.指定Redis监听端口，默认端口为6379，作者在自己的一篇博文中解释了为什么选用6379作为默认端口，因为6379在手机按键上MERZ对应的号码，而MERZ取自意大利歌女Alessia Merz的名字 port 6379 4.绑定的主机地址 bind 127.0.0.1 5.当客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能 timeout 300 6.指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose loglevel verbose 7.日志记录方式，默认为标准输出，如果配置Redis为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给/dev/null logfile stdout 8.设置数据库的数量，默认数据库为0，可以使用SELECT \命令在连接上指定数据库id databases 16 9.指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合 save \ \ Redis默认配置文件中提供了三个条件： save 900 1 save 300 10 save 60 10000 分别表示900秒（15分钟）内有1个更改，300秒（5分钟）内有10个更改以及60秒内有10000个更改。 10.指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大 rdbcompression yes 11.指定本地数据库文件名，默认值为dump.rdb dbfilename dump.rdb 12.指定本地数据库存放目录 dir ./ 13.设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步 slaveof \ \ 14.当master服务设置了密码保护时，slav服务连接master的密码 masterauth 15.设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过AUTH \命令提供密码，默认关闭 requirepass foobared 16.设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息 maxclients 128 17.指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，当此方法处理 后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis新的vm机制，会把Key存放内存，Value会存放在swap区 maxmemory \ 18.指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no appendonly no 19.指定更新日志文件名，默认为appendonly.aof appendfilename appendonly.aof 20.指定更新日志条件，共有3个可选值： no：表示等操作系统进行数据缓存同步到磁盘（快）always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全）everysec：表示每秒同步一次（折衷，默认值） appendfsync everysec 21.指定是否启用虚拟内存机制，默认值为no，简单的介绍一下，VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中（在后面的文章我会仔细分析Redis的VM机制） vm-enabled no 22.虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享 vm-swap-file /tmp/redis.swap 23.将所有大于vm-max-memory的数据存入虚拟内存,无论vm-max-memory设置多小,所有索引数据都是内存存储的(Redis的索引数据 就是keys),也就是说,当vm-max-memory设置为0的时候,其实是所有value都存在于磁盘。默认值为0 vm-max-memory 0 24.Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page大小最好设置为32或者64bytes；如果存储很大大对象，则可以使用更大的page，如果不 确定，就使用默认值 vm-page-size 32 25.设置swap文件中的page数量，由于页表（一种表示页面空闲或使用的bitmap）是在放在内存中的，在磁盘上每8个pages将消耗1byte的内存。 vm-pages 134217728 26.设置访问swap文件的线程数,最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为4 vm-max-threads 4 27.设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启 glueoutputbuf yes 28.指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法 hash-max-zipmap-entries 64 hash-max-zipmap-value 512 29.指定是否激活重置哈希，默认为开启（后面在介绍Redis的哈希算法时具体介绍） activerehashing yes 30.指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件 include /path/to/local.conf]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx学习笔记]]></title>
    <url>%2Fblog%2F2017%2F01%2F10%2F2017-01-10-Nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[前言Nginx是一款轻量级的Web服务器、反向代理服务器，由于它的内存占用少，启动极快，高并发能力强，访问静态资源速度快，在互联网项目中广泛应用。 上图基本上说明了当下流行的技术架构，其中Nginx有点入口网关的味道。 反向代理服务器经常听人说到一些术语，如反向代理，那么什么是反向代理，什么又是正向代理呢？ 由于防火墙的原因，我们并不能直接访问谷歌，那么我们可以借助VPN来实现，这就是一个简单的正向代理的例子。这里你能够发现，正向代理“代理”的是客户端，而且客户端是知道目标的，而目标是不知道客户端是通过VPN访问的。 当我们在外网访问百度的时候，其实会进行一个转发，代理到内网去，这就是所谓的反向代理，即反向代理“代理”的是服务器端，而且这一个过程对于客户端而言是透明的。 Nginx的Master-Worker模式 启动Nginx后，其实就是在80端口启动了Socket服务进行监听，如图所示，Nginx涉及Master进程和Worker进程。 Master进程的作用是？读取并验证配置文件nginx.conf；管理worker进程； Worker进程的作用是？每一个Worker进程都维护一个线程（避免线程切换），处理连接和请求；注意Worker进程的个数由配置文件决定，一般和CPU个数相关（有利于进程切换），配置几个就有几个Worker进程。 Nginx如何做到热部署？所谓热部署，就是配置文件nginx.conf修改后，不需要stop Nginx，不需要中断请求，就能让配置文件生效！（nginx -s reload 重新加载/nginx -t检查配置/nginx -s stop强行停止）通过上文我们已经知道worker进程负责处理具体的请求，那么如果想达到热部署的效果，可以想象： 方案一：修改配置文件nginx.conf后，主进程master负责推送给woker进程更新配置信息，woker进程收到信息后，更新进程内部的线程信息。（有点valatile的味道） 方案二：修改配置文件nginx.conf后，重新生成新的worker进程，当然会以新的配置进行处理请求，而且新的请求必须都交给新的worker进程，至于老的worker进程，等把那些以前的请求处理完毕后，kill掉即可。 Nginx采用的就是方案二来达到热部署的！ Nginx如何做到高并发下的高效处理？上文已经提及Nginx的worker进程个数与CPU绑定、worker进程内部包含一个线程高效回环处理请求，这的确有助于效率，但这是不够的。作为专业的程序员，我们可以开一下脑洞：BIO/NIO/AIO、异步/同步、阻塞/非阻塞… 要同时处理那么多的请求，要知道，有的请求需要发生IO，可能需要很长时间，如果等着它，就会拖慢worker的处理速度。 Nginx采用了Linux的epoll模型，epoll模型基于事件驱动机制，它可以监控多个事件是否准备完毕，如果OK，那么放入epoll队列中，这个过程是异步的。worker只需要从epoll队列循环处理即可 Nginx挂了怎么办？Nginx既然作为入口网关，很重要，如果出现单点问题，显然是不可接受的。 答案是：Keepalived+Nginx实现高可用。 Keepalived是一个高可用解决方案，主要是用来防止服务器单点发生故障，可以通过和Nginx配合来实现Web服务的高可用。（其实，Keepalived不仅仅可以和Nginx配合，还可以和很多其他服务配合） Keepalived+Nginx实现高可用的思路：第一：请求不要直接打到Nginx上，应该先通过Keepalived（这就是所谓虚拟IP，VIP）第二：Keepalived应该能监控Nginx的生命状态（提供一个用户自定义的脚本，定期检查Nginx进程状态，进行权重变化,，从而实现Nginx故障切换） 我们的主战场：nginx.conf很多时候，在开发、测试环境下，我们都得自己去配置Nginx，就是去配置nginx.conf。 nginx.conf是典型的分段配置文件，下面我们来分析下。 虚拟主机 其实这是把Nginx作为web server来处理静态资源。 第一：location可以进行正则匹配，应该注意正则的几种形式以及优先级。从高到低分别为：确切的名字、最长的以*起始的通配符名字、最长的以*结束的通配符名字、第一个匹配的正则表达式名字 第二：Nginx能够提高速度的其中一个特性就是：动静分离，就是把静态资源放到Nginx上，由Nginx管理，动态请求转发给后端。 第三：我们可以在Nginx下把静态资源、日志文件归属到不同域名下（也即是目录），这样方便管理维护。 第四：Nginx可以进行IP访问控制，有些电商平台，就可以在Nginx这一层，做一下处理，内置一个黑名单模块，那么就不必等请求通过Nginx达到后端在进行拦截，而是直接在Nginx这一层就处理掉。 反向代理【proxy_pass】所谓反向代理，很简单，其实就是在location这一段配置中的root替换成proxy_pass即可。root说明是静态资源，可以由Nginx进行返回；而proxy_pass说明是动态请求，需要进行转发，比如代理到Tomcat上。 反向代理，上面已经说了，过程是透明的，比如说request -&gt; Nginx -&gt; Tomcat，那么对于Tomcat而言，请求的IP地址就是Nginx的地址，而非真实的request地址，这一点需要注意。不过好在Nginx不仅仅可以反向代理请求，还可以由用户自定义设置HTTP HEADER。 负载均衡【upstream】上面的反向代理中，我们通过proxy_pass来指定Tomcat的地址，很显然我们只能指定一台Tomcat地址，那么我们如果想指定多台来达到负载均衡呢？ 第一，通过upstream来定义一组Tomcat，并指定负载策略（IPHASH、加权论调、最少连接），健康检查策略（Nginx可以监控这一组Tomcat的状态）等。第二，将proxy_pass替换成upstream指定的值即可。 负载均衡可能带来的问题？负载均衡所带来的明显的问题是，一个请求，可以到A server，也可以到B server，这完全不受我们的控制，当然这也不是什么问题，只是我们得注意的是：用户状态的保存问题，如Session会话信息，不能在保存到服务器上。一般是通过redis进行session共享。 缓存缓存，是Nginx提供的，可以加快访问速度的机制，说白了，在配置上就是一个开启，同时指定目录，让缓存可以存储到磁盘上。具体配置，大家可以参考Nginx官方文档，这里就不在展开了。]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java研发知识点总结]]></title>
    <url>%2Fblog%2F2016%2F12%2F24%2F2016-12-24-Java%E7%A0%94%E5%8F%91%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[一、Java基础（语言、集合框架、OOP、设计模式等）1. HashMap和Hashtable的区别 Hashtable是基于陈旧的Dictionary的Map接口的实现，而HashMap是基于哈希表的Map接口的实现 从方法上看，HashMap去掉了Hashtable的contains方法 HashTable是同步的(线程安全)，而HashMap线程不安全，效率上HashMap更快 HashMap允许空键值，而Hashtable不允许 HashMap的iterator迭代器执行快速失败机制，也就是说在迭代过程中修改集合结构，除非调用迭代器自身的remove方法，否则以其他任何方式的修改都将抛出并发修改异常。而Hashtable返回的Enumeration不是快速失败的。 注：Fast-fail机制:在使用迭代器的过程中有其它线程修改了集合对象结构或元素数量,都将抛出ConcurrentModifiedException，但是抛出这个异常是不保证的，我们不能编写依赖于此异常的程序。 2. java的线程安全Vector、Stack、HashTable、ConcurrentHashMap、Properties 3. java集合框架(常用)123456789101112Collection - List - ArrayListCollection - List - LinkedListCollection - List - VectorCollection - List - Vector - StackCollection - Set - HashSetCollection - Set - TreeSetCollection - List - LinkedHashSetMap - HashMapMap - TreeMapMap - HashTableMap - LinkedHashMapMap - ConcurrentHashMap 3.1 List集合和Set集合 List中元素存取是有序的、可重复的；Set集合中元素是无序的，不可重复的。CopyOnWriteArrayList:COW的策略，即写时复制的策略。适用于读多写少的并发场景Set集合元素存取无序，且元素不可重复。HashSet不保证迭代顺序，线程不安全；LinkedHashSet是Set接口的哈希表和链接列表的实现，保证迭代顺序，线程不安全。TreeSet：可以对Set集合中的元素排序，元素以二叉树形式存放，线程不安全。 3.2 ArrayList、LinkedList、Vector的区别 首先它们均是List接口的实现。 ArrayList、LinkedList的区别：1.随机存取：ArrayList是基于可变大小的数组实现，LinkedList是链接列表的实现。这也就决定了对于随机访问的get和set的操作，ArrayList要优于LinkedList，因为LinkedList要移动指针。2.插入和删除：LinkedList要好一些，因为ArrayList要移动数据，更新索引。3.内存消耗：LinkedList需要更多的内存，因为需要维护指向后继结点的指针。 Vector从JDK 1.0起就存在，在1.2时改为实现List接口，功能与ArrayList类似，但是Vector具备线程安全。 3.3 Map集合 Hashtable:基于Dictionary类，线程安全，速度快。底层是哈希表数据结构。是同步的。不允许null作为键，null作为值。Properties:Hashtable的子类。用于配置文件的定义和操作，使用频率非常高，同时键和值都是字符串。HashMap：线程不安全，底层是数组加链表实现的哈希表。允许null作为键，null作为值。HashMap去掉了contains方法。注意：HashMap不保证元素的迭代顺序。如果需要元素存取有序，请使用LinkedHashMapTreeMap：可以用来对Map集合中的键进行排序。ConcurrentHashMap:是JUC包下的一个并发集合。 3.4 为什么使用ConcurrentHashMap而不是HashMap或Hashtable？ HashMap的缺点：主要是多线程同时put时，如果同时触发了rehash操作，会导致HashMap中的链表中出现循环节点，进而使得后面get的时候，会死循环，CPU达到100%，所以在并发情况下不能使用HashMap。让HashMap同步：Map m = Collections.synchronizeMap(hashMap);而Hashtable虽然是同步的，使用synchronized来保证线程安全，但在线程竞争激烈的情况下HashTable的效率非常低下。因为当一个线程访问HashTable的同步方法时，其他线程访问HashTable的同步方法时，可能会进入阻塞或轮询状态。如线程1使用put进行添加元素，线程2不但不能使用put方法添加元素，并且也不能使用get方法来获取元素，所以竞争越激烈效率越低。 ConcurrentHashMap的原理： HashTable容器在竞争激烈的并发环境下表现出效率低下的原因在于所有访问HashTable的线程都必须竞争同一把锁，那假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术，首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 ConcurrentHashMap的结构： ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入互斥锁ReentrantLock，在ConcurrentHashMap里扮演锁的角色，HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构， 一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，当对某个HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。 ConcurrentHashMap的构造、get、put操作： 构造函数：传入参数分别为 1、初始容量，默认16 2、装载因子 装载因子用于rehash的判定，就是当ConcurrentHashMap中的元素大于装载因子乘以最大容量时进行扩容，默认0.75 3、并发级别 这个值用来确定Segment的个数，Segment的个数是大于等于concurrencyLevel的第一个2的n次方的数。比如，如果concurrencyLevel为12，13，14，15，16这些数，则Segment的数目为16(2的4次方)。默认值为static final int DEFAULT_CONCURRENCY_LEVEL = 16;。理想情况下ConcurrentHashMap的真正的并发访问量能够达到concurrencyLevel，因为有concurrencyLevel个Segment，假如有concurrencyLevel个线程需要访问Map，并且需要访问的数据都恰好分别落在不同的Segment中，则这些线程能够无竞争地自由访问（因为他们不需要竞争同一把锁），达到同时访问的效果。这也是为什么这个参数起名为“并发级别”的原因。默认16. 初始化的一些动作： 初始化segments数组（根据并发级别得到数组大小ssize），默认16初始化segmentShift和segmentMask（这两个全局变量在定位segment时的哈希算法里需要使用），默认情况下segmentShift为28，segmentMask为15初始化每个Segment，这一步会确定Segment里HashEntry数组的长度. put操作： 1、判断value是否为null，如果为null，直接抛出异常。2、key通过一次hash运算得到一个hash值。将得到hash值向右按位移动segmentShift位，然后再与segmentMask做&amp;运算得到segment的索引j。即segmentFor方法3、使用Unsafe的方式从Segment数组中获取该索引对应的Segment对象。向这个Segment对象中put值，这个put操作也基本是一样的步骤（通过&amp;运算获取HashEntry的索引，然后set）。 get操作： 1、和put操作一样，先通过key进行hash确定应该去哪个Segment中取数据。2、使用Unsafe获取对应的Segment，然后再进行一次&amp;运算得到HashEntry链表的位置，然后从链表头开始遍历整个链表（因为Hash可能会有碰撞，所以用一个链表保存），如果找到对应的key，则返回对应的value值，如果链表遍历完都没有找到对应的key，则说明Map中不包含该key，返回null。 定位Segment的hash算法：(hash &gt;&gt;&gt; segmentShift) &amp; segmentMask定位HashEntry所使用的hash算法：int index = hash &amp; (tab.length - 1); 注： 1.tab为HashEntry数组2.ConcurrentHashMap既不允许null key也不允许null value 3.5 Collection 和 Collections的区别 Collection是集合类的上级接口，子接口主要有Set 和List、QueueCollections是针对集合类的一个辅助类，提供了操作集合的工具方法：一系列静态方法实现对各种集合的搜索、排序、线程安全化等操作。 3.6 Map、Set、List、Queue、Stack的特点与用法 Set集合类似于一个罐子，”丢进”Set集合里的多个对象之间没有明显的顺序。 List集合代表元素有序、可重复的集合，集合中每个元素都有其对应的顺序索引。 Stack是Vector提供的一个子类，用于模拟”栈”这种数据结构(LIFO后进先出) Queue用于模拟”队列”这种数据结构(先进先出 FIFO)。 Map用于保存具有”映射关系”的数据，因此Map集合里保存着两组值。 3.7 HashMap的工作原理 HashMap维护了一个Entry数组，Entry内部类有key,value，hash和next四个字段，其中next也是一个Entry类型。可以将Entry数组理解为一个个的散列桶。每一个桶实际上是一个单链表。当执行put操作时，会根据key的hashcode定位到相应的桶。遍历单链表检查该key是否已经存在，如果存在，覆盖该value，反之，新建一个新的Entry，并放在单链表的头部。当通过传递key调用get方法时，它再次使用key.hashCode()来找到相应的散列桶，然后使用key.equals()方法找出单链表中正确的Entry，然后返回它的值。 3.8 Map的实现类的介绍 HashMap基于散列表来的实现，即使用hashCode()进行快速查询元素的位置，显著提高性能。插入和查询“键值对”的开销是固定的。可以通过设置容量和装载因子，以调整容器的性能。 LinkedHashMap, 类似于HashMap,但是迭代遍历它时，保证迭代的顺序是其插入的次序，因为它使用链表维护内部次序。此外可以在构造器中设定LinkedHashMap，使之采用LRU算法。使没有被访问过的元素或较少访问的元素出现在前面，访问过的或访问多的出现在后面。这对于需要定期清理元素以节省空间的程序员来说，此功能使得程序员很容易得以实现。 TreeMap, 是基于红黑树的实现。同时TreeMap实现了SortedMap接口，该接口可以确保键处于排序状态。所以查看“键”和“键值对”时，所有得到的结果都是经过排序的，次序由自然排序或提供的Comparator决定。SortedMap接口拥有其他额外的功能，如：返回当前Map使用的Comparator比较强，firstKey()，lastKey(),headMap(toKey),tailMap(fromKey)以及可以返回一个子树的subMap()方法等。 WeakHashMap，表示弱键映射，WeakHashMap 的工作与正常的 HashMap 类似，但是使用弱引用作为 key，意思就是当 key 对象没有任何引用时，key/value 将会被回收。 ConcurrentHashMap， 在HashMap基础上分段锁机制实现的线程安全的HashMap。 IdentityHashMap 使用==代替equals() 对“键”进行比较的散列映射。专为解决特殊问题而设计。 HashTable：基于Dictionary类的Map接口的实现，它是线程安全的。 3.9 LinkedList 和 PriorityQueue 的区别 它们均是Queue接口的实现。拥有FIFO的特点，它们的区别在于排序行为。LinkedList 支持双向列表操作，PriorityQueue 按优先级组织的队列，元素的出队次序由元素的自然排序或者由Comparator比较器指定。 3.10 线程安全的集合类。 Vector、Hashtable、Properties和Stack、ConcurrentHashMap 3.11 BlockingQueue Java.util.concurrent.BlockingQueue是一个队列，在进行获取元素时，它会等待队列变为非空；当在添加一个元素时，它会等待队列中的可用空间。BlockingQueue接口是Java集合框架的一部分，主要用于实现生产者-消费者模式。我们不需要担心等待生产者有可用的空间，或消费者有可用的对象，因为它都在BlockingQueue的实现类中被处理了。Java提供了集中BlockingQueue的实现，比如ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue,、SynchronousQueue等。 **3.12 如何对一组对象进行排序 如果需要对一个对象数组进行排序，我们可以使用Arrays.sort()方法。如果我们需要排序一个对象列表，我们可以使用Collections.sort()方法。排序时是默认根据元素的自然排序（使用Comparable）或使用Comparator外部比较器。Collections内部使用数组排序方法，所有它们两者都有相同的性能，只是Collections需要花时间将列表转换为数组。 4. ArrayList 无参构造 容量为10 ArrayList(Collections&lt;?extends E&gt; c)构造包含指定collection的元素的列表 ArrayList(int initialCapacity) 指定初始容量 5. final关键字final修饰的变量是常量，必须进行初始化，可以显示初始化，也可以通过构造进行初始化，如果不初始化编译会报错。 6. 接口与抽象类6.1 一个子类只能继承一个抽象类,但能实现多个接口6.2 抽象类可以有构造方法,接口没有构造方法6.3 抽象类可以有普通成员变量,接口没有普通成员变量6.4 抽象类和接口都可有静态成员变量,抽象类中静态成员变量访问类型任意，接口只能public static final(默认)6.5 抽象类可以没有抽象方法,抽象类可以有普通方法,接口中都是抽象方法6.6 抽象类可以有静态方法，接口不能有静态方法6.7 抽象类中的方法可以是public、protected;接口方法只有public abstract 7. 抽象类和最终类抽象类可以没有抽象方法, 最终类可以没有最终方法 最终类不能被继承, 最终方法不能被重写(可以重载) 8.异常相关的关键字 throw、throws、try…catch、finally throws 用在方法签名上, 以便抛出的异常可以被调用者处理 throw 方法内部通过throw抛出异常 try 用于检测包住的语句块, 若有异常, catch子句捕获并执行catch块 9. 关于finally finally不管有没有异常都要处理 当try和catch中有return时，finally仍然会执行，finally比return先执行 不管有木有异常抛出, finally在return返回前执行 finally是在return后面的表达式运算后执行的（此时并没有返回运算后的值，而是先把要返回的值保存起来，管finally中的代码怎么样，返回的值都不会改变，仍然是之前保存的值），所以函数返回值是在finally执行前确定的 注意：finally中最好不要包含return，否则程序会提前退出，返回值不是try或catch中保存的返回值 finally不执行的几种情况：程序提前终止如调用了System.exit, 病毒，断电 10. 受检查异常和运行时异常 10.1 粉红色的是受检查的异常(checked exceptions),其必须被try…catch语句块所捕获, 或者在方法签名里通过throws子句声明。受检查的异常必须在编译时被捕捉处理,命名为Checked Exception是因为Java编译器要进行检查, Java虚拟机也要进行检查, 以确保这个规则得到遵守。 常见的checked exception：ClassNotFoundException IOException FileNotFoundException EOFException 10.2 绿色的异常是运行时异常(runtime exceptions), 需要程序员自己分析代码决定是否捕获和处理,比如空指针,被0除… 常见的runtime exception：NullPointerException ArithmeticException ClassCastException IllegalArgumentException IllegalStateException IndexOutOfBoundsException NoSuchElementException 10.3 而声明为Error的，则属于严重错误，如系统崩溃、虚拟机错误、动态链接失败等，这些错误无法恢复或者不可能捕捉，将导致应用程序中断，Error不需要捕获。 11. this &amp; super11.1 super出现在父类的子类中。有三种存在方式 super.xxx(xxx为变量名或对象名)意思是获取父类中xxx的变量或引用 super.xxx(); (xxx为方法名)意思是直接访问并调用父类中的方法 super() 调用父类构造 注：super只能指代其直接父类 11.2 this() &amp; super()在构造方法中的区别 调用super()必须写在子类构造方法的第一行, 否则编译不通过 super从子类调用父类构造, this在同一类中调用其他构造 均需要放在第一行 尽管可以用this调用一个构造器, 却不能调用2个 this和super不能出现在同一个构造器中, 否则编译不通过 this()、super()都指的对象,不可以在static环境中使用 本质this指向本对象的指针。super是一个关键字 12. 修饰符一览12345修饰符 类内部 同一个包 子类 任何地方private yesdefault yes yesprotected yes yes yespublic yes yes yes yes 13. 构造内部类和静态内部类对象12345678910111213public class Enclosingone &#123; public class Insideone &#123;&#125; public static class Insideone&#123;&#125;&#125;public class Test &#123; public static void main(String[] args) &#123; // 构造内部类对象需要外部类的引用 Enclosingone.Insideone obj1 = new Enclosingone().new Insideone(); // 构造静态内部类的对象 Enclosingone.Insideone obj2 = new Enclosingone.Insideone(); &#125;&#125; 静态内部类不需要有指向外部类的引用。但非静态内部类需要持有对外部类的引用。非静态内部类能够访问外部类的静态和非静态成员。静态内部类不能访问外部类的非静态成员，只能访问外部类的静态成员。 14. 序列化声明为static和transient类型的数据不能被序列化， 反序列化需要一个无参构造函数 15.正则表达式次数符号12345* 0或多次+ 1或多次？0或1次&#123;n&#125; 恰n次&#123;n,m&#125; 从n到m次 其他符号 符号 等价形式1234567\d [0-9]\D [^0-9] \w [a-zA-Z_0-9]\W [^a-zA-Z_0-9]\s [\t\n\r\f]\S [^\t\n\r\f]. 任何字符 边界匹配器 行开头 ^行结尾 $单词边界 \b 贪婪模式:最大长度匹配 非贪婪模式:匹配到结果就好,最短匹配 环视 1234567891011121314字符 描述 匹配对象. 单个任意字符 [...] 字符组 列出的任意字符[^...] 未列出的任意字符^ caret 行的起始位置$ dollar 行的结束位置\&lt; 单词的起始位置\&gt; 单词的结束位置\b 单词边界\B 非单词边界(?=Expression) 顺序肯定环视 成功,如果右边能够匹配(?!Expression) 顺序否定环视 成功,如果右边不能够匹配(?&lt;=Expression) 逆序肯定环视 成功,如果左边能够匹配(?&lt;!Expression) 逆序否定环视 成功,如果左边不能够匹配 举例:北京市(海淀区)(朝阳区)(西城区) Regex: .*(?=\() 模式和匹配器的典型调用次序 把正则表达式编译到模式中Pattern p = Pattern.compile(“a*b”); 创建给定输入与此模式的匹配器Matcher m = p.matcher(“aaab”); 尝试将整个区域与此模式匹配boolean b = m.matches(); 16. 面向对象的五大基本原则(solid) S单一职责SRP:Single-Responsibility Principle一个类,最好只做一件事,只有一个引起它的变化。单一职责原则可以看做是低耦合,高内聚在面向对象原则的引申,将职责定义为引起变化的原因,以提高内聚性减少引起变化的原因。 O开放封闭原则OCP:Open-Closed Principle软件实体应该是可扩展的,而不是可修改的。对扩展开放,对修改封闭 L里氏替换原则LSP:Liskov-Substitution Principle子类必须能够替换其基类。这一思想表现为对继承机制的约束规范,只有子类能够替换其基类时,才能够保证系统在运行期内识别子类,这是保证继承复用的基础。 I接口隔离原则ISP:Interface-Segregation Principle使用多个小的接口,而不是一个大的总接口 D依赖倒置原则DIP:Dependency-Inversion Principle依赖于抽象。具体而言就是高层模块不依赖于底层模块,二者共同依赖于抽象。抽象不依赖于具体,具体依赖于抽象。 17. 面向对象设计其他原则 封装变化 少用继承 多用组合 针对接口编程 不针对实现编程 为交互对象之间的松耦合设计而努力 类应该对扩展开发 对修改封闭（开闭OCP原则） 依赖抽象，不要依赖于具体类（依赖倒置DIP原则） 密友原则：只和朋友交谈（最少知识原则，迪米特法则） 说明：一个对象应当对其他对象有尽可能少的了解，将方法调用保持在界限内，只调用属于以下范围的方法：该对象本身（本地方法）对象的组件 被当作方法参数传进来的对象 此方法创建或实例化的任何对象 别找我（调用我） 我会找你（调用你）（好莱坞原则） 一个类只有一个引起它变化的原因（单一职责SRP原则） 18. null可以被强制转型为任意类型的对象19.代码执行次序 多个静态成员变量, 静态代码块按顺序执行 单个类中: 静态代码 -&gt; main方法 -&gt; 构造代码块 -&gt; 构造方法 构造块在每一次创建对象时执行 静态代码只执行一次 涉及父类和子类的初始化过程a.初始化父类中的静态成员变量和静态代码块b.初始化子类中的静态成员变量和静态代码块c.初始化父类的普通成员变量和构造代码块(按次序)，再执行父类的构造方法(注意父类构造方法中的子类方法覆盖)d.初始化子类的普通成员变量和构造代码块(按次序)，再执行子类的构造方法 20. 数组复制方法 for逐一复制 System.arraycopy() -&gt; 效率最高native方法 Arrays.copyOf() -&gt; 本质调用arraycopy clone方法 -&gt; 返回Object[],需要强制类型转换 21. 多态 Java通过方法重写和方法重载实现多态 方法重写是指子类重写了父类的同名方法 方法重载是指在同一个类中，方法的名字相同，但是参数列表不同 22. Java文件.java文件可以包含多个类，唯一的限制就是：一个文件中只能有一个public类， 并且此public类必须与文件名相同。而且这些类和写在多个文件中没有区别。 23. Java移位运算符java中有三种移位运算符 &lt;&lt; :左移运算符,x &lt;&lt; 1,相当于x乘以2(不溢出的情况下),低位补0 &gt;&gt; :带符号右移,x &gt;&gt; 1,相当于x除以2,正数高位补0,负数高位补1 &gt;&gt;&gt; :无符号右移,忽略符号位,空位都以0补齐 24. 形参&amp;实参 形式参数可被视为local variable.形参和局部变量一样都不能离开方法。只有在方法中使用，不会在方法外可见。 形式参数只能用final修饰符，其它任何修饰符都会引起编译器错误。但是用这个修饰符也有一定的限制，就是在方法中不能对参数做任何修改。不过一般情况下，一个方法的形参不用final修饰。只有在特殊情况下，那就是：方法内部类。一个方法内的内部类如果使用了这个方法的参数或者局部变量的话，这个参数或局部变量应该是final。 形参的值在调用时根据调用者更改，实参则用自身的值更改形参的值（指针、引用皆在此列），也就是说真正被传递的是实参。 25. IO流一览 26. 局部变量为什么要初始化局部变量是指类方法中的变量，必须初始化。局部变量运行时被分配在栈中，量大，生命周期短，如果虚拟机给每个局部变量都初始化一下，是一笔很大的开销，但变量不初始化为默认值就使用是不安全的。出于速度和安全性两个方面的综合考虑，解决方案就是虚拟机不初始化，但要求编写者一定要在使用前给变量赋值。 27. Java语言的鲁棒性Java在编译和运行程序时，都要对可能出现的问题进行检查，以消除错误的产生。它提供自动垃圾收集来进行内存管理，防止程序员在管理内存时容易产生的错误。通过集成的面向对象的例外处理机制，在编译时，Java揭示出可能出现但未被处理的异常，帮助程序员正确地进行选择以防止系统的崩溃。另外，Java在编译时还可捕获类型声明中的许多常见错误，防止动态运行时不匹配问题的出现。 28. Java语言特性 Java致力于检查程序在编译和运行时的错误 Java虚拟机实现了跨平台接口 类型检查帮助检查出许多开发早期出现的错误 Java自己操纵内存减少了内存出错的可能性 Java还实现了真数组，避免了覆盖数据的可能 29. 包装类的equals()方法不处理数据转型，必须类型和值都一样才相等。30. 子类可以继承父类的静态方法！但是不能覆盖。因为静态方法是在编译时确定了，不能多态，也就是不能运行时绑定。31. Java语法糖 Java7的switch用字符串 - hashcode方法 switch用于enum枚举 伪泛型 - List原始类型 自动装箱拆箱 - Integer.valueOf和Integer.intValue foreach遍历 - Iterator迭代器实现 条件编译 enum枚举类、内部类 可变参数 - 数组 断言语言 try语句中定义和关闭资源 32. Java 中应该使用什么数据类型来代表价格？如果不是特别关心内存和性能的话，使用BigDecimal，否则使用预定义精度的 double 类型。 33. 怎么将 byte 转换为 String？可以使用 String 接收 byte[] 参数的构造器来进行转换，需要注意的点是要使用的正确的编码，否则会使用平台默认编码，这个编码可能跟原来的编码相同，也可能不同。 34. Java 中怎样将 bytes 转换为 long 类型？String接收bytes的构造器转成String，再Long.parseLong 35. 我们能将 int 强制转换为 byte 类型的变量吗？如果该值大于 byte 类型的范围，将会出现什么现象？是的，我们可以做强制转换，但是 Java 中 int 是 32 位的，而 byte 是 8 位的，所以，如果强制转化是，int 类型的高 24 位将会被丢弃，byte 类型的范围是从 -128 到 127。 36. 存在两个类，B 继承 A，C 继承 B，我们能将 B 转换为 C 么？如 C = (C) B；可以，向下转型。但是不建议使用，容易出现类型转型异常. 37. 哪个类包含 clone 方法？是 Cloneable 还是 Object？java.lang.Cloneable 是一个标示性接口，不包含任何方法，clone 方法在 object 类中定义。并且需要知道 clone() 方法是一个本地方法，这意味着它是由 c 或 c++ 或 其他本地语言实现的。 38. Java 中 ++ 操作符是线程安全的吗？不是线程安全的操作。它涉及到多个指令，如读取变量值，增加，然后存储回内存，这个过程可能会出现多个线程交差。还会存在竞态条件（读取-修改-写入）。 39. a = a + b 与 a += b 的区别+= 隐式的将加操作的结果类型强制转换为持有结果的类型。如果两这个整型相加，如 byte、short 或者 int，首先会将它们提升到 int 类型，然后在执行加法操作。 1234byte a = 127;byte b = 127;b = a + b; // error : cannot convert from int to byteb += a; // ok （因为 a+b 操作会将 a、b 提升为 int 类型，所以将 int 类型赋值给 byte 就会编译出错） 40. 我能在不进行强制转换的情况下将一个 double 值赋值给 long 类型的变量吗？不行，你不能在没有强制类型转换的前提下将一个 double 值赋值给 long 类型的变量，因为 double 类型的范围比 long 类型更广，所以必须要进行强制转换。 41. 3*0.1 == 0.3 将会返回什么？true 还是 false？false，因为有些浮点数不能完全精确的表示出来。 42. int 和 Integer 哪个会占用更多的内存？Integer 对象会占用更多的内存。Integer 是一个对象，需要存储对象的元数据。但是 int 是一个原始类型的数据，所以占用的空间更少。 43. 为什么 Java 中的 String 是不可变的（Immutable）？Java 中的 String 不可变是因为 Java 的设计者认为字符串使用非常频繁，将字符串设置为不可变可以允许多个客户端之间共享相同的字符串。而且是为了安全和效率的考虑。 44. 我们能在 Switch 中使用 String 吗？从 Java 7 开始，我们可以在 switch case 中使用字符串，但这仅仅是一个语法糖。内部实现在 switch 中使用字符串的 hash code。 45. Java 中的构造器链是什么？当你从一个构造器中调用另一个构造器，就是Java 中的构造器链。这种情况只在重载了类的构造器的时候才会出现。 46. 枚举类JDK1.5出现 每个枚举值都需要调用一次构造函数 47. 什么是不可变对象（immutable object）？不可变对象指对象一旦被创建，状态就不能再改变。任何修改都会创建一个新的对象，如 String、Integer及其它包装类。 48. Java 中怎么创建一个不可变对象？如何在Java中写出Immutable的类？ 要写出这样的类，需要遵循以下几个原则： 1）immutable对象的状态在创建之后就不能发生改变，任何对它的改变都应该产生一个新的对象。 2）Immutable类的所有的属性都应该是final的。 3）对象必须被正确的创建，比如：对象引用在对象创建过程中不能泄露(leak)。 4）对象应该是final的，以此来限制子类继承父类，以避免子类改变了父类的immutable特性。 5）如果类中包含mutable类对象，那么返回给客户端的时候，返回该对象的一个拷贝，而不是该对象本身（该条可以归为第一条中的一个特例） 49. 我们能创建一个包含可变对象的不可变对象吗？是的，我们是可以创建一个包含可变对象的不可变对象的，你只需要谨慎一点，不要共享可变对象的引用就可以了，如果需要变化时，就返回原对象的一个拷贝。最常见的例子就是对象中包含一个日期对象的引用。 50. List和SetList 是一个有序集合，允许元素重复。它的某些实现可以提供基于下标值的常量访问时间，但是这不是 List 接口保证的。Set 是一个无序集合。 51. poll() 方法和 remove() 方法的区别？poll() 和 remove() 都是从队列中取出一个元素，但是 poll() 在获取元素失败的时候会返回空，但是 remove() 失败的时候会抛出异常。 52. Java 中 LinkedHashMap 和 PriorityQueue 的区别是什么？PriorityQueue 保证最高或者最低优先级的的元素总是在队列头部，但是 LinkedHashMap 维持的顺序是元素插入的顺序。当遍历一个 PriorityQueue 时，没有任何顺序保证，但是 LinkedHashMap 课保证遍历顺序是元素插入的顺序。 53. ArrayList 与 LinkedList 的区别？最明显的区别是 ArrrayList 底层的数据结构是数组，支持随机访问，而 LinkedList 的底层数据结构书链表，不支持随机访问。使用下标访问一个元素，ArrayList 的时间复杂度是 O(1)，而 LinkedList 是 O(n)。 54. 用哪两种方式来实现集合的排序？你可以使用有序集合，如 TreeSet 或 TreeMap，你也可以使用有顺序的的集合，如 list，然后通过 Collections.sort() 来排序。 55. Java 中怎么打印数组？你可以使用 Arrays.toString() 和 Arrays.deepToString() 方法来打印数组。由于数组没有实现 toString() 方法，所以如果将数组传递给 System.out.println() 方法，将无法打印出数组的内容，但是 Arrays.toString() 可以打印每个元素。 56. Java 中的 LinkedList 是单向链表还是双向链表？是双向链表，你可以检查 JDK 的源码。在 Eclipse，你可以使用快捷键 Ctrl + T，直接在编辑器中打开该类。 57. Java 中的 TreeMap 是采用什么树实现的？Java 中的 TreeMap 是使用红黑树实现的。 58. Java 中的 HashSet，内部是如何工作的？HashSet 的内部采用 HashMap来实现。由于 Map 需要 key 和 value，所以所有 key 的都有一个默认 value。类似于 HashMap，HashSet 不允许重复的 key，只允许有一个null key，意思就是 HashSet 中只允许存储一个 null 对象。 59. 写一段代码在遍历 ArrayList 时移除一个元素？该问题的关键在于面试者使用的是 ArrayList 的 remove() 还是 Iterator 的 remove()方法。这有一段示例代码，是使用正确的方式来实现在遍历的过程中移除元素，而不会出现 ConcurrentModificationException 异常的示例代码。 60. 我们能自己写一个容器类，然后使用 for-each 循环吗？可以，你可以写一个自己的容器类。如果你想使用 Java 中增强的循环来遍历，你只需要实现 Iterable 接口。如果你实现 Collection 接口，默认就具有该属性。 61. ArrayList 和 HashMap 的默认大小是多数？在 Java 7 中，ArrayList 的默认大小是 10 个元素，HashMap 的默认大小是16个元素（必须是2的幂）。这就是 Java 7 中 ArrayList 和 HashMap 类的代码片段： 12345// from ArrayList.java JDK 1.7private static final int DEFAULT_CAPACITY = 10; //from HashMap.java JDK 7static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 62. 有没有可能两个不相等的对象有有相同的 hashcode？有可能，两个不相等的对象可能会有相同的 hashcode 值，这就是为什么在 hashmap 中会有冲突。相等 hashcode 值的规定只是说如果两个对象相等，必须有相同的hashcode 值，但是没有关于不相等对象的任何规定。 63. 两个相同的对象会有不同的的 hash code 吗？不能，根据 hash code 的规定，这是不可能的。 64. 我们可以在 hashcode() 中使用随机数字吗？不行，因为对象的 hashcode 值必须是相同的。 65. Java 中，Comparator 与 Comparable 有什么不同？Comparable 接口用于定义对象的自然顺序，而 comparator 通常用于定义用户定制的顺序。Comparable 总是只有一个，但是可以有多个 comparator 来定义对象的顺序。 66. 为什么在重写 equals 方法的时候需要重写 hashCode 方法？因为有强制的规范指定需要同时重写 hashcode 与 equal 是方法，许多容器类，如 HashMap、HashSet 都依赖于 hashcode 与 equals 的规定。 67. “a==b”和”a.equals(b)”有什么区别？如果 a 和 b 都是对象，则 a==b 是比较两个对象的引用，只有当 a 和 b 指向的是堆中的同一个对象才会返回 true，而 a.equals(b) 是进行逻辑比较，所以通常需要重写该方法来提供逻辑一致性的比较。例如，String 类重写 equals() 方法，所以可以用于两个不同对象，但是包含的字母相同的比较。 68. a.hashCode() 有什么用？与 a.equals(b) 有什么关系？简介：hashCode() 方法是相应对象整型的 hash 值。它常用于基于 hash 的集合类，如 Hashtable、HashMap、LinkedHashMap等等。它与 equals() 方法关系特别紧密。根据 Java 规范，两个使用 equal() 方法来判断相等的对象，必须具有相同的 hash code。 1、hashcode的作用 List和Set，如何保证Set不重复呢？通过迭代使用equals方法来判断，数据量小还可以接受，数据量大怎么解决？引入hashcode，实际上hashcode扮演的角色就是寻址，大大减少查询匹配次数。 2、hashcode重要吗 对于数组、List集合就是一个累赘。而对于hashmap, hashset, hashtable就异常重要了。 3、equals方法遵循的原则 对称性 若x.equals(y)true，则y.equals(x)true 自反性 x.equals(x)必须true 传递性 若x.equals(y)true,y.equals(z)true,则x.equals(z)必为true 一致性 只要x,y内容不变，无论调用多少次结果不变 其他 x.equals(null) 永远false，x.equals(和x数据类型不同)始终false 69. final、finalize 和 finally 的不同之处？final 是一个修饰符，可以修饰变量、方法和类。如果 final 修饰变量，意味着该变量的值在初始化后不能被改变。Java 技术允许使用 finalize() 方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。这个方法是由垃圾收集器在确定这个对象没有被引用时对这个对象调用的，但是什么时候调用 finalize 没有保证。finally 是一个关键字，与 try 和 catch 一起用于异常的处理。finally 块一定会被执行，无论在 try 块中是否有发生异常。 70. Java 中的编译期常量是什么？使用它又什么风险？变量也就是我们所说的编译期常量，这里的 public 可选的。实际上这些变量在编译时会被替换掉，因为编译器知道这些变量的值，并且知道这些变量在运行时不能改变。这种方式存在的一个问题是你使用了一个内部的或第三方库中的公有编译时常量，但是这个值后面被其他人改变了，但是你的客户端仍然在使用老的值，甚至你已经部署了一个新的jar。为了避免这种情况，当你在更新依赖 JAR 文件时，确保重新编译你的程序。 71. 说出几点 Java 中使用 Collections 的最佳实践这是我在使用 Java 中 Collectionc 类的一些最佳实践：a）使用正确的集合类，例如，如果不需要同步列表，使用 ArrayList 而不是 Vector。b）优先使用并发集合，而不是对集合进行同步。并发集合提供更好的可扩展性。c）使用接口代表和访问集合，如使用List存储 ArrayList，使用 Map 存储 HashMap 等等。d）使用迭代器来循环集合。e）使用集合的时候使用泛型。 72. 静态内部类与顶级类有什么区别？一个公共的顶级类的源文件名称与类名相同，而嵌套静态类没有这个要求。一个嵌套类位于顶级类内部，需要使用顶级类的名称来引用嵌套静态类，如 HashMap.Entry 是一个嵌套静态类，HashMap 是一个顶级类，Entry是一个嵌套静态类。 73. Java 中，Serializable 与 Externalizable 的区别？Serializable 接口是一个序列化 Java 类的接口，以便于它们可以在网络上传输或者可以将它们的状态保存在磁盘上，是 JVM 内嵌的默认序列化方式，成本高、脆弱而且不安全。Externalizable 允许你控制整个序列化过程，指定特定的二进制格式，增加安全机制。 74. 说出 JDK 1.7 中的三个新特性？虽然 JDK 1.7 不像 JDK 5 和 8 一样的大版本，但是，还是有很多新的特性，如 try-with-resource 语句，这样你在使用流或者资源的时候，就不需要手动关闭，Java 会自动关闭。Fork-Join 池某种程度上实现 Java 版的 Map-reduce。允许 Switch 中有 String 变量和文本。菱形操作符(&lt;&gt;)用于泛型推断，不再需要在变量声明的右边申明泛型，因此可以写出可读写更强、更简洁的代码。另一个值得一提的特性是改善异常处理，如允许在同一个 catch 块中捕获多个异常。 75. 说出 5 个 JDK 1.8 引入的新特性？Java 8 在 Java 历史上是一个开创新的版本，下面 JDK 8 中 5 个主要的特性：Lambda 表达式，允许像对象一样传递匿名函数Stream API，充分利用现代多核 CPU，可以写出很简洁的代码Date 与 Time API，最终，有一个稳定、简单的日期和时间库可供你使用扩展方法，现在，接口中可以有静态、默认方法。重复注解，现在你可以将相同的注解在同一类型上使用多次。 下述包含 Java 面试过程中关于 SOLID 的设计原则，OOP 基础，如类，对象，接口，继承，多态，封装，抽象以及更高级的一些概念，如组合、聚合及关联。也包含了 GOF 设计模式的问题。 76. 接口是什么？为什么要使用接口而不是直接使用具体类？接口用于定义 API。它定义了类必须得遵循的规则。同时，它提供了一种抽象，因为客户端只使用接口，这样可以有多重实现，如 List 接口，你可以使用可随机访问的 ArrayList，也可以使用方便插入和删除的 LinkedList。接口中不允许普通方法，以此来保证抽象，但是 Java 8 中你可以在接口声明静态方法和默认普通方法。 77. Java 中，抽象类与接口之间有什么不同？Java 中，抽象类和接口有很多不同之处，但是最重要的一个是 Java 中限制一个类只能继承一个类，但是可以实现多个接口。抽象类可以很好的定义一个家族类的默认行为，而接口能更好的定义类型，有助于后面实现多态机制参见第六条。 78. 除了单例模式，你在生产环境中还用过什么设计模式?这需要根据你的经验来回答。一般情况下，你可以说依赖注入，工厂模式，装饰模式或者观察者模式，随意选择你使用过的一种即可。不过你要准备回答接下的基于你选择的模式的问题。 79. 你能解释一下里氏替换原则吗?严格定义：如果对每一个类型为S的对象o1，都有类型为T的对象o2，使得以T定义的所有程序P在所有的对象用o1替换o2时，程序P的行为没有变化，那么类型S是类型T的子类型。 通俗表述：所有引用基类（父类）的地方必须能透明地使用其子类的对象。也就是说子类可以扩展父类的功能，但不能改变父类原有的功能。它包含以下4层含义： 子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法。 子类中可以增加自己特有的方法。 当子类的方法重载父类的方法时，方法的前置条件（即方法的形参）要比父类方法的输入参数更宽松。 当子类的方法实现父类的抽象方法时，方法的后置条件（即方法的返回值）要比父类更严格。 80.什么情况下会违反迪米特法则？为什么会有这个问题？迪米特法则建议“只和朋友说话，不要陌生人说话”，以此来减少类之间的耦合。 81. 适配器模式是什么？什么时候使用？适配器模式提供对接口的转换。如果你的客户端使用某些接口，但是你有另外一些接口，你就可以写一个适配去来连接这些接口。 82. 构造器注入和 setter 依赖注入，那种方式更好？**每种方式都有它的缺点和优点。构造器注入保证所有的注入都被初始化，但是 setter 注入提供更好的灵活性来设置可选依赖。如果使用 XML 来描述依赖，Setter 注入的可读写会更强。经验法则是强制依赖使用构造器注入，可选依赖使用 setter 注入。 83. 依赖注入和工厂模式之间有什么不同？虽然两种模式都是将对象的创建从应用的逻辑中分离，但是依赖注入比工程模式更清晰。通过依赖注入，你的类就是 POJO，它只知道依赖而不关心它们怎么获取。使用工厂模式，你的类需要通过工厂来获取依赖。因此，使用 DI 会比使用工厂模式更容易测试。 84. 适配器模式和装饰器模式有什么区别？虽然适配器模式和装饰器模式的结构类似，但是每种模式的出现意图不同。适配器模式被用于桥接两个接口，而装饰模式的目的是在不修改类的情况下给类增加新的功能。 85. 适配器模式和代理模式之前有什么不同？这个问题与前面的类似，适配器模式和代理模式的区别在于他们的意图不同。由于适配器模式和代理模式都是封装真正执行动作的类，因此结构是一致的，但是适配器模式用于接口之间的转换，而代理模式则是增加一个额外的中间层，以便支持分配、控制或智能访问。 86. 什么是模板方法模式？模板方法提供算法的框架，你可以自己去配置或定义步骤。例如，你可以将排序算法看做是一个模板。它定义了排序的步骤，但是具体的比较，可以使用 Comparable 或者其语言中类似东西，具体策略由你去配置。列出算法概要的方法就是众所周知的模板方法。 87. 什么时候使用访问者模式？访问者模式用于解决在类的继承层次上增加操作，但是不直接与之关联。这种模式采用双派发的形式来增加中间层。 88. 什么时候使用组合模式？组合模式使用树结构来展示部分与整体继承关系。它允许客户端采用统一的形式来对待单个对象和对象容器。当你想要展示对象这种部分与整体的继承关系时采用组合模式。 89. 继承和组合之间有什么不同？虽然两种都可以实现代码复用，但是组合比继承共灵活，因为组合允许你在运行时选择不同的实现。用组合实现的代码也比继承测试起来更加简单。 90. 描述 Java 中的重载和重写？重载和重写都允许你用相同的名称来实现不同的功能，但是重载是编译时活动，而重写是运行时活动。你可以在同一个类中重载方法，但是只能在子类中重写方法。重写必须要有继承。 91. OOP 中的 组合、聚合和关联有什么区别？如果两个对象彼此有关系，就说他们是彼此相关联的。组合和聚合是面向对象中的两种形式的关联。组合是一种比聚合更强力的关联。组合中，一个对象是另一个的拥有者，而聚合则是指一个对象使用另一个对象。如果对象 A 是由对象 B 组合的，则 A 不存在的话，B一定不存在，但是如果 A 对象聚合了一个对象 B，则即使 A 不存在了，B 也可以单独存在。 92. 给我一个符合开闭原则的设计模式的例子？开闭原则要求你的代码对扩展开放，对修改关闭。这个意思就是说，如果你想增加一个新的功能，你可以很容易的在不改变已测试过的代码的前提下增加新的代码。有好几个设计模式是基于开闭原则的，如策略模式，如果你需要一个新的策略，只需要实现接口，增加配置，不需要改变核心逻辑。一个正在工作的例子是 Collections.sort() 方法，这就是基于策略模式，遵循开闭原则的，你不需为新的对象修改 sort() 方法，你需要做的仅仅是实现你自己的 Comparator 接口。 93. 什么时候使用享元模式（蝇量模式）？享元模式通过共享对象来避免创建太多的对象。为了使用享元模式，你需要确保你的对象是不可变的，这样你才能安全的共享。JDK 中 String 池、Integer 池以及 Long 池都是很好的使用了享元模式的例子。 94. Java 中如何格式化一个日期？如格式化为 ddMMyyyy 的形式？Java 中，可以使用 SimpleDateFormat 类或者 joda-time 库来格式日期。DateFormat 类允许你使用多种流行的格式来格式化日期。 95. Java 中，怎么在格式化的日期中显示时区？pattern中加z yyyy-MM-dd HH:mm:ss.SSS Z 96. Java 中 java.util.Date 与 java.sql.Date 有什么区别？java.sql.Date是针对SQL语句使用的，它只包含日期而没有时间部分,它们都有getTime方法返回毫秒数，自然就可以直接构建。java.util.Date 是 java.sql.Date 的父类，前者是常用的表示时间的类，我们通常格式化或者得到当前时间都是用他，后者之后在读写数据库的时候用他，因为PreparedStament的setDate()的第2参数和ResultSet的getDate()方法的第2个参数都是java.sql.Date。 97. Java 中，如何计算两个日期之间的差距？12345678public static int dateDiff(Date d1, Date d2) throws Exception &#123;long n1 = d1.getTime();long n2 = d2.getTime();long diff = Math.abs(n1 - n2);diff /= 3600 * 1000 * 24;return diff;&#125; 98. Java 中，如何将字符串 YYYYMMDD 转换为日期？SimpleDateFormat的parse方法 99. 说出几条 Java 中方法重载的最佳实践？下面有几条可以遵循的方法重载的最佳实践来避免造成自动装箱的混乱。a）不要重载这样的方法：一个方法接收 int 参数，而另个方法接收 Integer 参数。b）不要重载参数数量一致，而只是参数顺序不同的方法。c）如果重载的方法参数个数多于 5 个，采用可变参数。 100. 说出 5 条 IO 的最佳实践IO 对 Java 应用的性能非常重要。理想情况下，你应该在你应用的关键路径上避免 IO 操作。下面是一些你应该遵循的 Java IO 最佳实践： a）使用有缓冲区的 IO 类，而不要单独读取字节或字符b）使用 NIO 和 NIO2c）在 finally 块中关闭流，或者使用 try-with-resource（Java7） 语句d）使用内存映射文件获取更快的 IO 101. Object有哪些公用方法？clone equals hashcode wait notify notifyall finalize toString getClass除了clone和finalize其他均为公共方法。 11个方法，wait被重载了两次 102. equals与==的区别区别1. ==是一个运算符 equals是Object类的方法 区别2. 比较时的区别 a. 用于基本类型的变量比较时：==用于比较值是否相等，equals不能直接用于基本数据类型的比较，需要转换为其对应的包装类型。b. 用于引用类型的比较时。==和equals都是比较栈内存中的地址是否相等 。相等为true 否则为false。但是通常会重写equals方法去实现对象内容的比较。 103. String、StringBuffer与StringBuilder的区别第一点：可变和适用范围。String对象是不可变的，而StringBuffer和StringBuilder是可变字符序列。每次对String的操作相当于生成一个新的String对象，而对StringBuffer和StringBuilder的操作是对对象本身的操作，而不会生成新的对象，所以对于频繁改变内容的字符串避免使用String，因为频繁的生成对象将会对系统性能产生影响。 第二点：线程安全。String由于有final修饰，是immutable的，安全性是简单而纯粹的。StringBuilder和StringBuffer的区别在于StringBuilder不保证同步，也就是说如果需要线程安全需要使用StringBuffer，不需要同步的StringBuilder效率更高。 104. switch能否用String做参数Java1.7开始支持，但实际这是一颗Java语法糖。除此之外，byte，short，long，枚举，boolean均可用于switch，只有浮点型不可以。 105. 封装、继承、多态封装： 1.概念：就是把对象的属性和操作（或服务）结合为一个独立的整体，并尽可能隐藏对象的内部实现细节。 2.好处： (1)隐藏内部实现细节。 继承： 1.概念：继承是从已有的类中派生出新的类，新的类能吸收已有类的数据属性和行为，并能扩展新的能力 2.好处：提高代码的复用，缩短开发周期。 多态： 1.概念：多态（Polymorphism）按字面的意思就是“多种状态，即同一个实体同时具有多种形式。一般表现形式是程序在运行的过程中，同一种类型在不同的条件下表现不同的结果。多态也称为动态绑定，一般是在运行时刻才能确定方法的具体执行对象。 2.好处：1）将接口和实现分开，改善代码的组织结构和可读性，还能创建可拓展的程序。2）消除类型之间的耦合关系。允许将多个类型视为同一个类型。3）一个多态方法的调用允许有多种表现形式 106. Comparable和Comparator接口区别Comparator位于包java.util下，而Comparable位于包java.lang下 如果我们需要使用Arrays或Collections的排序方法对对象进行排序时，我们需要在自定义类中实现Comparable接口并重写compareTo方法，compareTo方法接收一个参数，如果this对象比传递的参数小，相等或大时分别返回负整数、0、正整数。Comparable被用来提供对象的自然排序。String、Integer实现了该接口。 Comparator比较器的compare方法接收2个参数，根据参数的比较大小分别返回负整数、0和正整数。Comparator 是一个外部的比较器，当这个对象自然排序不能满足你的要求时，你可以写一个比较器来完成两个对象之间大小的比较。用 Comparator 是策略模式（strategy design pattern），就是不改变对象自身，而用一个策略对象（strategy object）来改变它的行为。 107. 与Java集合框架相关的有哪些最好的实践（1）根据需要选择正确的集合类型。比如，如果指定了大小，我们会选用Array而非ArrayList。如果我们想根据插入顺序遍历一个Map，我们需要使用TreeMap。如果我们不想重复，我们应该使用Set。 （2）一些集合类允许指定初始容量，所以如果我们能够估计到存储元素的数量，我们可以使用它，就避免了重新哈希或大小调整。 （3）基于接口编程，而非基于实现编程，它允许我们后来轻易地改变实现。 （4）总是使用类型安全的泛型，避免在运行时出现ClassCastException。 （5）使用JDK提供的不可变类作为Map的key，可以避免自己实现hashCode()和equals()。 108. IO和NIO简述1、简述 在以前的Java IO中，都是阻塞式IO，NIO引入了非阻塞式IO。第一种方式：我从硬盘读取数据，然后程序一直等，数据读完后，继续操作。这种方式是最简单的，叫阻塞IO。第二种方式：我从硬盘读取数据，然后程序继续向下执行，等数据读取完后，通知当前程序（对硬件来说叫中断，对程序来说叫回调），然后此程序可以立即处理数据，也可以执行完当前操作在读取数据。 2.流与块的比较 原来的 I/O 以流的方式处理数据，而 NIO 以块的方式处理数据。面向流 的 I/O 系统一次一个字节地处理数据。一个输入流产生一个字节的数据，一个输出流消费一个字节的数据。这样做是相对简单的。不利的一面是，面向流的 I/O 通常相当慢。一个 面向块 的 I/O 系统以块的形式处理数据。每一个操作都在一步中产生或者消费一个数据块。按块处理数据比按(流式的)字节处理数据要快得多。但是面向块的 I/O 缺少一些面向流的 I/O 所具有的优雅性和简单性。 3.通道与流 Channel是一个对象，可以通过它读取和写入数据。通道与流功能类似，不同之处在于通道是双向的。而流只是在一个方向上移动(一个流必须是 InputStream 或者 OutputStream 的子类)， 而通道可以用于读、写或者同时用于读写。 4.缓冲区Buffer 在 NIO 库中，所有数据都是用缓冲区处理的。在 NIO 库中，所有数据都是用缓冲区处理的。 Position: 表示下一次访问的缓冲区位置Limit: 表示当前缓冲区存放的数据容量。Capacity:表示缓冲区最大容量 flip()方法:读写模式切换 clear方法:它将 limit 设置为与 capacity 相同。它设置 position 为 0。 二、Java高级（JavaEE、框架、服务器、工具等）1. Servlet1.1 Servlet继承实现结构 1234Servlet(接口) --&gt; init|service|destroy方法GenericServlet(抽象类) --&gt; 与协议无关的ServletHttpServlet(抽象类) --&gt; 实现了http协议自定义Servlet --&gt; 重写doGet/doPost 1.2 编写Servlet的步骤 继承HttpServlet 重写doGet/doPost方法 在web.xml中注册servlet 1.3 Servlet生命周期 init:仅执行一次,负责装载servlet时初始化servlet对象 service:核心方法,一般get/post两种方式 destroy:停止并卸载servlet,释放资源 1.4 过程 客户端request请求 -&gt; 服务器检查Servlet实例是否存在 -&gt; 若存在调用相应service方法 客户端request请求 -&gt; 服务器检查Servlet实例是否存在 -&gt; 若不存在装载Servlet类并创建实例 -&gt; 调用init初始化 -&gt; 调用service 加载和实例化、初始化、处理请求、服务结束 1.5 doPost方法要抛出的异常:ServletExcception、IOException 1.6 Servlet容器装载Servlet web.xml中配置load-on-startup启动时装载 客户首次向Servlet发送请求 Servlet类文件被更新后, 重新装载Servlet 1.7 HttpServlet容器响应web客户请求流程 Web客户向servlet容器发出http请求 servlet容器解析Web客户的http请求 servlet容器创建一个HttpRequest对象, 封装http请求信息 servlet容器创建一个HttpResponse对象 servlet容器调用HttpServlet的service方法, 把HttpRequest和HttpResponse对象作为service方法的参数传给HttpServlet对象 HttpServlet调用httprequest的有关方法, 获取http请求信息 httpservlet调用httpresponse的有关方法, 生成响应数据 Servlet容器把HttpServlet的响应结果传给web客户 1.8 HttpServletRequest完成的一些功能 request.getCookie() request.getHeader(String s) request.getContextPath() request.getSession() 12HttpSession session = request.getSession(boolean create)// 返回当前请求的会话 1.9 HttpServletResponse完成一些的功能 设http响应头 设置Cookie 输出返回数据 1.10 Servlet与JSP九大内置对象的关系 JSP对象 怎样获得1234567891. out -&gt; response.getWriter2. request -&gt; Service方法中的req参数3. response -&gt; Service方法中的resp参数4. session -&gt; request.getSession5. application -&gt; getServletContext6. exception -&gt; Throwable7. page -&gt; this8. pageContext -&gt; PageContext9. Config -&gt; getServletConfig exception是JSP九大内置对象之一，其实例代表其他页面的异常和错误。只有当页面是错误处理页面时，即isErroePage为 true时，该对象才可以使用。 2. JSPJSP的前身就是Servlet 3. Tomcat3.1 Tomcat容器的等级 Tomcat - Container - Engine - Host - Servlet - 多个Context(一个Context对应一个web工程)-Wrapper 4. struts struts可进行文件上传 struts基于MVC模式 struts让流程结构更清晰 struts有许多action类, 会增加类文件数目 5. Hibernate的7大鼓励措施 尽量使用many-to-one, 避免使用单项one-to-many 灵活使用单项one-to-many 不用一对一, 使用多对一代替一对一 配置对象缓存, 不使用集合对象 一对多使用bag, 多对一使用set 继承使用显示多态 消除大表, 使用二级缓存 6. Hibernate延迟加载 Hibernate2延迟加载实现：a)实体对象 b)集合（Collection） Hibernate3 提供了属性的延迟加载功能当Hibernate在查询数据的时候，数据并没有存在与内存中，当程序真正对数据的操作时，对象才存在与内存中，就实现了延迟加载，他节省了服务器的内存开销，从而提高了服务器的性能。 hibernate使用Java反射机制，而不是字节码增强程序来实现透明性。 hibernate的性能非常好，因为它是个轻量级框架。映射的灵活性很出色。它支持各种关系数据库，从一对一到多对多的各种复杂关系。 7. Java 中，DOM 和 SAX 解析器有什么不同？DOM 解析器将整个 XML 文档加载到内存来创建一棵 DOM 模型树，这样可以更快的查找节点和修改 XML 结构，而 SAX 解析器是一个基于事件的解析器，不会将整个 XML 文档加载到内存。由于这个原因，DOM 比 SAX 更快，也要求更多的内存，但不适合于解析大的 XML 文件。 8. Java 中，Maven 和 ANT 有什么区别？虽然两者都是构建工具，都用于创建 Java 应用，但是 Maven 做的事情更多，在基于“约定优于配置”的概念下，提供标准的Java 项目结构，同时能为应用自动管理依赖（应用中所依赖的 JAR 文件）。 9. 解析XML不同方式对比DOM、SAX、JDOM、DOM4J [x] DOM DOM树驻留内存 可以进行修改和写入,耗费内存。 步骤：创建DocumentBuilderFactory对象 -&gt; 创建DocumentBuilder对象 -&gt; Document document = db.parse(“xml”) [x] SAX 事件驱动模式 获取一个SAXParserFactory工厂的实例 -&gt; 根据该实例获取SAXParser -&gt; 创建Handler对象 -&gt; 调用SAXParser的parse方法解析 用于读取节点数据 不易编码 事件有顺序 很难同时访问xml的多处数据 [x] JDOM 创建一个SAXBuilder的对象 -&gt; 创建一个输入流，加载xml文件 -&gt;通过saxBuilder的build方法将输入流加载至saxBuilder并接收Document对象 使用具体类而不使用接口 [x] DOM4J 通过SAXReader的read方法加载xml文件并获取document对象 使用接口和抽象类，灵活性好，功能强大 10. Nginx相关11. XML与JSON对比和区别XML 1231）应用广泛，可扩展性强，被广泛应用各种场合2）读取、解析没有JSON快3）可读性强，可描述复杂结构 JSON 1234561）结构简单，都是键值对2）读取、解析速度快，很多语言支持3）传输数据量小，传输速率大大提高4）描述复杂结构能力较弱JavaScript、PHP等原生支持，简化了读取解析。成为当前互联网时代普遍应用的数据结构。 三、多线程和并发0. Java 中的 volatile 变量是什么Java 语言提供了一种稍弱的同步机制,即volatile变量。但是volatile并不容易完全被正确、完整的理解。一般来说，volatile具备2条语义，或者说2个特性。第一是保证volatile修饰的变量对所有线程的可见性，这里的可见性是指当一条线程修改了该变量，新值对于其它线程来说是立即可以得知的。而普通变量做不到这一点。 第二条语义是禁止指令重排序优化，这条语义在JDK1.5才被修复。 关于第一点：根据JMM，所有的变量存储在主内存，而每个线程还有自己的工作内存，线程的工作内存保存该线程使用到的变量的主内存副本拷贝，线程对变量的操作在工作内存中进行，不能直接读写主内存的变量。在volatile可见性这一点上，普通变量做不到的原因正因如此。比如，线程A修改了一个普通变量的值，然后向主内存进行回写，线程B在线程A回写完成后再从主内存读取，新变量才能对线程B可见。其实，按照虚拟机规范，volatile变量依然有工作内存的拷贝，要借助主内存来实现可见性。但由于volatile的特殊规则保证了新值能立即同步回主内存，以及每次使用从主内存刷新，以此保证了多线程操作volatile变量的可见性。 关于第二点：先说指令重排序，指令重排序是指CPU采用了允许将多条指令不按规定顺序分开发送给相应的处理单元处理，但并不是说任意重排，CPU需要正确处理指令依赖情况确保最终的正确结果，指令重排序是机器级的优化操作。那么为什么volatile要禁止指令重排序呢，又是如何去做的。举例，DCL（双重检查加锁）的单例模式。volatile修饰后，代码中将会插入许多内存屏障指令保证处理器不发生乱序执行。同时由于Happens-before规则的保证，在刚才的例子中写操作会发生在后续的读操作之前。 除了以上2点，volatile还保证对于64位long和double的读取是原子性的。因为在JMM中允许虚拟机对未被volatile修饰的64位的long和double读写操作分为2次32位的操作来执行，这也就是所谓的long和double的非原子性协定。 基于以上几点，我们知道volatile虽然有这些语义和特性在并发的情况下仍然不能保证线程安全。大部分情况下仍然需要加锁。 除非是以下2种情况，1.运算结果不依赖变量的当前值，或者能够确保只有单一线程修改变量的值；2.变量不需要与其他的状态变量共同参与不变约束。 1. volatile简述Java 语言提供了一种稍弱的同步机制,即volatile变量.用来确保将变量的更新操作通知到其他线程,保证了新值能立即同步到主内存,以及每次使用前立即从主内存刷新。 当把变量声明为volatile类型后,编译器与运行时都会注意到这个变量是共享的。volatile修饰变量,每次被线程访问时强迫其从主内存重读该值,修改后再写回。保证读取的可见性,对其他线程立即可见。volatile的另一个语义是禁止指令重排序优化。但是volatile并不保证原子性,也就不能保证线程安全。 2. Java 中能创建 volatile 数组吗？能，Java 中可以创建 volatile 类型数组，不过只是一个指向数组的引用，而不是整个数组。我的意思是，如果改变引用指向的数组，将会受到 volatile 的保护，但是如果多个线程同时改变数组的元素，volatile 就不能起到之前的保护作用了。 3. volatile 能使得一个非原子操作变成原子操作吗？一个典型的例子是在类中有一个 long 类型的成员变量。如果你知道该成员变量会被多个线程访问，如计数器、价格等，你最好是将其设置为 volatile。为什么？因为 Java 中读取 long 类型变量不是原子的，需要分成两步，如果一个线程正在修改该 long 变量的值，另一个线程可能只能看到该值的一半（前 32 位）。但是对一个 volatile 型的 long 或 double 变量的读写是原子。 4. volatile 禁止指令重排序的底层原理指令重排序，是指CPU允许多条指令不按程序规定的顺序分开发送给相应电路单元处理。但并不是说任意重排，CPU需要能正确处理指令依赖情况以正确的执行结果。volatile禁止指令重排序是通过内存屏障实现的，指令重排序不能把后面的指令重排序到内存屏障之前。由内存屏障保证一致性。注：该条语义在JDK1.5才得以修复，这点也是JDK1.5之前无法通过双重检查加锁来实现单例模式的原因。 5. volatile 类型变量提供什么保证？volatile 变量提供有序性和可见性保证，例如，JVM 或者 JIT为了获得更好的性能会对语句重排序，但是 volatile 类型变量即使在没有同步块的情况下赋值也不会与其他语句重排序。 volatile 提供 happens-before 的保证，确保一个线程的修改能对其他线程是可见的。某些情况下，volatile 还能提供原子性，如读 64 位数据类型，像 long 和 double 都不是原子的，但 volatile 类型的 double 和 long 就是原子的。 volatile的使用场景： 运算结果不依赖变量的当前值，或者能够确保只有单一的线程修改该值 变量不需要与其他状态变量共同参与不变约束 6. volatile的性能volatile变量的读操作性能消耗和普通变量差不多，但是写操作可能相对慢一些，因为它需要在本地代码中插入许多内存屏障指令以确保处理器不发生乱序执行。大多数情况下，volatile总开销比锁低，但我们要注意volatile的语义能否满足使用场景。 7. 10 个线程和 2 个线程的同步代码，哪个更容易写？从写代码的角度来说，两者的复杂度是相同的，因为同步代码与线程数量是相互独立的。但是同步策略的选择依赖于线程的数量，因为越多的线程意味着更大的竞争，所以你需要利用同步技术，如锁分离，这要求更复杂的代码和专业知识。 8. 你是如何调用 wait（）方法的？使用 if 块还是循环？为什么？wait() 方法应该在循环调用，因为当线程获取到 CPU 开始执行的时候，其他条件可能还没有满足，所以在处理前，循环检测条件是否满足会更好。下面是一段标准的使用 wait 和 notify 方法的代码： 123456// The standard idiom for using the wait methodsynchronized (obj) &#123;while (condition does not hold)obj.wait(); // (Releases lock, and reacquires on wakeup)... // Perform action appropriate to condition&#125; 参见 Effective Java 第 69 条，获取更多关于为什么应该在循环中来调用 wait 方法的内容。 9. 什么是多线程环境下的伪共享（false sharing）？伪共享是多线程系统（每个处理器有自己的局部缓存）中一个众所周知的性能问题。伪共享发生在不同处理器的上的线程对变量的修改依赖于相同的缓存行，如下图所示： 伪共享问题很难被发现，因为线程可能访问完全不同的全局变量，内存中却碰巧在很相近的位置上。如其他诸多的并发问题，避免伪共享的最基本方式是仔细审查代码，根据缓存行来调整你的数据结构。 10. 线程的run方法和start方法 run方法 只是thread类的一个普通方法,若直接调用程序中依然只有主线程这一个线程,还要顺序执行,依然要等待run方法体执行完毕才可执行下面的代码。 start方法 用start方法来启动线程,是真正实现了多线程。调用thread类的start方法来启动一个线程,此时线程处于就绪状态,一旦得到cpu时间片,就开始执行run方法。 11. ReadWriteLock(读写锁)写写互斥 读写互斥 读读并发, 在读多写少的情况下可以提高效率 12. resume(继续挂起的线程)和suspend(挂起线程)一起用13. wait与notify、notifyall一起用14. sleep与wait的异同点 sleep是Thread类的静态方法, wait来自object类 sleep方法短暂停顿不释放锁, wait方法条件等待要释放锁，因为只有这样，其他等待的线程才能在满足条件时获取到该锁。 wait, notify, notifyall必须在同步代码块中使用, sleep可以在任何地方使用 都可以抛出InterruptedException 15. 让一个线程停止执行异常 - 停止执行休眠 - 停止执行阻塞 - 停止执行 16. ThreadLocal简介16.1 ThreadLocal解决了变量并发访问的冲突问题 当使用ThreadLocal维护变量时,ThreadLocal为每个使用该变量的线程提供独立的变量副本,每个线程都可以独立地改变自己的副本,而不会影响其它线程所对应的副本,是线程隔离的。线程隔离的秘密在于ThreadLocalMap类(ThreadLocal的静态内部类) 16.2 与synchronized同步机制的比较 首先,它们都是为了解决多线程中相同变量访问冲突问题。不过,在同步机制中,要通过对象的锁机制保证同一时间只有一个线程访问该变量。该变量是线程共享的, 使用同步机制要求程序缜密地分析什么时候对该变量读写, 什么时候需要锁定某个对象, 什么时候释放对象锁等复杂的问题,程序设计编写难度较大, 是一种“以时间换空间”的方式。而ThreadLocal采用了以“以空间换时间”的方式。 17. 线程局部变量原理当使用ThreadLocal维护变量时,ThreadLocal为每个使用该变量的线程提供独立的变量副本,每个线程都可以独立地改变自己的副本,而不会影响其它线程所对应的副本,是线程隔离的。线程隔离的秘密在于ThreadLocalMap类(ThreadLocal的静态内部类) 线程局部变量是局限于线程内部的变量，属于线程自身所有，不在多个线程间共享。Java 提供 ThreadLocal 类来支持线程局部变量，是一种实现线程安全的方式。但是在管理环境下（如 web 服务器）使用线程局部变量的时候要特别小心，在这种情况下，工作线程的生命周期比任何应用变量的生命周期都要长。任何线程局部变量一旦在工作完成后没有释放，Java 应用就存在内存泄露的风险。 ThreadLocal的方法：void set(T value)、T get()以及T initialValue()。 ThreadLocal是如何为每个线程创建变量的副本的： 首先，在每个线程Thread内部有一个ThreadLocal.ThreadLocalMap类型的成员变量threadLocals，这个threadLocals就是用来存储实际的变量副本的，键值为当前ThreadLocal变量，value为变量副本（即T类型的变量）。初始时，在Thread里面，threadLocals为空，当通过ThreadLocal变量调用get()方法或者set()方法，就会对Thread类中的threadLocals进行初始化，并且以当前ThreadLocal变量为键值，以ThreadLocal要保存的副本变量为value，存到threadLocals。然后在当前线程里面，如果要使用副本变量，就可以通过get方法在threadLocals里面查找。 总结： 实际的通过ThreadLocal创建的副本是存储在每个线程自己的threadLocals中的 为何threadLocals的类型ThreadLocalMap的键值为ThreadLocal对象，因为每个线程中可有多个threadLocal变量，就像上面代码中的longLocal和stringLocal； 在进行get之前，必须先set，否则会报空指针异常；如果想在get之前不需要调用set就能正常访问的话，必须重写initialValue()方法 18. JDK提供的用于并发编程的同步器 Semaphore Java并发库的Semaphore可以很轻松完成信号量控制，Semaphore可以控制某个资源可被同时访问的个数，通过 acquire() 获取一个许可，如果没有就等待，而 release() 释放一个许可。 CyclicBarrier 主要的方法就是一个：await()。await()方法每被调用一次，计数便会减少1，并阻塞住当前线程。当计数减至0时，阻塞解除，所有在此CyclicBarrier上面阻塞的线程开始运行。 CountDownLatch 直译过来就是倒计数(CountDown)门闩(Latch)。倒计数不用说，门闩的意思顾名思义就是阻止前进。在这里就是指 CountDownLatch.await() 方法在倒计数为0之前会阻塞当前线程。 19. 什么是 Busy spin？我们为什么要使用它？Busy spin 是一种在不释放 CPU 的基础上等待事件的技术。它经常用于避免丢失 CPU 缓存中的数据（如果线程先暂停，之后在其他CPU上运行就会丢失）。所以，如果你的工作要求低延迟，并且你的线程目前没有任何顺序，这样你就可以通过循环检测队列中的新消息来代替调用 sleep() 或 wait() 方法。它唯一的好处就是你只需等待很短的时间，如几微秒或几纳秒。LMAX 分布式框架是一个高性能线程间通信的库，该库有一个 BusySpinWaitStrategy 类就是基于这个概念实现的，使用 busy spin 循环 EventProcessors 等待屏障。 20. Java 中怎么获取一份线程 dump 文件？在 Linux 下，你可以通过命令 kill -3 PID （Java 进程的进程 ID）来获取 Java 应用的 dump 文件。在 Windows 下，你可以按下 Ctrl + Break 来获取。这样 JVM 就会将线程的 dump 文件打印到标准输出或错误文件中，它可能打印在控制台或者日志文件中，具体位置依赖应用的配置。 21. Swing 是线程安全的？不是，Swing 不是线程安全的。你不能通过任何线程来更新 Swing 组件，如 JTable、JList 或 JPanel，事实上，它们只能通过 GUI 或 AWT 线程来更新。这就是为什么 Swing 提供 invokeAndWait() 和 invokeLater() 方法来获取其他线程的 GUI 更新请求。这些方法将更新请求放入 AWT 的线程队列中，可以一直等待，也可以通过异步更新直接返回结果。 22. 用 wait-notify 写一段代码来解决生产者-消费者问题？记住在同步块中调用 wait() 和 notify()方法，如果阻塞，通过循环来测试等待条件。 23. 用 Java 写一个线程安全的单例模式（Singleton）？当我们说线程安全时，意思是即使初始化是在多线程环境中，仍然能保证单个实例。Java 中，使用枚举作为单例类是最简单的方式来创建线程安全单例模式的方式。 24. Java 中，编写多线程程序的时候你会遵循哪些最佳实践？这是我在写Java 并发程序的时候遵循的一些最佳实践： a）给线程命名，这样可以帮助调试。 b）最小化同步的范围，而不是将整个方法同步，只对关键部分做同步。 c）如果可以，更偏向于使用 volatile 而不是 synchronized。 d）使用更高层次的并发工具，而不是使用 wait() 和 notify() 来实现线程间通信，如 BlockingQueue，CountDownLatch 及 Semeaphore。 e）优先使用并发集合，而不是对集合进行同步。并发集合提供更好的可扩展性。 25. 说出至少 5 点在 Java 中使用线程的最佳实践。这个问题与之前的问题类似，你可以使用上面的答案。对线程来说，你应该： a）对线程命名 b）将线程和任务分离，使用线程池执行器来执行 Runnable 或 Callable。 c）使用线程池 26. 在多线程环境下，SimpleDateFormat 是线程安全的吗？不是，非常不幸，DateFormat 的所有实现，包括 SimpleDateFormat 都不是线程安全的，因此你不应该在多线程序中使用，除非是在对外线程安全的环境中使用，如将 SimpleDateFormat 限制在 ThreadLocal 中。如果你不这么做，在解析或者格式化日期的时候，可能会获取到一个不正确的结果。因此，从日期、时间处理的所有实践来说，我强力推荐 joda-time 库。 27. Happens-Before规则 程序次序规则 按控制流顺序先后发生 管程锁定规则 一个unlock操作先行发生于后面对同一个锁的lock操作 volatile变量规则 对一个volatile变量的写操作先行发生于后面对这个变量的读操作 线程启动规则 start方法先行发生于线程的每一个动作 线程中断规则 对线程的interrupt方法调用先行发生于被中断线程的代码检测到中断时间的发生 线程终止规则 线程内的所有操作都先行发生于对此线程的终止检测 对象终结规则 一个对象的初始化完成先行发生于它的finalize方法的开始 传递性 如果A先行发生于操作B，B先行发生于操作C，则A先行发生于操作C 28. 什么是线程线程是操作系统能够进行运算调度的最小单位，它被包含在进程之中，是进程中的实际运作单位。程序员可以通过它进行多处理器编程，可以使用多线程对运算密集型任务提速。比如，如果一个线程完成一个任务要100 毫秒，那么用十个线程完成改任务只需 10 毫秒。Java在语言层面对多线程提供了很好的支持。 29. 线程和进程有什么区别从概念上： 进程：一个程序对一个数据集的动态执行过程，是分配资源的基本单位。线程：存在于进程内，是进程内的基本调度单位。共享进程的资源。 从执行过程中来看： 进程：拥有独立的内存单元，而多个线程共享内存，从而提高了应用程序的运行效率。线程：每一个独立的线程，都有一个程序运行的入口、顺序执行序列、和程序的出口。但是线程不能够独立的执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 从逻辑角度来看：（重要区别） 多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但是，操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理及资源分配。 简言之，一个程序至少有一个进程,一个进程至少有一个线程。进程是资源分配的基本单位，线程共享进程的资源。 30. 用 Runnable 还是 ThreadJava 不支持类的多重继承，但允许你调用多个接口。所以如果你要继承其他类，当然是实现Runnable接口好了。 31. Java 中 Runnable 和 Callable 有什么不同Runnable和 Callable 都代表那些要在不同的线程中执行的任务。Runnable 从 JDK1.0 开始就有了，Callable 是在 JDK1.5 增加的。它们的主要区别是 Callable 的 call () 方法可以返回值和抛出异常，而 Runnable 的 run ()方法没有这些功能。 32. Java 中 CyclicBarrier 和 CountDownLatch 有什么不同它们都是JUC下的类，CyclicBarrier 和 CountDownLatch 都可以用来让一组线程等待其它线程。区别在于CountdownLatch计数无法被重置。如果需要重置计数，请考虑使用 CyclicBarrier。 33. Java 内存模型是什么Java 内存模型规定和指引Java 程序在不同的内存架构、CPU 和操作系统间有确定性地行为。它在多线程的情况下尤其重要。Java内存模型对一个线程所做的变动能被其它线程可见提供了保证，它们之间是先行发生关系。这个关系定义了一些规则让程序员在并发编程时思路更清晰。 线程内的代码能够按先后顺序执行，这被称为程序次序规则。 对于同一个锁，一个解锁操作一定要发生在时间上后发生的另一个锁定操作之前，也叫做管程锁定规则。 前一个对volatile的写操作在后一个volatile的读操作之前，也叫volatile变量规则。 一个线程内的任何操作必需在这个线程的 start ()调用之后，也叫作线程启动规则。 一个线程的所有操作都会在线程终止之前，线程终止规则。 一个对象的终结操作必需在这个对象构造完成之后，也叫对象终结规则。 a先行于b，b先行于c，传递性 34. 什么是线程安全？Vector 是一个线程安全类吗如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。一个线程安全的计数器类的同一个实例对象在被多个线程使用的情况下也不会出现计算失误。很显然你可以将集合类分成两组，线程安全和非线程安全的。Vector 是用同步方法来实现线程安全的，而和它相似的 ArrayList 不是线程安全的。 35. Java 中什么是竞态条件？ 举个例子说明。竞态条件会导致程序在并发情况下出现一些 bugs。多线程对一些资源的竞争的时候就会产生竞态条件，如果首先要执行的程序竞争失败排到后面执行了，那么整个程序就会出现一些不确定的 bugs。这种 bugs 很难发现而且会重复出现，因为线程间的随机竞争。几类竞态条件check-and-act、读取-修改-写入、put-if-absent。 36. Java 中如何停止一个线程当 run () 或者 call () 方法执行完的时候线程会自动结束，如果要手动结束一个线程，你可以用 volatile 布尔变量来退出 run ()方法的循环或者是取消任务来中断线程。其他情形：异常 - 停止执行 休眠 - 停止执行 阻塞 - 停止执行 37. 一个线程运行时发生异常会怎样简单的说，如果异常没有被捕获该线程将会停止执行。Thread.UncaughtExceptionHandler 是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候 JVM 会使用 Thread.getUncaughtExceptionHandler ()来查询线程的 UncaughtExceptionHandler 并将线程和异常作为参数传递给 handler 的 uncaughtException ()方法进行处理。 38. 如何在两个线程间共享数据？通过共享对象来实现这个目的，或者是使用像阻塞队列这样并发的数据结构 39. Java 中 notify 和 notifyAll 有什么区别notify ()方法不能唤醒某个具体的线程，所以只有一个线程在等待的时候它才有用武之地。而 notifyAll ()唤醒所有线程并允许他们争夺锁确保了至少有一个线程能继续运行。 40. 为什么 wait, notify 和 notifyAll 这些方法不在 thread 类里面一个很明显的原因是 JAVA 提供的锁是对象级的而不是线程级的。如果线程需要等待某些锁那么调用对象中的 wait ()方法就有意义了。如果 wait ()方法定义在 Thread 类中，线程正在等待的是哪个锁就不明显了。简单的说，由于 wait，notify 和 notifyAll 都是锁级别的操作，所以把他们定义在 Object 类中因为锁属于对象。 41. 什么是 FutureTask？在 Java 并发程序中 FutureTask 表示一个可以取消的异步运算。它有启动和取消运算、查询运算是否完成和取回运算结果等方法。只有当运算完成的时候结果才能取回，如果运算尚未完成 get 方法将会阻塞。一个 FutureTask 对象可以对调用了 Callable 和 Runnable 的对象进行包装，由于 FutureTask 也是调用了 Runnable 接口所以它可以提交给 Executor 来执行。 42. Java 中 interrupted 和 isInterruptedd 方法的区别interrupted是静态方法，isInterruptedd是一个普通方法 如果当前线程被中断（没有抛出中断异常，否则中断状态就会被清除），你调用interrupted方法，第一次会返回true。然后，当前线程的中断状态被方法内部清除了。第二次调用时就会返回false。如果你刚开始一直调用isInterrupted，则会一直返回true，除非中间线程的中断状态被其他操作清除了。也就是说isInterrupted 只是简单的查询中断状态，不会对状态进行修改。 43. 为什么 wait 和 notify 方法要在同步块中调用如果不这么做，代码会抛出 IllegalMonitorStateException异常。还有一个原因是为了避免 wait 和 notify 之间产生竞态条件。 44. 为什么你应该在循环中检查等待条件？处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。因此，当一个等待线程醒来时，不能认为它原来的等待状态仍然是有效的，在 notify 方法调用之后和等待线程醒来之前这段时间它可能会改变。这就是在循环中使用 wait 方法效果更好的原因。 45. Java 中的同步集合与并发集合有什么区别同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在 Java1.5 之前程序员们只有同步集合来用且在多线程并发的时候会导致争用，阻碍了系统的扩展性。Java1.5加入了并发集合像 ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分区等现代技术提高了可扩展性。它们大部分位于JUC包下。 46. 什么是线程池？ 为什么要使用它？创建线程要花费昂贵的资源和时间，如果任务来了才创建线程那么响应时间会变长，而且一个进程能创建的线程数有限。为了避免这些问题，在程序启动的时候就创建若干线程来响应处理，它们被称为线程池，里面的线程叫工作线程。从 JDK1.5 开始，Java API 提供了 Executor 框架让你可以创建不同的线程池。比如单线程池，每次处理一个任务；数目固定的线程池或者是缓存线程池（一个适合很多生存期短的任务的程序的可扩展线程池）。 47. 如何写代码来解决生产者消费者问题？在现实中你解决的许多线程问题都属于生产者消费者模型，就是一个线程生产任务供其它线程进行消费，你必须知道怎么进行线程间通信来解决这个问题。比较低级的办法是用 wait 和 notify 来解决这个问题，比较赞的办法是用 Semaphore 或者 BlockingQueue 来实现生产者消费者模型。 48.如何避免死锁？死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起无法完成任务，死锁的发生必须满足以下四个条件： 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁。 49. Java 中活锁和死锁有什么区别？活锁和死锁类似，不同之处在于处于活锁的线程或进程的状态是不断改变的，活锁可以认为是一种特殊的饥饿。一个现实的活锁例子是两个人在狭小的走廊碰到，两个人都试着避让对方好让彼此通过，但是因为避让的方向都一样导致最后谁都不能通过走廊。简单的说就是，活锁和死锁的主要区别是前者进程的状态可以改变但是却不能继续执行。 50. 怎么检测一个线程是否拥有锁在 java.lang.Thread 中有一个方法叫 holdsLock，当且仅当当前线程拥有某个具体对象的锁时它返回true。 51. 你如何在 Java 中获取线程堆栈在 Linux 下，你可以通过命令 kill -3 PID （Java 进程的进程 ID）来获取 Java 应用的 dump 文件。在 Windows 下，你可以按下 Ctrl + Break 来获取。这样 JVM 就会将线程的 dump 文件打印到标准输出或错误文件中，它可能打印在控制台或者日志文件中，具体位置依赖应用的配置。 52.Java 中 synchronized 和 ReentrantLock 有什么不同Java 在过去很长一段时间只能通过 synchronized 关键字来实现互斥，它有一些缺点。比如你不能扩展锁之外的方法或者块边界，尝试获取锁时不能中途取消等。Java 5 通过 Lock 接口提供了更复杂的控制来解决这些问题。 ReentrantLock 类实现了 Lock，它拥有与 synchronized 相同的并发性和内存语义且它还具有可扩展性。 53.有三个线程 T1，T2，T3，怎么确保它们按顺序执行可以用线程类的 join ()方法。具体操作是在T3的run方法中调用t2.join()，让t2执行完再执行t3；T2的run方法中调用t1.join()，让t1执行完再执行t2。这样就按T1，T2，T3的顺序执行了 54.Thread 类中的 yield 方法有什么作用Yield 方法可以暂停当前正在执行的线程对象，让其它有相同优先级的线程执行。它是一个静态方法而且只保证当前线程放弃 CPU 占用而不能保证使其它线程一定能占用 CPU，执行 yield的线程有可能在进入到暂停状态后马上又被执行。 55.Java 中 ConcurrentHashMap 的并发度是什么ConcurrentHashMap 把实际 map 划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是 ConcurrentHashMap 类构造函数的一个可选参数，默认值为 16，这样在多线程情况下就能避免争用。 56.Java 中 Semaphore是什么JUC下的一种新的同步类，它是一个计数信号。从概念上讲，Semaphore信号量维护了一个许可集合。如有必要，在许可可用前会阻塞每一个 acquire，然后再获取该许可。每个 release添加一个许可，从而可能释放一个正在阻塞的获取者。但是，不使用实际的许可对象，Semaphore 只对可用许可的号码进行计数，并采取相应的行动。信号量常常用于多线程的代码中，比如数据库连接池。 57.如果你提交任务时，线程池队列已满。会发会生什么？这个问题问得很狡猾，许多程序员会认为该任务会阻塞直到线程池队列有空位。事实上如果一个任务不能被调度执行那么 ThreadPoolExecutor’s submit ()方法将会抛出一个 RejectedExecutionException 异常。 58.Java 线程池中 submit () 和 execute ()方法有什么区别两个方法都可以向线程池提交任务，execute ()方法的返回类型是 void，它定义在 Executor 接口中， 而 submit ()方法可以返回持有计算结果的 Future 对象，它定义在 ExecutorService 接口中，它扩展了 Executor 接口，其它线程池类像 ThreadPoolExecutor 和 ScheduledThreadPoolExecutor 都有这些方法。 59.什么是阻塞式方法？阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket 的 accept ()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。 60.Swing 是线程安全的吗？你可以很肯定的给出回答，Swing 不是线程安全的。你不能通过任何线程来更新 Swing 组件，如 JTable、JList 或 JPanel，事实上，它们只能通过 GUI 或 AWT 线程来更新。这就是为什么 Swing 提供 invokeAndWait() 和 invokeLater() 方法来获取其他线程的 GUI 更新请求。这些方法将更新请求放入 AWT 的线程队列中，可以一直等待，也可以通过异步更新直接返回结果。 61.Java 中 invokeAndWait 和 invokeLater 有什么区别这两个方法是 Swing API 提供给 Java 开发者用来从当前线程而不是事件派发线程更新 GUI 组件用的。InvokeAndWait ()同步更新 GUI 组件，比如一个进度条，一旦进度更新了，进度条也要做出相应改变。如果进度被多个线程跟踪，那么就调用 invokeAndWait ()方法请求事件派发线程对组件进行相应更新。而 invokeLater ()方法是异步调用更新组件的。 62.Swing API 中那些方法是线程安全的？虽然Swing不是线程安全的但是有一些方法是可以被多线程安全调用的。如repaint ()， revalidate ()。 JTextComponent 的 setText ()方法和 JTextArea 的 insert () 和 append () 方法也是线程安全的。 63.如何在 Java 中创建 Immutable 对象Immutable 对象可以在没有同步的情况下共享，降低了对该对象进行并发访问时的同步化开销。可是 Java 没有@Immutable 这个注解符，要创建不可变类，要实现下面几个步骤：通过构造方法初始化所有成员、对变量不要提供 setter 方法、将所有的成员声明为私有的，这样就不允许直接访问这些成员、在 getter 方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝。 64.Java 中的 ReadWriteLock 是什么？一般而言，读写锁是用来提升并发程序性能的锁分离技术的成果。Java 中的 ReadWriteLock 是 Java 5 中新增的一个接口，一个 ReadWriteLock 维护一对关联的锁，一个用于只读操作一个用于写。在没有写线程的情况下一个读锁可能会同时被多个读线程持有。写锁是独占的，你可以使用 JDK 中的 ReentrantReadWriteLock 来实现这个规则，它最多支持 65535 个写锁和 65535 个读锁。 65.多线程中的忙循环是什么?忙循环就是程序员用循环让一个线程等待，不像传统方法 wait ()， sleep () 或 yield () 它们都放弃了 CPU 控制，而忙循环不会放弃 CPU，它就是在运行一个空循环。这么做的目的是为了保留 CPU 缓存，在多核系统中，一个等待线程醒来的时候可能会在另一个内核运行，这样会重建缓存。为了避免重建缓存和减少等待重建的时间就可以使用它了。 66.volatile 变量和 atomic 变量有什么不同volatile 变量和 atomic 变量看起来很像，但功能却不一样。volatile 变量可以确保先行关系，即写操作会发生在后续的读操作之前， 但它并不能保证原子性。例如用 volatile 修饰 count 变量那么 count++ 操作并不是原子性的。而 AtomicInteger 类提供的 atomic 方法可以让这种操作具有原子性如 getAndIncrement ()方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。 67.如果同步块内的线程抛出异常会发生什么？无论你的同步块是正常还是异常退出的，里面的线程都会释放锁，所以对比锁接口我更喜欢同步块，因为它不用我花费精力去释放锁，该功能可以在 finally block 里释放锁实现。 68.如何在 Java 中创建线程安全的 Singleton5种，急加载，同步方法，双检锁，静态内部类，枚举 69.如何强制启动一个线程？这个问题就像是如何强制进行 Java 垃圾回收，目前还没有觉得方法，虽然你可以使用 System.gc ()来进行垃圾回收，但是不保证能成功。在 Java 里面没有办法强制启动一个线程，它是被线程调度器控制着且 Java 没有公布相关的 API。 70.Java 中的 fork join 框架是什么？fork join 框架是 JDK7 中出现的一款高效的工具，Java 开发人员可以通过它充分利用现代服务器上的多处理器。它是专门为了那些可以递归划分成许多子模块设计的，目的是将所有可用的处理能力用来提升程序的性能。fork join 框架一个巨大的优势是它使用了工作窃取算法，可以完成更多任务的工作线程可以从其它线程中窃取任务来执行。 71.Java 多线程中调用 wait () 和 sleep ()方法有什么不同？Java 程序中 wait 和 sleep 都会造成某种形式的暂停，它们可以满足不同的需要。wait ()方法意味着条件等待，如果等待条件为真且其它线程被唤醒时它会释放锁，而 sleep ()方法仅仅释放 CPU 资源或者让当前线程短暂停顿，但不会释放锁。 72.可重入锁可重入锁：如果当前线程已经获得了某个监视器对象所持有的锁，那么该线程在该方法中调用另外一个同步方法也同样持有该锁。 12345678public synchrnozied void test() &#123; xxxxxx; test2();&#125;public synchronized void test2() &#123; yyyyy;&#125; 在上面代码段中，执行 test 方法需要获得当前对象作为监视器的对象锁，但方法中又调用了 test2 的同步方法。 如果锁是具有可重入性的话，那么该线程在调用 test2 时并不需要再次获得当前对象的锁，可以直接进入 test2 方法进行操作。 如果锁是不具有可重入性的话，那么该线程在调用test2前会等待当前对象锁的释放，实际上该对象锁已被当前线程所持有，不可能再次获得。 如果锁是不具有可重入性特点的话，那么线程在调用同步方法、含有锁的方法时就会产生死锁。 73. 同步方法和同步代码块同步方法默认用this或者当前类class对象作为锁；同步代码块可以选择以什么来加锁，比同步方法要更细颗粒度，我们可以选择只同步会发生同步问题的部分代码而不是整个方法。 四、Java虚拟机0. 对哪些区域回收Java运行时数据区域：程序计数器、JVM栈、本地方法栈、方法区和堆。 由于程序计数器、JVM栈、本地方法栈3个区域随线程而生随线程而灭，对这几个区域内存的回收和分配具有确定性。而方法区和堆则不一样，程序需要在运行时才知道创建哪些对象，对这部分内存的分配是动态的，GC关注的也就是这部分内存。 1. 主动GC调用system.gc() Runtime.getRuntime.gc() 2. 垃圾回收释放那些不在持有任何引用的对象的内存 3. 怎样判断是否需要收集 引用计数法：对象没有任何引用与之关联(无法解决循环引用) ext：Python使用引用计数法 可达性分析法：通过一组称为GC Root的对象为起点,从这些节点向下搜索，如果某对象不能从这些根对象的一个(至少一个)所到达,则判定该对象应当回收。 ext：可作为GCRoot的对象：虚拟机栈中引用的对象。方法区中类静态属性引用的对象，方法区中类常量引用的对象，本地方法栈中JNI引用的对象 4.对象的自我救赎即使在可达性算法中判定为不可达时，也并非一定被回收。对象存在自我救赎的可能。要真正宣告对象的死亡，需要经历2次标记的过程。如果对象经过可达性分析法发现不可达时，对象将被第一次标记被进行筛选，筛选的条件是此对象是否有必要执行finalize方法。如果对象没有重写finalize方法或finalize方法已经被JVM调用过，则判定为不需要执行。 如果对象被判定为需要执行finalize方法，该对象将被放置在一个叫做F-Queue的队列中，JVM会建立一个低优先级的线程执行finalize方法，如果对象想要完成自我救赎需要在finalize方法中与引用链上的对象关联，比如把自己也就是this赋值给某个类变量。当GC第二次对F-Queue中对象标记时，该对象将被移出“即将回收”的集合，完成自我救赎。简言之，finalize方法是对象逃脱死亡命运的最后机会，并且任何对象的finalize方法只会被JVM调用一次。 5.垃圾回收算法Mark-Sweep法：标记清除法 容易产生内存碎片，导致分配较大对象时没有足够的连续内存空间而提前出发GC。这里涉及到另一个问题，即对象创建时的内存分配，对象创建内存分配主要有2种方法，分别是指针碰撞法和空闲列表法。指针碰撞法：使用的内存在一侧，空闲的在另一侧，中间使用一个指针作为分界点指示器，对象内存分配时只要指针向空闲的移动对象大小的距离即可。空闲列表法：使用的和空闲的内存相互交错无法进行指针碰撞，JVM必须维护一个列表记录哪些内存块可用，分配时从列表中找出一个足够的分配给对象，并更新列表记录。所以，当采用Mark-Sweep算法的垃圾回收器时，内存分配通常采用空闲列表法。 Copy法：将内存分为2块，每次使用其中的一块，当一块满了，将存活的对象复制到另一块，把使用过的那一块一次性清除。显然，Copy法解决了内存碎片的问题，但算法的代价是内存缩小为原来的一半。现代的垃圾收集器对新生代采用的正是Copy算法。但通常不执行1:1的策略，HotSpot虚拟机默认Eden区Survivor区8:1。每次使用Eden和其中一块Survivor区。也就是说新生代可用内存为新生代内存空间的90%。 Mark-Compact法：标记整理法。它的第一阶段与Mark-Sweep法一样，但不直接清除，而是将存活对象向一端移动，然后清除端边界以外的内存，这样也不存在内存碎片。 分代收集算法：将堆内存划分为新生代，老年代，根据新生代老年代的特点选取不同的收集算法。因为新生代对象大多朝生夕死，而老年代对象存活率高，没有额外空间进行分配担保，通常对新生代执行复制算法，老年代执行Mark-Sweep算法或Mark-Compact算法。 6.垃圾收集器通常来说，新生代老年代使用不同的垃圾收集器。新生代的垃圾收集器有Serial（单线程）、ParNew（Serial的多线程版本）、ParallelScavenge（吞吐量优先的垃圾收集器），老年代有SerialOld（单线程老年代）、ParallelOld（与ParallelScavenge搭配的多线程执行标记整理算法的老年代收集器）、CMS（标记清除算法，容易产生内存碎片，可以开启内存整理的参数），以及当前最先进的垃圾收集器G1，G1通常面向服务器端的垃圾收集器，在我自己的Java应用程序中通过-XX:+PrintGCDetails，发现自己的垃圾收集器是使用了ParallelScavenge + ParallelOld的组合。 7. 不同垃圾回收算法对比 标记清除法(Mark-Sweeping):易产生内存碎片 复制回收法(Copying)：为了解决Mark-Sweep法而提出,内存空间减至一半 标记压缩法(Mark-Compact):为了解决Copying法的缺陷,标记后移动到一端再清除 分代回收法(GenerationalCollection):新生代对象存活周期短,需要大量回收对象,需要复制的少,执行copy算法;老年代对象存活周期相对长,回收少量对象,执行mark-compact算法.新生代划分：较大的eden区 和 2个survivor区 8. 内存分配 新生代的三部分 |Eden Space|From Space|To Space|，对象主要分配在新生代的Eden区 大对象直接进入老年代大对象比如大数组直接进入老年代，可通过虚拟机参数-XX：PretenureSizeThreshold参数设置 长期存活的对象进入老年代ext：虚拟机为每个对象定义一个年龄计数器，如果对象在Eden区出生并经过一次MinorGC仍然存活，将其移入Survivor的To区，GC完成标记互换后，相当于存活的对象进入From区，对象年龄加1，当增加到默认15岁时，晋升老年代。可通过-XX：MaxTenuringThreshold设置 GC的过程：GC开始前，对象只存在于Eden区和From区，To区逻辑上始终为空。对象分配在Eden区，Eden区空间不足，发起MinorGC，将Eden区所有存活的对象复制到To区，From区存活的对象根据年龄判断去向，若到达年龄阈值移入老年代，否则也移入To区，GC完成后Eden区和From区被清空，From区和To区标记互换。对象每在Survivor区躲过一次MinorGC年龄加一。MinorGC将重复这样的过程，直到To区被填满，To区满了以后，将把所有对象移入老年代。 动态对象年龄判定 suvivor区相同年龄对象总和大于suvivor区空间的一半,年龄大于等于该值的对象直接进入老年代 空间分配担保 在MinorGC开始前，虚拟机检查老年代最大可用连续空间是否大于新生代所有对象总空间，如果成立，MinorGC可以确保是安全的。否则，虚拟机会查看HandlePromotionFailure设置值是否允许担保失败，如果允许，继续查看老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于则尝试MinorGC，尽管这次MinorGC是有风险的。如果小于，或者HandlerPromotionFailure设置不允许，则要改为FullGC。 新生代的回收称为MinorGC,对老年代的回收成为MajorGC又名FullGC 9. 关于GC的虚拟机参数GC相关 -XX:NewSize和-XX:MaxNewSize 新生代大小-XX:SurvivorRatio Eden和其中一个survivor的比值-XX：PretenureSizeThreshold 大对象进入老年代的阈值-XX:MaxTenuringThreshold 晋升老年代的对象年龄 收集器设置-XX:+UseSerialGC:设置串行收集器-XX:+UseParallelGC:设置并行收集器-XX:+UseParalledlOldGC:设置并行年老代收集器-XX:+UseConcMarkSweepGC:设置并发收集器 堆大小设置 -Xmx:最大堆大小-Xms:初始堆大小(最小内存值)-Xmn:年轻代大小-XXSurvivorRatio:3 意思是Eden:Survivor=3:2-Xss栈容量 垃圾回收统计信息 -XX:+PrintGC 输出GC日志-XX:+PrintGCDetails 输出GC的详细日志 10. 方法区的回收方法区通常会与永久代划等号，实际上二者并不等价，只不过是HotSpot虚拟机设计者用永久代实现方法区，并将GC分代扩展至方法区。永久代垃圾回收通常包括两部分内容：废弃常量和无用的类。常量的回收与堆区对象的回收类似，当没有其他地方引用该字面量时，如果有必要，将被清理出常量池。 判定无用的类的3个条件： 1.该类的所有实例都已经被回收，也就是说堆中不存在该类的任何实例 2.加载该类的ClassLoader已经被回收 3.该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 当然，这也仅仅是判定，不代表立即卸载该类。 11. JVM工具命令行 jps(jvm processor status)虚拟机进程状况工具 jstat(jvm statistics monitoring)统计信息监视 jinfo(configuration info for java)配置信息工具 jmap(memory map for java)Java内存映射工具 jhat(JVM Heap Analysis Tool)虚拟机堆转储快照分析工具 jstack(Stack Trace for Java)Java堆栈跟踪工具 HSDIS：JIT生成代码反汇编 可视化 JConsole(Java Monitoring and Management Console):Java监视与管理控制台 VisualVM(All-in-one Java Troubleshooting Tool):多合一故障处理工具 12. JVM内存结构 堆:新生代和年老代 方法区(非堆):持久代, 代码缓存, 线程共享 JVM栈:中间结果,局部变量,线程隔离 本地栈:本地方法(非Java代码) 程序计数器 ：线程私有，每个线程都有自己独立的程序计数器，用来指示下一条指令的地址 注：持久代Java8消失, 取代的称为元空间(本地堆内存的一部分) 13. JVM的方法区与堆一样，是线程共享的区域。方法区中存储：被虚拟机加载的类信息，常量，静态变量，JIT编译后的代码等数据。参见我是一个Java Class。 14. Java类加载器一个jvm中默认的classloader有Bootstrap ClassLoader、Extension ClassLoader、App ClassLoader，分别各司其职： Bootstrap ClassLoader(引导类加载器) 负责加载java基础类，主要是 %JRE_HOME/lib/目录下的rt.jar、resources.jar、charsets.jar等 Extension ClassLoader(扩展类加载器) 负责加载java扩展类，主要是 %JRE_HOME/lib/ext目录下的jar等 App ClassLoader(系统类加载器) 负责加载当前java应用的classpath中的所有类。classloader 加载类用的是全盘负责委托机制。 所谓全盘负责，即是当一个classloader加载一个Class的时候，这个Class所依赖的和引用的所有 Class也由这个classloader负责载入，除非是显式的使用另外一个classloader载入。所以，当我们自定义的classloader加载成功了com.company.MyClass以后，MyClass里所有依赖的class都由这个classLoader来加载完成。 15. 64 位 JVM 中，int 的长度是多大？Java 中，int 类型变量的长度是一个固定值，与平台无关，都是 32 位。意思就是说，在 32 位 和 64 位 的Java 虚拟机中，int 类型的长度是相同的。 16. Serial 与 Parallel GC之间的不同之处？Serial 与 Parallel 在GC执行的时候都会引起 stop-the-world。它们之间主要不同 serial 收集器是默认的复制收集器，执行 GC 的时候只有一个线程，而 parallel 收集器使用多个 GC 线程来执行。 17.Java 中 WeakReference 与 SoftReference的区别？Java中一共有四种类型的引用。StrongReference、 SoftReference、 WeakReference 以及 PhantomReference。 StrongReference：Java 的默认引用实现, 它会尽可能长时间的存活于 JVM 内，当没有任何对象指向它时将会被GC回收 SoftReference：尽可能长时间保留引用，直到JVM内存不足，适合某些缓存应用 WeakReference：顾名思义, 是一个弱引用, 当所引用的对象在 JVM 内不再有强引用时, 下一次将被GC回收 PhantomReference：它是最弱的一种引用关系，也无法通过PhantomReference取得对象的实例。仅用来当该对象被回收时收到一个通知 虽然 WeakReference 与 SoftReference 都有利于提高 GC 和 内存的效率，但是 WeakReference ，一旦失去最后一个强引用，就会被 GC 回收，而 SoftReference 会尽可能长的保留引用直到 JVM 内存不足时才会被回收(虚拟机保证), 这一特性使得 SoftReference 非常适合缓存应用。 18. WeakHashMap 是怎么工作的？WeakHashMap 的工作与正常的 HashMap 类似，但是使用弱引用作为 key，意思就是当 key 对象没有任何引用时，key/value 将会被回收。 19. JVM 选项 -XX:+UseCompressedOops 有什么作用？为什么要使用？当你将你的应用从 32 位的 JVM 迁移到 64 位的 JVM 时，由于对象的指针从 32 位增加到了 64 位，因此堆内存会突然增加，差不多要翻倍。这也会对 CPU 缓存（容量比内存小很多）的数据产生不利的影响。因为，迁移到 64 位的 JVM 主要动机在于可以指定最大堆大小，通过压缩 OOP 可以节省一定的内存。通过 -XX:+UseCompressedOops 选项，JVM 会使用 32 位的 OOP，而不是 64 位的 OOP。 20. 怎样通过 Java 程序来判断 JVM 是 32 位 还是 64 位？你可以检查某些系统属性如 sun.arch.data.model 或 os.arch 来获取该信息。 21. 32 位 JVM 和 64 位 JVM 的最大堆内存分别是多数？理论上说上 32 位的 JVM 堆内存可以到达 2^32，即 4GB，但实际上会比这个小很多。不同操作系统之间不同，如 Windows 系统大约 1.5 GB，Solaris 大约 3GB。64 位 JVM允许指定最大的堆内存，理论上可以达到 2^64，这是一个非常大的数字，实际上你可以指定堆内存大小到 100GB。甚至有的 JVM，如 Azul，堆内存到 1000G 都是可能的。 22. JRE、JDK、JVM 及 JIT 之间有什么不同？JRE 代表 Java 运行时（Java run-time），是运行 Java 应用所必须的。JDK 代表 Java 开发工具（Java development kit），是 Java 程序的开发工具，如 Java 编译器，它也包含 JRE。JVM 代表 Java 虚拟机（Java virtual machine），它的责任是运行 Java 应用。JIT 代表即时编译（Just In Time compilation），当代码执行的次数超过一定的阈值时，会将 Java 字节码转换为本地代码，如，主要的热点代码会被准换为本地代码，这样有利大幅度提高 Java 应用的性能。 23. 解释 Java 堆空间及 GC？当通过 Java 命令启动 Java 进程的时候，会为它分配内存。内存的一部分用于创建堆空间，当程序中创建对象的时候，就从对空间中分配内存。GC 是 JVM 内部的一个后台进程，回收无效对象的内存用于将来的分配。 24. 你能保证 GC 执行吗？不能，虽然你可以调用 System.gc() 或者 Runtime.getRuntime().gc()，但是没有办法保证 GC 的执行。 25. 怎么获取 Java 程序使用的内存？堆使用的百分比？可以通过 java.lang.Runtime 类中与内存相关方法来获取剩余的内存，总内存及最大堆内存。通过这些方法你也可以获取到堆使用的百分比及堆内存的剩余空间。Runtime.freeMemory() 方法返回剩余空间的字节数，Runtime.totalMemory() 方法总内存的字节数，Runtime.maxMemory() 返回最大内存的字节数。 26. Java 中堆和栈有什么区别？JVM 中堆和栈属于不同的内存区域，使用目的也不同。栈常用于保存方法帧和局部变量，而对象总是在堆上分配。栈通常都比堆小，也不会在多个线程之间共享，而堆被整个 JVM 的所有线程共享。 27. JVM调优使用工具Jconsol、VisualVM、JProfiler等 堆信息查看 可查看堆空间大小分配（年轻代、年老代、持久代分配）提供即时的垃圾回收功能垃圾监控（长时间监控回收情况） 查看堆内类、对象信息查看：数量、类型等 对象引用情况查看 有了堆信息查看方面的功能，我们一般可以顺利解决以下问题： 年老代年轻代大小划分是否合理 内存泄漏 垃圾回收算法设置是否合理 线程监控 线程信息监控：系统线程数量。线程状态监控：各个线程都处在什么样的状态下 Dump线程详细信息：查看线程内部运行情况死锁检查 热点分析 CPU热点：检查系统哪些方法占用的大量CPU时间内存热点：检查哪些对象在系统中数量最大（一定时间内存活对象和销毁对象一起统计） 快照系统两个不同运行时刻，对象（或类、线程等）的不同举例说，我要检查系统进行垃圾回收以后，是否还有该收回的对象被遗漏下来的了。那么，我可以在进行垃圾回收前后，分别进行一次堆情况的快照，然后对比两次快照的对象情况。 内存泄漏检查 年老代堆空间被占满持久代被占满堆栈溢出线程堆栈满系统内存被占满 28. Java中有内存泄漏吗？内存泄露的定义: 当某些对象不再被应用程序所使用,但是由于仍然被引用而导致垃圾收集器不能释放。 内存泄漏的原因：对象的生命周期不同。比如说对象A引用了对象B. A的生命周期比B的要长得多，当对象B在应用程序中不会再被使用以后, 对象 A 仍然持有着B的引用. (根据虚拟机规范)在这种情况下GC不能将B从内存中释放。这种情况很可能会引起内存问题，倘若A还持有着其他对象的引用,那么这些被引用的(无用)对象也不会被回收,并占用着内存空间。甚至有可能B也持有一大堆其他对象的引用。这些对象由于被B所引用,也不会被垃圾收集器所回收，所有这些无用的对象将消耗大量宝贵的内存空间。并可能导致内存泄漏。 怎样防止：1、当心集合类, 比如HashMap, ArrayList等,因为这是最容易发生内存泄露的地方.当集合对象被声明为static时,他们的生命周期一般和整个应用程序一样长。 29. OOM解决办法内存溢出的空间：Permanent Generation和Heap Space，也就是永久代和堆区 1、永久代的OOM 解决办法有2种：a.通过虚拟机参数-XX：PermSize和-XX：MaxPermSize调整永久代大小b.清理程序中的重复的Jar文件，减少类的重复加载 2、堆区的溢出 发生这种问题的原因是java虚拟机创建的对象太多，在进行垃圾回收之间，虚拟机分配的到堆内存空间已经用满了，与Heap Space的size有关。解决这类问题有两种思路： 检查程序，看是否存在死循环或不必要地重复创建大量对象，定位原因，修改程序和算法。 通过虚拟机参数-Xms和-Xmx设置初始堆和最大堆的大小 30. DirectMemory直接内存直接内存并不是Java虚拟机规范定义的内存区域的一部分，但是这部分内存也被频繁使用，而且也可能导致OOM异常的出现。 JDK1.4引入了NIO，这是一种基于通道和缓冲区的非阻塞IO模式，它可以使用Native函数库分配直接堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作，使得在某些场合显著提高性能，因为它避免了在Java堆和本地堆之间来回复制数据。 31. Java 中堆和栈有什么不同每个线程都有自己的栈内存，用于存储本地变量，方法参数和栈调用，一个线程中存储的变量对其它线程是不可见的。而堆是所有线程共享的一片公用内存区域。对象都在堆里创建，为了提升效率线程会从堆中弄一个缓存到自己的栈，如果多个线程使用该变量就可能引发问题，这时 volatile 变量就可以发挥作用了，它要求线程从主存中读取变量的值。 32. 双亲委派模型中的方法findLoadedClass(),LoadClass(),findBootstrapClassOrNull(),findClass(),resolveClass() 33. IO模型一般来说 I/O 模型可以分为：同步阻塞，同步非阻塞，异步阻塞，异步非阻塞 四种IO模型 同步阻塞 IO ：在此种方式下，用户进程在发起一个 IO 操作以后，必须等待 IO 操作的完成，只有当真正完成了 IO 操作以后，用户进程才能运行。 JAVA传统的 IO 模型属于此种方式！ 同步非阻塞 IO：在此种方式下，用户进程发起一个 IO 操作以后可返回做其它事情，但是用户进程需要时不时的询问 IO 操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的 CPU 资源浪费。其中目前 JAVA 的 NIO 就属于同步非阻塞 IO 。 异步阻塞 IO：此种方式下是指应用发起一个 IO 操作以后，不等待内核 IO 操作的完成，等内核完成 IO 操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问 IO 是否完成，那么为什么说是阻塞的呢？因为此时是通过 select 系统调用来完成的，而 select 函数本身的实现方式是阻塞的，而采用 select 函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！ 异步非阻塞 IO：在此种模式下，用户进程只需要发起一个 IO 操作然后立即返回，等 IO 操作真正的完成以后，应用程序会得到 IO 操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的 IO 读写操作，因为 真正的 IO读取或者写入操作已经由 内核完成了。目前 Java7的AIO正是此种类型。 BIO即同步阻塞IO，适用于连接数目较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4之前的唯一选择，但程序直观、简单、易理解。 NIO即同步非阻塞IO，适用于连接数目多且连接比较短的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。 AIO即异步非阻塞IO，适用于连接数目多且连接比较长的架构，如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK1.7开始支持 34. 类加载器按照层次，从顶层到底层，分别加载哪些类？启动类加载器：负责将存放在JAVA_HOME/lib下的，或者被－Xbootclasspath参数所指定的路径中的，并且是虚拟机识别的类库加载到虚拟机内存中。启动类加载器无法被Java程序直接引用。 扩展类加载器：这个加载器负责加载JAVA_HOME/lib/ext目录中的，或者被java.ext.dirs系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器 应用程序类加载器：这个加载器是ClassLoader中getSystemClassLoader()方法的返回值，所以一般也称它为系统类加载器。它负责加载用户类路径（Classpath）上所指定的类库，可直接使用这个加载器，如果应用程序没有自定义自己的类加载器，一般情况下这个就是程序中默认的类加载器 实现自己的加载器 只需要继承ClassLoader，并覆盖findClass方法。在调用loadClass方法时，会先根据委派模型在父加载器中加载，如果加载失败，则会调用自己的findClass方法来完成加载 五、数据库（Sql、MySQL、Redis等）1. Statement1.1 基本内容 Statement是最基本的用法, 不传参, 采用字符串拼接，存在注入漏洞 PreparedStatement传入参数化的sql语句, 同时检查合法性, 效率高可以重用, 防止sql注入 CallableStatement接口扩展PreparedStatement，用来调用存储过程 BatchedStatement用于批量操作数据库，BatchedStatement不是标准的Statement类 12public interface CallableStatement extends PreparedStatement public interface PreparedStatement extends Statement 1.2 Statement与PrepareStatement的区别 创建时的区别 12Statement statement = conn.createStatement();PreparedStatement preStatement = conn.prepareStatement(sql); 执行的时候 12ResultSet rSet = statement.executeQuery(sql);ResultSet pSet = preStatement.executeQuery(); 由上可以看出，PreparedStatement有预编译的过程，已经绑定sql，之后无论执行多少遍，都不会再去进行编译，而 statement 不同，如果执行多遍，则相应的就要编译多少遍sql，所以从这点看，preStatement 的效率会比 Statement要高一些 安全性 PreparedStatement是预编译的，所以可以有效的防止SQL注入等问题 代码的可读性和可维护性 PreparedStatement更胜一筹 2. 游标3. 列出 5 个应该遵循的 JDBC 最佳实践有很多的最佳实践，你可以根据你的喜好来例举。下面是一些更通用的原则： a）使用批量的操作来插入和更新数据b）使用 PreparedStatement 来避免 SQL 异常，并提高性能c）使用数据库连接池d）通过列名来获取结果集，不要使用列的下标来获取 4. 数据库索引的实现数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 B树： 一棵m阶B树(balanced tree of order m)是一棵平衡的m路搜索树。它或者是空树，或者是满足下列性质的树： 1、根结点至少有两个子女；2、每个非根节点所包含的关键字个数 j 满足：┌m/2┐ - 1 &lt;= j &lt;= m - 1；3、除根结点以外的所有结点（不包括叶子结点）的度数正好是关键字总数加1，故内部子树个数 k 满足：┌m/2┐ &lt;= k &lt;= m ；4、所有的叶子结点都位于同一层。 由于B-Tree的特性，在B-Tree中按key检索数据的算法非常直观：首先从根节点进行二分查找，如果找到则返回对应节点的data，否则对相应区间的指针指向的节点递归进行查找，直到找到节点或找到null指针，前者查找成功，后者查找失败。 一个度为d的B-Tree，设其索引N个key，则其树高h的上限为logd((N+1)/2)，检索一个key，其查找节点个数的渐进复杂度为O(logdN)。从这点可以看出，B-Tree是一个非常有效率的索引数据结构。 B+树： B-Tree有许多变种，其中最常见的是B+Tree，例如MySQL就普遍使用B+Tree实现其索引结构。 B+树是B树的变形，它把所有的data都放在叶子结点中，只将关键字和子女指针保存于内结点，内结点完全是索引的功能。 与B-Tree相比，B+Tree有以下不同点： 1、每个节点的指针上限为2d而不是2d+1。 2、内节点不存储data，只存储key；叶子节点存储data不存储指针。 一般在数据库系统或文件系统中使用的B+Tree结构都在经典B+Tree的基础上进行了优化，增加了顺序访问指针。 在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针 例如图4中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 为什么B树（B+树）？ 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。 这涉及到磁盘存取原理、局部性原理和磁盘预读。 先从B-Tree分析，根据B-Tree的定义，### 可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。**为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。 综上所述，用B-Tree作为索引结构效率是非常高的。 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。 至于B+Tree为什么更适合外存索引，原因和内节点出度d有关。 由于B+Tree内节点去掉了data域，因此可以拥有更大的出度，拥有更好的性能。 六、算法与数据结构1. 二叉搜索树(Binary Search Tree又名：二叉查找树,二叉排序树)它或者是一棵空树,或者是具有下列性质的二叉树： 若它的左子树不空,则左子树上所有结点的值均小于它的根结点的值；若它的右子树不空,则右子树上所有结点的值均大于它的根结点的值；它的左、右子树也分别为二叉搜索树。 2. RBT红黑树红黑树是一棵二叉搜索树，它在每个结点上增加一个存储位来表示结点的颜色，可以是RED或BLACK。通过对任何一条从根到叶子的简单路径上各个结点的颜色进行约束，红黑树没有一条路径会比其他路径长出2倍，所以红黑树是近似平衡的，使得红黑树的查找、插入、删除等操作的时间复杂度最坏为O(log n)，但需要注意到在红黑树上执行插入或删除后将不在满足红黑树性质，恢复红黑树的属性需要少量(O(logn))的颜色变更(实际是非常快速的)和不超过三次树旋转(对于插入操作是两次)。虽然插入和删除很复杂，但操作时间仍可以保持为 O(log n) 次。具体如何保证？引出红黑树的5个性质。 红黑树的5个性质：满足以下五个性质的二叉搜索树 每个结点或是红色的或是黑色的 根结点是黑色的 每个叶结点是黑色的 如果一个结点是红色的,则它的两个子结点是黑色的 对于每个结点,从该结点到其后代叶结点的简单路径上,均包含相同数目的黑色结点 插入操作： 由于性质的约束，插入的结点都是红色的。插入时性质1、3始终保持。破坏性质2当且仅当当前插入结点为根节点。变一下颜色即可。如果是破坏性质4或5，则需要旋转和变色来继续满足红黑树的性质。下面说一说插入的几种情况，约定当前插入结点为N，其父结点为P，叔叔为U，祖父为G 情形1：树空，直接插入违反性质1，将红色改黑。 情形2：N的父结点为黑，不必修改，直接插入 从情形3开始的情形假定N结点的父结点P为红色，所以存在G，并且G为黑色。且N存在一个叔叔结点U，尽管U可能为叶结点。 情形3：P为红，U为红（G结点一定存在且为黑）这里不论P是G的左孩子还是右孩子；不论N是P的左孩子还是右孩子。 首先把P、U改黑，G改红，并以G作为一个新插入的红结点重新进行各种情况的检查，若一路检索至根节点还未结束，则将根结点变黑。 情形4：P为红，U为黑或不存在（G结点一定存在且为黑），且P为G的左孩子，N为P的左孩子（或者P为G的右孩子，N为P的右孩子，保证同向的）。P、G右旋并将P、G变相反色。因为P取代之前黑G的位置，所以P变黑可以理解，而G变红是为了不违反性质5。 情形5：P为红，U为黑或不存在，且P为G的左孩子，N为P的右孩子（或P为G的右孩子，N为P的左孩子，保证是反向的），对N，P进行一次左旋转换为情形4 删除操作比插入复杂一些，但最多不超过三次旋转可以让红黑树恢复平衡。 其他 黑高从某个结点x出发(不含x)到达一个叶结点的任意一条简单路径上的黑色结点个数称为该结点的黑高。红黑树的黑高为其根结点的黑高。 一个具有n个内部结点的红黑树的高度h&lt;=2lg(n+1) 结点的属性(五元组):color key left right p 动态集合操作最坏时间复杂度为O(lgn) 3. 排序算法 稳定排序:插入排序、冒泡排序、归并排序、基数排序 插入排序[稳定]适用于小数组,数组已排好序或接近于排好序速度将会非常快复杂度：O(n^2) - O(n) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 归并排序[稳定]采用分治法复杂度：O(nlogn) - O(nlgn) - O(nlgn) - O(n)[平均 - 最好 - 最坏 - 空间复杂度] 冒泡排序[稳定]复杂度：O(n^2) - O(n) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 基数排序 分配+收集[稳定]复杂度： O(d(n+r)) r为基数d为位数 空间复杂度O(n+r) 树排序[不稳定]应用：TreeSet的add方法、TreeMap的put方法不支持相同元素,没有稳定性问题复杂度：平均最差O(nlogn) 堆排序(就地排序)[不稳定]复杂度：O(nlogn) - O(nlgn) - O(nlgn) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 快速排序[不稳定]复杂度：O(nlgn) - O(nlgn) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度]栈空间0(lgn) - O(n) 选择排序[不稳定]复杂度：O(n^2) - O(n^2) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 希尔排序[不稳定]复杂度 小于O(n^2) 平均 O(nlgn) 最差O(n^s)[1&lt;s&lt;2] 空间O(1) 4. 查找与散列4.1 散列函数设计 直接定址法:f(key) = a*key+b 简单、均匀,不易产生冲突。但需事先知道关键字的分布情况,适合查找表较小且连续的情况,故现实中并不常用 除留余数法:f(key) = key mod p (p&lt;=m) p取小于表长的最大质数 m为表长 DJBX33A算法(time33哈希算法hash = hash*33+(unsigned int)str[i]; 平方取中法 折叠法 更多…. 4.2 冲突处理 闭散列(开放地址方法):要求装填因子a较小，闭散列方法把所有记录直接存储在散列表中 线性探测:易产生堆积现象(基地址不同堆积在一起) 二次探测:f(key) = (f(key)+di) % m di=1^2,-1^2,2^2,-2^2…可以消除基本聚集 随机探测:f(key) = (f(key)+di),di采用随机函数得到,可以消除基本聚集 双散列:避免二次聚集 开散列(链地址法):原地处理 同义词记录存储在一个单链表中,散列表中子存储单链表的头指针。 优点:无堆积 事先无需确定表长 删除结点易于实现 装载因子a&gt;=1,缺点:需要额外空间 5. 跳表为什么选择跳表？ 目前经常使用的平衡数据结构有：B树，红黑树，AVL树，Splay Tree, Treep等。想象一下，给你一张草稿纸，一只笔，一个编辑器，你能立即实现一颗红黑树，或者AVL树出来吗？ 很难吧，这需要时间，要考虑很多细节，要参考一堆算法与数据结构之类的树，还要参考网上的代码，相当麻烦。用跳表吧，跳表是一种随机化的数据结构，目前开源软件 Redis 和 LevelDB 都有用到它，它的效率和红黑树以及 AVL 树不相上下，但跳表的原理相当简单，只要你能熟练操作链表，就能去实现一个 SkipList。 跳跃表是一种随机化数据结构，基于并联的链表，其效率可比拟于二叉查找树(对于大多数操作需要O(log n)平均时间)，并且对并发算法友好。 Skip list(跳表）是一种可以代替平衡树的数据结构，默认是按照Key值升序的。Skip list让已排序的数据分布在多层链表中，以0-1随机数决定一个数据的向上攀升与否，是一种“空间来换取时间”的一个算法，在每个节点中增加了指向下一层的指针，在插入、删除、查找时可以忽略一些不可能涉及到的结点，从而提高了效率。 在Java的API中已经有了实现：分别是ConcurrentSkipListMap(在功能上对应HashTable、HashMap、TreeMap) ；ConcurrentSkipListSet(在功能上对应HashSet) Skip list的性质(1) 由很多层结构组成，level是通过一定的概率随机产生的(2) 每一层都是一个有序的链表，默认是升序(3) 最底层(Level 1)的链表包含所有元素(4) 如果一个元素出现在Level i 的链表中，则它在Level i 之下的链表也都会出现(5) 每个节点包含两个指针，一个指向同一链表中的下一个元素，一个指向下面一层的元素时间复杂度O(lgn) 最坏O(2lgn) 6. AVL树1.LL型 在某一节点的左孩子的左子树上插入一个新的节点，使得该节点不再平衡。举例 A B Ar Bl Br 在Bl下插入N，执行一次右旋即可，即把B变为父结点，原来的根节点A变为B的左孩子，B的右子树变为A的左子树。 2.RR型 与LL型是对称的，执行一次左旋即可。 3.LR型 指在AVL树某一结点左孩子的右子树上插入一个结点，使得该节点不在平衡。这时需要两次旋转，先左旋再右旋。 4.RL型 与LR对称，执行一次右旋，再执行一次左旋。 删除 1、被删的节点是叶子节点 将该节点直接从树中删除，并利用递归的特点和高度的变化，反向推算其父节点和祖先节点是否失衡。 2、被删的节点只有左子树或只有右子树 将左子树（右子树）替代原有节点的位置，并利用递归的特点和高度的变化，反向推算父节点和祖先节点是否失衡。 3、被删的节点既有左子树又有右子树 找到被删节点的左子树的最右端的节点，将该结点的的值赋给待删除结点，再用该结点的左孩子替换它本来的位置，然后释放该结点，并利用递归特点，反向推断父节点和祖父节点是否失衡。 7. 一致性Hash第一：简单介绍一致性哈希算法是分布式系统中常用的算法。比如，一个分布式的存储系统，要将对象存储到具体的节点上，如果采用普通的hash方法，将数据映射到具体的节点上，如key%N，N是机器节点数。 1、考虑到比如一个服务器down掉，服务器结点N变为N-1，映射公式必须变为key%(N-1) 2、访问量加重，需要添加服务器结点，N变为N+1，映射公式变为hash(object)%(N+1) 当出现1,2的情况意味着我们的映射都将无效，对服务器来说将是一场灾难，尤其是对缓存服务器来说，因为缓存服务器映射的失效，洪水般的访问都将冲向后台服务器。 第二点：hash算法的单调性 Hash 算法的一个衡量指标是单调性，单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。 consistent hash 也是一种hash 算法，简单的说，在移除 / 添加一个结点时，它能够尽可能小的改变已存在的映射关系，尽可能的满足单调性的要求。 第三点：将对象和服务器结点分别映射到环型空间 通常的一致性哈希做法是将 value 映射到一个 32 位的 key 值，也即是 0~2^32-1 次方的数值空间；我们可以将这个空间想象成一个首（ 0 ）尾（ 2^32-1 ）相接的圆环。 我们可以通过hash函数将我们的key映射到环型空间中，同时根据相同的哈希算法把服务器也映射到环型空间中，顺便提一下服务器或者某个计算节点的 hash 计算，一般的方法可以使用机器的 IP 地址或者机器名作为 hash 输入。 第四点：将对象映射到服务器 在这个环形空间中，如果沿着顺时针方向从对象的 key 值出发，直到遇见一个 服务器结点，那么就将该对象存储在这个服务器结点上，因为对象和服务器的hash 值是固定的，因此这个 cache 必然是唯一和确定的。 这时候考察某个服务器down机或者需要添加服务器结点，也就是移除和添加的操作，我们只需要几个对象的映射。 第五点：虚拟结点 Hash 算法的另一个指标是平衡性 (Balance)。平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。 对于上述的做法，可能导致某些对象都映射到某个服务器，使得分布不平衡。为此可以采用“虚拟结点”的做法。 “虚拟结点”（ virtual node ）是实际节点在 hash 空间的复制品，一实际结点对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以 hash 值排列。引入“虚拟结点”会让我们的映射分布更为平衡一些。 引入“虚拟结点”前：Hash(“192.168.1.1”); 引入“虚拟结点”后：Hash(“192.168.1.1#1”);Hash(“192.168.1.1#2”); 8. 如何判断链表是否有环方法1：快慢指针法 2.设两个工作指针p、q，p总是向前走，但q每次都从头开始走，对于每个节点，看p走的步数是否和q一样。比如p从A走到D，用了4步，而q则用了14步。因而步数不等，出现矛盾，存在环。 9. 熟悉哪些算法？ [哈希算法] 一致性哈希 time33哈希 FNV1_32_HASH [排序算法] 快速排序 [搜索算法] DFS BFS [最小生成树算法] Kruskal Prim [最短路径算法] Dijkstra Floyed 七、计算机网络1.停止等待协议停止等待协议是最基本的数据链路层协议，它的工作原理是这样的。在发送端，每发送完一帧就停止发送，等待接收端的确认，如果收到确认就发送下一帧。在接收端，每收到一个无差错的帧，就把这个帧交付上层并向发送端发送确认。若该帧有差错，就丢弃，其他什么也不做。 其他细节：停止等待协议为了可靠交付，需要对帧进行编号，由于每次只发送一帧，所以停止等待协议使用1个比特编号，编号0和1停止等待协议会出现死锁现象（A等待B的确认），解决办法，启动超时计时器，超时计时器有一个重传时间。重传时间一般选择略大于“正常情况下从发完数据帧到收到确认帧所需的平均时间”。 2.滑动窗口协议再说滑动窗口之前，先说下连续ARQ，连续ARQ又称Go-back-N ARQ，意思是当出现差错必须重传时，要向回走N个帧，然后再开始重传，也就意味着只要有一帧出现差错，即使已经正确的帧也需要重传，白白浪费时间，增大开销。为此，应该对发送出去但未被确认的帧的数目加以限制，这就是滑动窗口协议。滑动窗口指收发两端分别维护一个发送窗口和接收窗口，发送窗口有一个窗口值Wt，窗口值Wt代表在没有收到对方确认的情况下最多可以发送的帧的数目。当发送的帧的序号被接收窗口正确收下后，接收端向前滑动并向发送端发去确认，发送端收到确认后，发送窗口向前滑动。收发两端按规律向前推进。 连续ARQ和选择重传ARQ均是窗口大于1的滑动窗口协议，而停止等待协议相当于收发两端窗口等于1。 滑动窗口指接收和发送两端的窗口按规律不断向前推进，是一种流量控制的策略。 3.Http1.0和Http1.1的区别 HTTP/1.0协议使用非持久连接,即在非持久连接下,一个tcp连接只传输一个Web对象。 HTTP/1.1默认使用持久连接(然而,HTTP/1.1协议的客户机和服务器可以配置成使用非持久连接)。在持久连接下,不必为每个Web对象的传送建立一个新的连接,一个连接中可以传输多个对象。 4.Post和Get的区别1.安全性上说：get的方式是把数据在地址栏中明文的形式发送，URL中可见，POST方式对用户是透明的，安全性更高。2.数据量说：Get传送的数据量较小，一般不能大于2KB，POST传送的数据量更大。3.适用范围说：查询用Get，数据添加、修改和删除建议Post 5. TCP/IP体系各层功能及协议TCP/IP体系共有四个层次，分别为网络接口层Host-to-Network Layer, 网际层 Internet Layer， 传输层Transport Layer，应用层Application Layer。 网络接口层 -&gt; 接收和发送数据报主要负责将数据发送到网络传输介质上以及从网络上接收TCP/IP数据报，相当于OSI参考模型的物理层和数据链路层。在实际中，先后流行的以太网、令牌环网、ATM、帧中继等都可视为其底层协议。它将发送的信息组装成帧并通过物理层向选定网络发送，或者从网络上接收物理帧，将去除控制信息后的IP数据报交给网络层。 网际层 -&gt; 数据报封装和路由寻址网际层主要功能是寻址和对数据报的封装以及路由选择功能。这些功能大部分通过IP协议完成，并通过地址解析协议ARP、逆地址解析协议RARP、因特网控制报文协议ICMP、因特网组管理协议IGMP从旁协助。所以IP协议是网络层的核心。 网际协议IP：IP协议是一个无连接的协议，主要负责将数据报从源结点转发到目的结点。也就是说IP协议通过对数据报中源地址和目的地址进行分析，然后进行路由选择，最后再转发到目的地。需要注意的是：IP协议只负责对数据进行转发，并不对数据进行检查，也就是说，它不负责数据的可靠性，这样设计的主要目的是提高IP协议传送和转发数据的效率。 ARP：该协议负责将IP地址解析转换为计算机的物理地址。 虽然我们使用IP地址进行通信，但IP地址只是主机在抽象的网络层中的地址。最终要传到数据链路层封装成MAC帧才能发送到实际的网络。因此不管使用什么协议最终需要的还是硬件地址。 每个主机拥有一个ARP高速缓存（存放所在局域网内主机和路由器的IP地址到硬件地址的映射表） 举例：A发送B (1)A在自己的ARP高速缓存中查到B的MAC地址，写入MAC帧发往此B (2)没查到，A向本局域网广播ARP请求分组，内容包括自己的地址映射和B的IP地址 (3)B发送ARP响应分组，内容为自己的IP地址到物理地址的映射，同时将A的映射写入自己的ARP高速缓存（单播的方式） 注：ARP Cache映射项目具有一个生存时间。 RARP：将计算机物理地址转换为IP地址 ICMP：该协议主要负责发送和传递包含控制信息的数据报，这些控制信息包括了哪台计算机出现了什么错误，网络路由出现了什么错误等内容。 传输层 -&gt; 应用进程间端到端的通信传输层主要负责应用进程间“端到端”的通信，即从某个应用进程传输到另一个应用进程，它与OSI参考模型的传输层功能类似。传输层在某个时刻可能要同时为多个不同的应用进程服务，因此传输层在每个分组中必须增加用于识别应用进程的标识，即端口。TCP/IP体系的传输层主要包含两个主要协议，即传输控制协议TCP和用户数据报协议UDP。TCP协议是一种可靠的、面向连接的协议，保证收发两端有可靠的字节流传输，进行了流量控制，协调双方的发送和接收速度，达到正确传输的目的。UDP是一种不可靠的、无连接的协议，其特点是协议简单、额外开销小、效率较高，不能保证可靠传输。传输层提供应用进程间的逻辑通信。它使应用进程看见的就好像是在两个运输层实体间一条端到端的逻辑通信信道。当运输层采用TCP时，尽管下面的网络是不可靠的，但这种逻辑通信信道相当于一条全双工的可靠信道。可以做到报文的无差错、按序、无丢失、无重复。 注：单单面向连接只是可靠的必要条件，不充分。还需要其他措施，如确认重传，按序接收，无丢失无重复。 熟知端口： 12345678920 FTP数据连接 21 FTP控制连接 22 SSH 23 TELNET 25 SMTP 53 DNS 69 TFTP80 HTTP161 SNMP UDP重要 UDP的优点： 1.发送之前无需建立连接，减小了开销和发送数据的时延2.UDP不使用连接，不使用可靠交付，因此主机不需要维护复杂的参数表、连接状态表3.UDP用户数据报只有8个字节的首部开销，而TCP要20字节。4.由于没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（IP电话等实时应用要求源主机以恒定的速率发送数据是有利的） Table，使用TCP和UDP的应用 应用 应用层协议 运输层协议 名字转换 DNS UDP 文件传送 TFTP UDP 路由选择协议 RIP UDP IP地址配置 BOOTTP,DHCP UDP 网络管理 SNMP UDP 远程文件服务器 NFS UDP IP电话 专用协议 UDP 流式多媒体通信 专用协议 UDP 电子邮件 SMTP TCP 远程终端接入 TELNET TCP 万维网 HTTP TCP 文件传送 FTP TCP 注：TFTP：Trivial File Transfer Protocol UDP的过程（以TFTP举例）： 服务器进程运行着，等待TFTP客户进程的服务请求。客户端TFTP进程启动时，向操作系统申请一个临时端口号，然后操作系统为该进程创建2个队列，入队列和出队列。只要进程在执行，2个队列一直存在。 客户进程将报文发送到出队列中。UDP按报文在队列的先后顺序发送。在传送到IP层之前给报文加上UDP首部，其中目的端口后为69，然后发给IP层。出队列若溢出，则操作系统通知应用层TFTP客户进程暂停发送。 客户端收到来自IP层的报文时，UDP检查报文中目的端口号是否正确，若正确，放入入队列队尾，客户进程按先后顺序一一取走。若不正确，UDP丢弃该报文，并请ICMP发送”端口不可达“差错报文给服务器端。入队列可能会溢出，若溢出，UDP丢弃该报文，不通知对方。 服务器端类似。 UDP首部：源端口 - 目的端口 - 长度 - 检验和，每个字段22字节。 注：IP数据报检验和只检验IP数据报的首部，而UDP的检验和将首部和数据部分一起都检验。 TCP重要 细节： TCP报文段是面向字节的数据流。 TCP首部：20字节固定首部 确认比特ACK，ACK=1 确认号字段才有效；同步比特SYN：SYN=1 ACK=0表示一个连接请求报文段；终止比特FIN，FIN=1时要求释放连接。 窗口：将TCP收发两端记为A和B，A根据TCP缓存空间的大小确定自己的接收窗口大小。并在A发送给B的窗口字段写入该值。作为B的发送窗口的上限。意味着B在未收到A的确认情况下，最多发送的字节数。 选项：最大报文段长度MSS，MSS告诉对方TCP：我的缓存所能接收的报文段的数据字段的最大长度是MSS个字节。若主机未填写，默认为536字节。 TCP的可靠是使用了序号和确认。当TCP发送一个报文时，在自己的重传队列中存放一个副本。若收到确认，删除副本。 TCP使用捎带确认。 TCP报文段的发送时机：1.维持一个变量等于MSS，发送缓存达到MSS就发送 2.发送端应用进程指明要发送，即TCP支持的PUSH操作。3.设定计时器 TCP的拥塞控制：TCP使用慢开始和拥塞避免算法进行拥塞控制 慢开始和拥塞避免 接收端根据自身资源情况控制发送端发送窗口的大小。 每个TCP连接需要维持一下2个状态变量： 接收端窗口rwnd（receiver window）：接收端根据目前接收缓存大小设置的窗口值，是来自接收端的流量控制 拥塞窗口cwnd（congestion window）：是发送端根据自己估计的网络拥塞程度设置的窗口值，是来自发送端的流量控制 发送端的窗口上限值=Min(rwnd, cwnd) 慢开始算法原理：主机刚开始发送数据时，如果立即将较大的发送窗口的全部字节注入网络，由于不清楚网络状况，可能会引起拥塞。通常的做法是将cwnd设置为1个MSS，每收到一个确认，将cwnd+1，由小到大逐步增大cwnd，使分组注入网络的速率更加合理。为了防止拥塞窗口增长引起网络拥塞，还需设置一个状态变量ssthresh，即慢开始门限。 慢开始门限：ssthresh，当cwnd &lt; ssthresh,执行慢开始算法；cwnd &gt; ssthresh，改用拥塞避免算法。 cwnd = ssthresh时，都可以。 拥塞避免算法使发送端的拥塞窗口每经过一个RTT增加一个MSS（而不管在此期间收到多少ACK），这样，拥塞窗口cwnd按线性规律增长，拥塞窗口此时比慢开始增长速率缓慢很多。这一过程称为加法增大，目的在于使拥塞窗口缓慢增长，防止网络过早拥塞。 无论是慢开始还是拥塞避免，只要发送端发现网络出现拥塞（根据是没有按时收到ACK或者收到重复ACK），就将慢开始门限ssthresh设置为拥塞窗口值的一半并将拥塞窗口cwnd置为1，重新执行慢开始算法。这一过程称为乘法减小。目的在于迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。 上述TCP确认都是通过捎带确认执行的。 快重传和快恢复 上述的慢开始和拥塞避免算法是早期TCP使用的拥塞控制算法。因为有时TCP连接会在重传时因等待重传计时器的超时时间而空闲。为此在快重传中规定：只要发送端一连收到三个重复的ACK,即可断定分组丢失，不必等待重传计数器，立即重传丢失的报文。 与快重传搭配使用的还有快恢复：当不使用快恢复时，发送端若发现网络拥塞就将拥塞窗口降为1，然后执行慢开始算法，这样的缺点是网络不能很快恢复到正常状态。快恢复是指当发送端收到3个重复的ACK时，执行乘法减小，ssthresh变为拥塞窗口值的一半。但是cwnd不是置为1，而是ssthresh+3xMSS。若收到的重复ACK为n(n &gt; 3)，则cwnd=ssthresh+n*MSS.这样做的理由是基于发送端已经收到3个重复的ACK，它表明已经有3个分组离开了网络，它们不在消耗网络的资源。 注意的是：在使用快恢复算法时，慢开始算法只在TCP连接建立时使用。 TCP的重传机制 每发送一个报文段，就对这个报文段设置一次计时器。新的重传时间=γ*旧的重传时间。 TCP连接建立和释放的过程 SYN置1和FIN的报文段要消耗一个序号。 客户端连接状态变迁：CLOSED -&gt; 主动打开,发送SYN=1 -&gt; SYN_SENT -&gt; 收到服务器的SYN=1和ACK时,发送三次握手的最后一个ACK-&gt; ESTABLISHED -&gt; 数据传送 -&gt; 主动关闭 -&gt; 发送FIN=1,等待确认ACK的到达 -&gt; FIN_WAIT_1 -&gt; 收到确认ACK后 -&gt; FIN_WAIT_2-&gt; 收到服务器发送的FIN=1报文，响应，发送四次挥手的的最后一个确认ACK -&gt; 进入TIME_WAIT状态-&gt; 经过2倍报文寿命，TCP删除连接记录 -&gt; 回到CLOSED状态 客户端状态：CLOSED - SYN_SENT- ESTABLISHED - FIN_WAIT_1 - FIN_WAIT_2 - TIME_WAIT - CLOSED 服务器端连接状态变迁：CLOSED -&gt; 被动打开 -&gt; LISTEN -&gt; 收到SYN=1的报文，发送SYN=1和确认ACK -&gt; 进入SYN_RCVD -&gt; 收到三次握手的最后一个确认ACK -&gt; ESTABLISHED -&gt; 数据传送 -&gt; 数据传送完毕，收到FIN=1 -&gt; 发送确认ACK并进入CLOSED_WAIT -&gt; 发送FIN=1给客户端 -&gt; LAST_ACK-&gt; 收到客户端四次挥手的最后一个确认ACK -&gt; 删除连接记录 -&gt; 回到CLOSED状态 服务器端：CLOSED - LISTEN - SYN_RCVD - ESTABLISHED - CLOSED_WAIT - LAST_ACK - CLOSED 应用层应用层位于TCP/IP体系结构的最高一层，也是直接为应用进程服务的一层，即当不同的应用进程数据交换时，就去调用应用层的不同协议实体，让这些实体去调用传输层的TCP或者UDP来进行网络传输。具体的应用层协议有，SMTP 25、DNS 53、HTTP 80、FTP 20数据端口 21控制端口、TFTP 69、TELNET 23、SNMP 161等 网络的划分按网络拓扑结构：总线、星型、环型、树型、网状结构和混合型。 按覆盖范围：局域网、城域网、广域网 按传播方式：广播网络和点对点网络 广播式网络是指网络中的计算机使用一个共享信道进行数据传播，网络中的所有结点都能收到某一结点发出的数据信息。 单播：一对一的发送形式。 组播：采用一对一组的发送形式，将数据发送给网络中的某一组主机。 广播：采用一对所有，将数据发送给网络所有目的结点。 点对点网络中两个结点间的通信方式是点对点的。如果两台计算机之间没有直连的线路，则需要中间结点的接收、存储、转发直至目的结点。 6. TCP的三次握手和四次挥手的过程以客户端为例 连接建立（三次握手）：首先Client端发送连接请求报文SYN并进入SYN_SENT状态，Server收到后发送ACK+SYN报文，并为这次连接分配资源。Client端接收到Server端的SYN+ACK后发送三次握手的最后一个ACK，并分配资源，连接建立。 连接释放（四次挥手）：假设Client端发起断开连接请求，首先发送FIN=1,等待确认ACK的到达 -&gt; FIN_WAIT_1 -&gt; 收到Server端的确认ACK后时 -&gt; FIN_WAIT_2-&gt;收到服务器发送的FIN=1报文，响应，发送四次挥手的的最后一个确认ACK -&gt;进入TIME_WAIT状态-&gt; 经过2倍报文寿命，TCP删除连接记录 -&gt; 回到CLOSED状态 7. 为什么连接建立是三次握手，而连接释放要四次挥手？因为当Server端收到Client端发送的SYN连接请求报文后，可以直接发送SYN+ACK报文，其中ACK用来应答，SYN用来同步。但是关闭连接时，当Server端收到FIN报文后，并不会立即关闭socket，所以先回复一个ACK，告诉Client端“你的FIN我收到了”，只有等Server端的所有报文发送完了，Server端才发送FIN报文，因此不能一起发送，故需要四次挥手。 8. 为什么TIME_WAIT状态需要2MSL（最大报文段生存时间）才能返回Closed状态？这是因为虽然双方都同意关闭连接了，而且四次挥手的报文也都协调发送完毕。但是我们必须假想网络是不可靠的，无法保证最后发送的ACK报文一定被对方收到，因此处于LAST_ACK状态下的Server端可能会因未收到ACK而重发FIN，所以TIME_WAIT状态的作用就是用来重发可能丢失的ACK报文。 9. Http报文格式Http请求报文格式：1.请求行 2.Http头 3.报文主体请求行由三部分组成，分别是请求方法，请求地址，Http版本Http头：有三种，分别为请求头（request header），普通头（General Header）和实体头（entity header）。Get方法没有实体头。报文主体：只在POST方法请求中存在。Http响应报文：1.状态行 2.Http头 3.返回内容状态行：第一部分为Http版本，第二部分为响应状态码 第三部分为状态码的描述其中第三部分为状态码的描述，信息类100-199 响应成功200-299 重定向类300-399 客户端错误400-499 服务器端错误500-599 常见的 12345678910111213100 continue 初始请求已接受，客户端应继续发送请求剩余部分200 OK202 Accepted 已接受，处理尚未完成 301 永久重定向302 临时重定向400 Bad Request401 Unauthorized403 Forbidden 资源不可用404 Not Found500 Internal Server Error 服务器错误502 Bad Gateway503 Service Unavailable 服务器负载过重504 Gateway Timeout 未能及时从远程服务器获得应答 Http头：响应头（Response Header），普通头（General Header）和实体头(Entity Header)返回内容：即Http请求的信息，可以是HTML也可以是图片等等。 10. Http和Https的区别Https即Secure Hypertext Transfer Protocol，即安全超文本传输协议，它是一个安全通信信道，基于Http开发，用于在客户机和服务器间交换信息。它使用安全套接字层SSL进行信息交换，是Http的安全版。Https协议需要到CA申请证书，一般免费证书很少，需要交费。Http是超文本传输协议，信息是明文传输，https则是具有安全性的tls/ssl加密传输协议。http是80端口，https是443端口 11. 浏览器输入一个URL的过程 浏览器向DNS服务器请求解析该URL中的域名所对应的IP地址 解析出IP地址后，根据IP地址和默认端口80和服务器建立TCP连接 浏览器发出Http请求，该请求报文作为TCP三次握手的第三个报文的数据发送给服务器 服务器做出响应，把对应的请求资源发送给浏览器 释放TCP连接 浏览器解析并显示内容 12. 中间人攻击中间人获取server发给client的公钥，自己伪造一对公私钥，然后伪造自己让client以为它是server，然后将伪造的公钥发给client，并拦截client发给server的密文，用伪造的私钥即可得到client发出去的内容，最后用真实的公钥对内容加密发给server。 解决办法：数字证书，证书链，可信任的中间人 13. 差错检测误码率：传输错误的比特与传输总比特数的比率CRC是检错方法并不能纠错，FCS（Frame Check Sequence）是冗余码。计算冗余码（余数R）的方法：先补0（n个）再对生成多项式取模。CRC只能表示以接近1的概率认为它没有差错。但不能做到可靠传输。可靠传输还需要确认和重传机制。生成多项式P(X)：CRC-16，CRC-CCITT，CRC-32 14. 数据链路层的协议停止等待协议 - 连续ARQ - 选择重传ARQ - PPP - 以太网协议- 帧中继 - ATM - HDLC 15. 截断二进制指数退避算法是以太网用于解决当发生碰撞时就停止发送然后重发再碰撞这一问题。 截断二进制指数退避算法：基本退避时间为2τ k=min{重传次数，10} r=random(0~2^k-1) 重传所需时延为r倍的基本退避时间 八、操作系统（OS基础、Linux等）1. 并发和并行“并行”是指无论从微观还是宏观，二者都是一起执行的，也就是同一时刻执行而“并发”在微观上不是同时执行的。是在同一时间间隔交替轮流执行 2. 进程间通信的方式 管道( pipe )：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。 有名管道 (named pipe) ： 有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。 信号量( semophore ) ：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 消息队列( message queue ) 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 信号 ( sinal ) ： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。 共享内存( shared memory ) 共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量配合使用，来实现进程间的同步和通信。 套接字( socket ) ：套接字也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同机器间的进程通信。 3. LinuxIO模型1、阻塞IO模型以socket为例，在进程空间调用recvfrom，其系统调用知道数据包到达且被复制到应用进程的缓冲区或者发生错误才返回，在此期间一直等待，进程从调用recvfrom开始到它返回的整段时间内都是被阻塞的，因此称为阻塞IO 2、非阻塞IO模型应用进程调用recvfrom，如果缓冲区没有数据直接返回EWOULDBLOCK错误。一般对非阻塞IO进行轮询，以确定是否有数据到来。 3、IO多路复用模型 Linux提供select/poll，通过将一个或多个fd传递给select或poll系统调用，阻塞在select上。select/poll顺序扫描fd是否就绪。 4、信号驱动IO 开启套接字接口信号驱动IO功能，并通过系统调用sigaction执行信号处理函数。当数据准备就绪时，为该进程生成SIGIO信号，通过信号回调通知应用程序调用recvfrom来读取数据，并通知主函数处理数据。 5、异步IO 告知内核启动某个操作，并让内核在整个操作完成后通知我们。它与信号驱动IO的区别在于信号驱动IO由内核通知我们何时可以开始IO操作。而异步IO模型由内核通知我们IO操作已经完成。 九、其他1. 开源软件有哪些？Eclipse、Linux及其Linux下的大多数软件、Git等。 Apache下的众多软件：Lucene、Velocity、Maven、高性能Java网络框架MINA、版本控制系统SVN、应用服务器Tomcat、Http服务器Apache、MVC框架Struts、持久层框架iBATIS、Apache SPARK、ActiveMQ 2. 开源协议 MIT：相对宽松。适用：JQuery Apache：相对宽松与MIT类似的协议，考虑有专利的情况。适用：Apache服务器、SVN GPL：GPLV2和GPLV3，如果你在乎作品的传播和别人的修改，希望别人也以相同的协议分享出来。 LGPL：主要用于一些代码库。衍生代码可以以此协议发布（言下之意你可以用其他协议），但与此协议相关的代码必需遵循此协议。 BSD：较为宽松的协议，包含两个变种BSD 2-Clause 和BSD 3-Clause，两者都与MIT协议只存在细微差异。 上面各协议只是针对软件或代码作品，如果你的作品不是代码，比如视频，音乐，图片，文章等，共享于公众之前，也最好声明一下协议以保证自己的权益不被侵犯，CC协议。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>知识点总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[23种设计模式-装饰者模式]]></title>
    <url>%2Fblog%2F2016%2F11%2F02%2F2016-11-02-23%E7%A7%8D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E8%A3%85%E9%A5%B0%E8%80%85%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[概述定义： ​ 在不必改变原类文件和原类使用的继承的情况下，动态地扩展一个对象的功能。 ​ 它是通过创建一个包装对象，也就是用装饰来包裹真实的对象来实现。角色： ​ 抽象构件角色（Project）：给出一个接口，以规范准备接收附加责任的对象。 ​ 具体构件角色（Employe）：定义一个将要接收附加责任的类。 ​ 装饰角色（Manager）：持有一个构件对象的实例，并定义一个与抽象构件接口一致的接口。 ​ 具体装饰角色（ManagerA、ManagerB）：负责给构件对象“贴上”附加的责任。 例子公共接口： 123public interface Person &#123; void eat(); &#125; 被装饰对象： 123456public class OldPerson implements Person &#123; @Override public void eat() &#123; System.out.println("吃饭"); &#125; &#125; 装饰对象： 123456789101112131415public class NewPerson implements Person &#123; private OldPerson p; NewPerson(OldPerson p) &#123; this.p = p; &#125; @Override public void eat() &#123; System.out.println("生火"); System.out.println("做饭"); p.eat(); System.out.println("刷碗"); &#125; &#125; 测试： 12345678public class PersonDemo &#123; public static void main(String[] args) &#123; OldPerson old = new OldPerson(); //old.eat(); NewPerson np = new NewPerson(old); np.eat(); &#125; &#125; ​ 通过例子可以看到，没有改变原来的OldPerson类，同时也没有定义他的子类而实现了Person的扩展，这就是装饰者模式的作用。 总结优点： ​ 1，使用装饰者模式比使用继承更加灵活，因为它选择通过一种动态的方式来扩展一个对象的功能，在运行时可以选择不同的装饰器，从而实现不同的行为。 ​ 2，通过使用不同的具体装饰类以及这些装饰类的排列组合，可以创造出很多不同行为的组合。可以使用多个具体装饰类来装饰同一对象，得到功能更为强大的对象。 ​ 3，具体构件类与具体装饰类可以独立变化，他能是低耦合的。用户可以根据需要来增加新的具体构件类和具体装饰类，在使用时再对其进行各种组合，原有代码无须改变，符合“开闭原则”。 缺点： ​ 1，会产生很多的小对象，增加了系统的复杂性 ​ 2，这种比继承更加灵活机动的特性，也同时意味着装饰模式比继承更加易于出错，排错也很困难，对于多次装饰的对象，调试时寻找错误可能需要逐级排查，较为烦琐。 装饰者与适配者模式的区别： 1，适配器模式主要用来兼容那些不能在一起工作的类，使他们转化为可以兼容目标接口，虽然也可以实现和装饰者一样的增加新职责，但目的不在此。 ​ 装饰者模式主要是给被装饰者增加新职责的。 2，适配器模式是用新接口来调用原接口，原接口对新系统是不可见或者说不可用的。 ​ 装饰者模式原封不动的使用原接口，系统对装饰的对象也通过原接口来完成使用。 3，适配器是知道被适配者的详细情况的（就是那个类或那个接口）。 ​ 装饰者只知道其接口是什么，至于其具体类型（是基类还是其他派生类）只有在运行期间才知道。 装饰者和继承的区别： 继承： 优点：代码结构清晰，而且实现简单 缺点：对于每一个的需要增强的类都要创建具体的子类来帮助其增强，这样会导致继承体系过于庞大。 装饰者： 优点：内部可以通过多态技术对多个需要增强的类进行增强 ​ 缺点：需要内部通过多态技术维护需要增强的类的实例。进而使得代码稍微复杂。 使用场景： ​ 1，需要扩展一个类的功能，或给一个类添加附加职责。 ​ 2，需要动态的给一个对象添加功能，这些功能可能不明确或者暂时的，可以随时很方便的动态撤销掉。 ​ 3，需要增加由一些基本功能的排列组合而产生的非常大量的功能，从而使继承关系变的不现实。 ​ 4. 当不能采用生成子类的方法进行扩充时。一种情况是，可能有大量独立的扩展，为支持每一种组合将产生大量的子类，使得子类数目呈爆炸性增长。另一种情况可能是因为类定义被隐藏，或类定义不能用于生成子类。]]></content>
      <categories>
        <category>设计模式</category>
        <category>装饰者模式</category>
      </categories>
      <tags>
        <tag>装饰者模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[23种设计模式-适配器模式]]></title>
    <url>%2Fblog%2F2016%2F10%2F31%2F2016-10-31-23%E7%A7%8D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E9%80%82%E9%85%8D%E5%99%A8%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[概述定义： 将一个类的接口转换成客户希望的另外一个接口。适配器模式使得原本由于接口不兼容而不能一起工作的那些类可以一起工作。角色： ​ 目标(Target)角色：这就是所期待得到的接口，也就是这类的接口是符合我们要求的。 ​ 源(Adapee)角色：我们要使用的接口，但是这个接口不符合我们的要求，也就是现在需要适配的接口。 ​ 适配器(Adaper)角色：适配器类是适配器模式的核心。适配器把源接口转换成目标接口。显然，这一角色不可以是接口，而必须是具体类。 分类 类适配器模式 12345678910class Adaptee &#123; publicvoid specificRequest() &#123; System.out.println("特殊请求，这个是源角色"); &#125;&#125;/*这个是目标角色，所期待的接口*/interface Target &#123; publicvoid request();&#125; 现在想要实现这个Target接口，但是不想重构，想要用上已有的Adaptee类，这时可以定义一个适配器类，继承想要使用的类，并且实现期待的接口。 12345class Adapter extends Adaptee implementsTarget&#123; publicvoid request() &#123; super.specificRequest(); &#125;&#125; 这样，使用适配器类和实现目标接口就完成了计划，测试： 1234567public class Test&#123; publicstatic void main(String[] args) &#123; //使用特殊功能类，即适配类 Targetadapter = new Adapter(); adapter.request(); &#125;&#125; 对象适配器模式 适配器类关联已有的Adaptee类，并且实现标准接口，这样做的好处是不再需要继承。 1234567891011class Adapter implements Target&#123; privateAdaptee adaptee; publicAdapter (Adaptee adaptee) &#123; this.adaptee= adaptee; &#125; publicvoid request() &#123; this.adaptee.specificRequest(); &#125;&#125; 我们可以想到，此时输出结果和类适配器模式是相同的，测试： 123456public class Test&#123; publicstatic void main(String[] args) &#123; Targetadapter = new Adapter(new Adaptee()); adapter.request(); &#125;&#125; 区别： 对象的适配器模式不是使用继承关系连接到Adaptee类，而是使用委派关系连接到Adaptee类。 总结适配器模式不适合在详细设计阶段使用它，它是一种补偿模式，专用来在系统后期扩展、修改时所用，适配器模式更像是一种补救措施。 优点： 复用性 系统需要使用现有的类，而此类的接口不符合系统的需要。那么通过适配器模式就可以让这些功能得到更好的复用。 扩展性 在实现适配器功能的时候，可以自由调用自己开发的功能，从而自然地扩展系统的功能。 缺点： 过多的使用适配器，会让系统非常零乱，不易整体进行把握。比如，明明看到调用的是A接口，其实内部被适配成了B接口的实现。所以适配器模式不适合在详细设计阶段使用它，它是一种补偿模式，专用来在系统后期扩展、修改时所用。 适用场景： 1、已经存在的类的接口不符合我们的需求； 2、创建一个可以复用的类，使得该类可以与其他不相关的类或不可预见的类协同工作； 3、使用一些已经存在的子类而不需要对其进行子类化来匹配接口。 4、旧的系统开发的类已经实现了一些功能，但是客户端却只能以另外接口的形式访问，但我们不希望手动更改原有类的时候。]]></content>
      <categories>
        <category>设计模式</category>
        <category>适配器模式</category>
      </categories>
      <tags>
        <tag>适配器模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[23种设计模式-模板方法模式]]></title>
    <url>%2Fblog%2F2016%2F10%2F25%2F2016-10-25-23%E7%A7%8D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E6%A8%A1%E6%9D%BF%E6%96%B9%E6%B3%95%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[概述定义：定义一个操作中算法的框架，而将一些步骤延迟到子类中，使得子类可以不改变算法的结构即可重定义该算法中的某些特定步骤。类型：行为类模式 例子某日，程序员A拿到一个任务：给定一个整数数组，把数组中的数由小到大排序，然后把排序之后的结果打印出来。经过分析之后，这个任务大体上可分为两部分，排序和打印，打印功能好实现，排序就有点麻烦了。但是A有办法，先把打印功能完成，排序功能另找人做。 12345678910111213141516abstract class AbstractSort &#123; /** * 将数组array由小到大排序 * @param array */ protected abstract void sort(int[] array); public void showSortResult(int[] array)&#123; this.sort(array); System.out.print("排序结果："); for (int i = 0; i &lt; array.length; i++)&#123; System.out.printf("%3s", array[i]); &#125; &#125; &#125; 写完后，A找到刚毕业入职不久的同事B说：有个任务，主要逻辑我已经写好了，你把剩下的逻辑实现一下吧。于是把AbstractSort类给B，让B写实现。B拿过来一看，太简单了，10分钟搞定，代码如下： 123456789101112131415161718192021222324class ConcreteSort extends AbstractSort &#123; @Override protected void sort(int[] array)&#123; for(int i=0; i&lt;array.length-1; i++)&#123; selectSort(array, i); &#125; &#125; private void selectSort(int[] array, int index) &#123; int MinValue = 32767; // 最小值变量 int indexMin = 0; // 最小值索引变量 int Temp; // 暂存变量 for (int i = index; i &lt; array.length; i++) &#123; if (array[i] &lt; MinValue)&#123; // 找到最小值 MinValue = array[i]; // 储存最小值 indexMin = i; &#125; &#125; Temp = array[index]; // 交换两数值 array[index] = array[indexMin]; array[indexMin] = Temp; &#125; &#125; 写好后交给A，A拿来一运行： 12345678public class Client &#123; public static int[] a = &#123; 10, 32, 1, 9, 5, 7, 12, 0, 4, 3 &#125;; // 预设数据数组 public static void main(String[] args)&#123; AbstractSort s = new ConcreteSort(); s.showSortResult(a); &#125; &#125; 运行结果： 排序结果： 0 1 3 4 5 7 9 10 12 32 ​ 运行正常。行了，任务完成。没错，这就是模版方法模式。大部分刚步入职场的毕业生应该都有类似B的经历。一个复杂的任务，由公司中的牛人们将主要的逻辑写好，然后把那些看上去比较简单的方法写成抽象的，交给其他的同事去开发。这种分工方式在编程人员水平层次比较明显的公司中经常用到。比如一个项目组，有架构师，高级工程师，初级工程师，则一般由架构师使用大量的接口、抽象类将整个系统的逻辑串起来，实现的编码则根据难度的不同分别交给高级工程师和初级工程师来完成。怎么样，是不是用到过模版方法模式？ 模版方法模式的结构模版方法模式由一个抽象类和一个（或一组）实现类通过继承结构组成，抽象类中的方法分为三种： 抽象方法：父类中只声明但不加以实现，而是定义好规范，然后由它的子类去实现。 模版方法：由抽象类声明并加以实现。一般来说，模版方法调用抽象方法来完成主要的逻辑功能，并且，模版方法大多会定义为final类型，指明主要的逻辑功能在子类中不能被重写。 钩子方法：由抽象类声明并加以实现。但是子类可以去扩展，子类可以通过扩展钩子方法来影响模版方法的逻辑。 抽象类的任务是搭建逻辑的框架，通常由经验丰富的人员编写，因为抽象类的好坏直接决定了程序是否稳定性。 ​ 实现类用来实现细节。抽象类中的模版方法正是通过实现类扩展的方法来完成业务逻辑。只要实现类中的扩展方法通过了单元测试，在模版方法正确的前提下，整体功能一般不会出现大的错误。 总结优点 容易扩展。一般来说，抽象类中的模版方法是不易反生改变的部分，而抽象方法是容易反生变化的部分，因此通过增加实现类一般可以很容易实现功能的扩展，符合开闭原则。 便于维护。对于模版方法模式来说，正是由于他们的主要逻辑相同，才使用了模版方法，假如不使用模版方法，任由这些相同的代码散乱的分布在不同的类中，维护起来是非常不方便的。 比较灵活。因为有钩子方法，因此，子类的实现也可以影响父类中主逻辑的运行。但是，在灵活的同时，由于子类影响到了父类，违反了里氏替换原则，也会给程序带来风险。这就对抽象类的设计有了更高的要求。 缺点 每个不同的实现都需要定义一个子类，这会导致类的个数的增加，设计更加抽象。 适用场景 在多个子类拥有相同的方法，并且这些方法逻辑相同时，可以考虑使用模版方法模式。 在程序的主框架相同，细节不同的场合下，也比较适合使用这种模式。]]></content>
      <categories>
        <category>设计模式</category>
        <category>模板方法模式</category>
      </categories>
      <tags>
        <tag>模板方法模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[23种设计模式-工厂模式]]></title>
    <url>%2Fblog%2F2016%2F10%2F20%2F2016-10-20-23%E7%A7%8D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%B7%A5%E5%8E%82%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[概述定义： ​ 工厂模式是 Java 中最常用的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。 ​ 工厂模式主要是为创建对象提供过渡接口，以便将创建对象的具体过程屏蔽隔离起来，达到提高灵活性的目的。工厂模式根据抽象程度的不同分为三种： 简单工厂模式（也叫静态工厂模式） 工厂方法模式（也叫多形性工厂） 抽象工厂模式（也叫工具箱） 简单工厂模式 ​ 实质是由一个工厂类根据传入的参数，动态决定应该创建哪一个产品类（这些产品类继承自一个父类或接口）的实例。它定义一个具体的工厂类负责创建一些类的实例。​ 优点：客户端不需要在负责对象的创建，从而明确了各个类的职责​ 缺点：这个静态工厂类负责所有对象的创建，如果有新的对象增加，或者某些对象的创建方式不同，就需要不断的修改工厂类，不利于后期的维护 工厂方法模式 ​ 工厂方法是粒度很小的设计模式，因为模式的表现只是一个抽象的方法。工厂方法模式中抽象工厂类负责定义创建对象的接口，具体对象的创建工作由继承抽象工厂的具体类实现。即在工厂和产品中间增加接口，工厂不再负责产品的创建，由接口针对不同条件返回具体的类实例，由具体类实例去实现。​ 优点：客户端不需要在负责对象的创建，从而明确了各个类的职责，如果有新的对象增加，只需要增加一个具体的类和具体的工厂类即可，不影响已有的代码，后期维护容易，增强了系统的扩展性​ 缺点：需要额外的编写代码，增加了工作量 抽象工厂模式 ​ 当有多个抽象角色时使用的一种工厂模式。抽象工厂模式可以向客户端提供一个接口，使客户端在不必指定产品的具体的情况下，创建多个产品对象。它有多个抽象产品类，每个抽象产品类可以派生出多个具体产品类，一个抽象工厂类，可以派生出多个具体工厂类，每个具体工厂类可以创建多个具体产品类的实例。 工厂方法模式举例 抽象的产品类：定义car 交通工具类 123public interface Car &#123; void gotowork();&#125; 定义实际的产品类，总共定义两个，bike 和bus 分别表示不同的交通工具类 123456public class Bike implements Car &#123; @Override public void gotowork() &#123; System.out.println("骑自行车去上班！"); &#125;&#125; 123456public class Bus implements Car &#123; @Override public void gotowork() &#123; System.out.println("坐公交车去上班！"); &#125;&#125; 定义抽象的工厂接口 123public interface ICarFactory &#123; Car getCar();&#125; 具体的工厂子类，分别为每个具体的产品类创建不同的工厂子类 123456public class BikeFactory implements ICarFactory &#123; @Override public Car getCar() &#123; return new Bike(); &#125;&#125; 123456public class BusFactory implements ICarFactory &#123; @Override public Car getCar() &#123; return new Bus(); &#125;&#125; 简单的测试类，来验证不同的工厂能够产生不同的产品对象 123456789101112131415public class TestFactory &#123; @Test public void test() &#123; ICarFactory factory = null; // bike factory = new BikeFactory(); Car bike = factory.getCar(); bike.gotowork(); // bus factory = new BusFactory(); Car bus = factory.getCar(); bus.gotowork(); &#125;&#125; 关于Java中的工厂模式的一些常见问题：利用父类的向下转型（使用父类类型的引用指向子类的对象）是可以达到类似于工厂模式的效果的，那为什么还要用工厂模式呢？ 把指向子类对象的父类引用赋给子类引用叫做向下转型，如： 123Class Student extends Person Person s = new Student(); s = (Student)person ; ​ 使用向下转型在客户端实例化子类的时候，严重依赖具体的子类的名字。当我们需要更改子类的构造方法的时候，比如增加一个参数，或者更改了子类的类名，所有的new出来的子类都需要跟着更改。 ​ 但如果我们使用工厂模式，我们仅仅需要在工厂中修改一下new的代码，其余项目中用到此实例的都会跟着改，而不需要我们手动去操作。 总结无论是简单工厂模式、工厂模式还是抽象工厂模式，它们本质上都是将不变的部分提取出来，将可变的部分留作接口，以达到最大程度上的复用。究竟用哪种设计模式更适合，这要根据具体的业务需求来决定。 工厂模式的优点： ​ 1、一个调用者想创建一个对象，只要知道其名称就可以了，降低了耦合度。 ​ 2、扩展性高，如果想增加一个产品，只要扩展一个工厂类就可以。使得代码结构更加清晰。 ​ 3、屏蔽产品的具体实现，调用者只关心产品的接口。 工厂模式的缺点： ​ 每次增加一个产品时，都需要增加一个具体类和对象实现工厂（这里可以使用反射机制来避免），使得系统中类的个数成倍增加，在一定程度上增加了系统的复杂度，同时也增加了系统具体类的依赖。所以对于简单对象来说，使用工厂模式反而增加了复杂度。 工厂模式的适用场景： ​ 1， 一个对象拥有很多子类。 ​ 2， 创建某个对象时需要进行许多额外的操作。 ​ 3， 系统后期需要经常扩展，它把对象实例化的任务交由实现类完成，扩展性好。]]></content>
      <categories>
        <category>设计模式</category>
        <category>工厂模式</category>
      </categories>
      <tags>
        <tag>工厂模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[23种设计模式-单例模式]]></title>
    <url>%2Fblog%2F2016%2F10%2F18%2F2016-10-18-23%E7%A7%8D%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F-%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[单例模式算是设计模式中最容易理解，也是最容易手写代码的模式了吧。但是其中的坑却不少，所以也常作为面试题来考。本文主要对几种单例写法的整理，并分析其优缺点。很多都是一些老生常谈的问题，但如果你不知道如何创建一个线程安全的单例，不知道什么是双检锁，那这篇文章可能会帮助到你。 概述定义： ​ 单例模式，是一种常用的软件设计模式。在它的核心结构中只包含一个被称为单例的特殊类。通过单例模式可以保证系统中一个类只有一个实例。即一个类只有一个对象实例。 特点： ​ 1、单例类只能有一个实例。 2、单例类必须自己自己创建自己的唯一实例。 3、单例类必须给所有其他对象提供这一实例 单例模式的要点： ​ 1，私有的构造方法 ​ 2，指向自己实例的私有静态引用 ​ 3，以自己实例为返回值的静态的公有的方法 单例模式根据实例化对象时机的不同分为两种： ​ 一种是饿汉式单例，一种是懒汉式单例。 ​ 饿汉式单例在单例类被加载时候，就实例化一个对象交给自己的引用；而懒汉式在调用取得实例方法的时候才会实例化对象。 懒汉式线程不安全当被问到要实现一个单例模式时，很多人的第一反应是写出如下的代码，包括教科书上也是这样教我们的。 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 这段代码简单明了，而且使用了懒加载模式，但是却存在致命的问题。当有多个线程并行调用 getInstance() 的时候，就会创建多个实例。也就是说在多线程下不能正常工作。 线程安全为了解决上面的问题，最简单的方法是将整个 getInstance() 方法设为同步（synchronized）。 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 虽然做到了线程安全，并且解决了多实例的问题，但是它并不高效。因为在任何时候只能有一个线程调用 getInstance() 方法。但是同步操作只需要在第一次调用时才被需要，即第一次创建单例实例对象时。这就引出了双重检验锁。 双重检验锁双重检验锁模式（double checked locking pattern），是一种使用同步块加锁的方法。程序员称其为双重检查锁，因为会有两次检查 instance == null，一次是在同步块外，一次是在同步块内。为什么在同步块内还要再检验一次？因为可能会有多个线程一起进入同步块外的 if，如果在同步块内不进行二次检验的话就会生成多个实例了。 12345678910public static Singleton getSingleton() &#123; if (instance == null) &#123; //Single Checked synchronized (Singleton.class) &#123; if (instance == null) &#123; //Double Checked instance = new Singleton(); &#125; &#125; &#125; return instance ;&#125; 这段代码看起来很完美，很可惜，它是有问题。主要在于instance = new Singleton()这句，这并非是一个原子操作，事实上在 JVM 中这句话大概做了下面 3 件事情。 给 instance 分配内存 调用 Singleton 的构造函数来初始化成员变量 将instance对象指向分配的内存空间（执行完这步 instance 就为非 null 了） 但是在 JVM 的即时编译器中存在指令重排序的优化。也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、2 未执行之前，被线程二抢占了，这时 instance 已经是非 null 了（但却没有初始化），所以线程二会直接返回 instance，然后使用，然后顺理成章地报错。 我们只需要将 instance 变量声明成 volatile 就可以了。 123456789101112131415public class Singleton &#123; private volatile static Singleton instance; //声明成 volatile private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; &#125; 有些人认为使用 volatile 的原因是可见性，也就是可以保证线程在本地不会存有 instance 的副本，每次都是去主内存中读取。但其实是不对的。使用 volatile 的主要原因是其另一个特性：禁止指令重排序优化。也就是说，在 volatile 变量的赋值操作后面会有一个内存屏障（生成的汇编代码上），读操作不会被重排序到内存屏障之前。比如上面的例子，取操作必须在执行完 1-2-3 之后或者 1-3-2 之后，不存在执行到 1-3 然后取到值的情况。从「先行发生原则」的角度理解的话，就是对于一个 volatile 变量的写操作都先行发生于后面对这个变量的读操作（这里的“后面”是时间上的先后顺序）。 但是特别注意在 Java 5 以前的版本使用了 volatile 的双检锁还是有问题的。其原因是 Java 5 以前的 JMM （Java 内存模型）是存在缺陷的，即时将变量声明成 volatile 也不能完全避免重排序，主要是 volatile 变量前后的代码仍然存在重排序问题。这个 volatile 屏蔽重排序的问题在 Java 5 中才得以修复，所以在这之后才可以放心使用 volatile。 相信你不会喜欢这种复杂又隐含问题的方式，当然我们有更好的实现线程安全的单例模式的办法。 静态内部类 static nested class我比较倾向于使用静态内部类的方法，这种方法也是《Effective Java》上所推荐的。 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 这种写法仍然使用JVM本身机制保证了线程安全问题；由于 SingletonHolder 是私有的，除了 getInstance() 之外没有办法访问它，因此它是懒汉式的；同时读取实例的时候不会进行同步，没有性能缺陷；也不依赖 JDK 版本。 饿汉式 static final field这种方法非常简单，因为单例的实例被声明成 static 和 final 变量了，在第一次加载类到内存中时就会初始化，所以创建实例本身是线程安全的。 123456789public class Singleton&#123; //类加载时就初始化 private static final Singleton instance = new Singleton(); private Singleton()&#123;&#125; public static Singleton getInstance()&#123; return instance; &#125;&#125; 这种写法如果完美的话，就没必要在啰嗦那么多双检锁的问题了。缺点是它不是一种懒加载模式（lazy initialization），单例会在加载类后一开始就被初始化，即使客户端没有调用 getInstance()方法。饿汉式的创建方式在一些场景中将无法使用：譬如 Singleton 实例的创建是依赖参数或者配置文件的，在 getInstance() 之前必须调用某个方法设置参数给它，那样这种单例写法就无法使用了。 枚举 Enum用枚举写单例实在太简单了！这也是它最大的优点。下面这段代码就是声明枚举实例的通常做法。 123public enum EasySingleton&#123; INSTANCE;&#125; 我们可以通过EasySingleton.INSTANCE来访问实例，这比调用getInstance()方法简单多了。创建枚举默认就是线程安全的，所以不需要担心double checked locking，而且还能防止反序列化导致重新创建新的对象。但是还是很少看到有人这样写，可能是因为不太熟悉吧。 关于Java中单例模式的一些常见问题单例模式的对象长时间不用会被jvm垃圾收集器收集吗？ ​ 除非人为地断开单例中静态引用到单例对象的联接，否则jvm垃圾收集器是不会回收单例对象的。 jvm卸载类的判定条件如下： ​ 1，该类所有的实例都已经被回收，也就是java堆中不存在该类的任何实例。 ​ 2，加载该类的ClassLoader已经被回收。 ​ 3，该类对应的java.lang.Class对象没有任何地方被引用，无法在任何地方通过反射访问该类的方法。 ​ 只有三个条件都满足，jvm才会在垃圾收集的时候卸载类。显然，单例的类不满足条件一，因此单例类也不会被回收。 在一个jvm中会出现多个单例吗？ ​ 在分布式系统、多个类加载器、以及序列化的的情况下，会产生多个单例，这一点是无庸置疑的。那么在同一个jvm中，会不会产生单例呢？使用单例提供的getInstance()方法只能得到同一个单例，除非是使用反射方式，将会得到新的单例。 代码如下： 1234Class c = Class.forName(Singleton.class.getName()); Constructor ct = c.getDeclaredConstructor(); ct.setAccessible(true); Singleton singleton = (Singleton)ct.newInstance(); 这样，每次运行都会产生新的单例对象。所以运用单例模式时，一定注意不要使用反射产生新的单例对象。 在getInstance()方法上同步有优势还是仅同步必要的块更优优势？ ​ 因为锁定仅仅在创建实例时才有意义，然后其他时候实例仅仅是只读访问的，因此只同步必要的块的性能更优，并且是更好的选择。 ​ 缺点：只有在第一次调用的时候，才会出现生成2个对象，才必须要求同步。而一旦singleton 不为null，系统依旧花费同步锁开销，有点得不偿失。 单例类可以被继承吗？ ​ 根据单例实例构造的时机和方式不同，单例模式还可以分成几种。但对于这种通过私有化构造函数，静态方法提供实例的单例类而言，是不支持继承的。 ​ 这种模式的单例实现要求每个具体的单例类自身来维护单例实例和限制多个实例的生成。但可以采用另外一种实现单例的思路：登记式单例，来使得单例对继承开放。 总结一般来说，单例模式有五种写法：懒汉、饿汉、双重检验锁、静态内部类、枚举。上述所说都是线程安全的实现，文章开头给出的第一种方法不算正确的写法。 就我个人而言，一般情况下直接使用饿汉式就好了，如果明确要求要懒加载（lazy initialization）会倾向于使用静态内部类，如果涉及到反序列化创建对象时会试着使用枚举的方式来实现单例。 单例模式的优点： ​ 1，在内存中只有一个对象，节省内存空间。 ​ 2，避免频繁的创建销毁对象，可以提高性能。 ​ 3，避免对共享资源的多重占用。 ​ 4，可以全局访问。 单例模式的缺点： ​ 1，扩展困难，由于getInstance静态函数没有办法生成子类的实例。如果要拓展，只有重写那个类。 ​ 2，隐式使用引起类结构不清晰。 ​ 3，导致程序内存泄露的问题。 适用场景： ​ 由于单例模式的以上优点，所以是编程中用的比较多的一种设计模式。以下为使用单例模式的场景： ​ 1，需要频繁实例化然后销毁的对象。 ​ 2，创建对象时耗时过多或者耗资源过多，但又经常用到的对象。 ​ 3，资源共享的情况下，避免由于资源操作时导致的性能或损耗等 ​ 4，控制资源的情况下，方便资源之间的互相通信。 单例模式注意事项： ​ 只能使用单例类提供的方法得到单例对象，不要使用反射，否则将会实例化一个新对象。 ​ 不要做断开单例类对象与类中静态引用的危险操作。 ​ 多线程使用单例使用共享资源时，注意线程安全问题。]]></content>
      <categories>
        <category>设计模式</category>
        <category>单例模式</category>
      </categories>
      <tags>
        <tag>单例模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis的五种数据类型及Jedis方法]]></title>
    <url>%2Fblog%2F2016%2F09%2F04%2F2016-09-04-Redis%E7%9A%84%E4%BA%94%E7%A7%8D%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%8F%8A%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[字符串string：字符串string是Redis中最为基础的数据存储类型，是一个由字节组成的序列，他在Redis中是二进制安全的，这便意味着该类型可以接受任何格式的数据，如JPEG图像数据货Json对象描述信息等，是标准的key-value，一般来存字符串，整数和浮点数。Value最多可以容纳的数据长度为512MB应用场景：很常见的场景用于统计网站访问数量，当前在线人数等。incr命令(++操作) 123456789101112131415set(key, value) // set设置相同的键时,后一个值会覆盖前面的值get(key) // 获取key对应的valuesetnx(key, value) // nx:表示not exists,如果key不存在就设置,如果存在就不设置setrange(key, startIndex, vlaue) // 替换字符串,表示把指定的key的值从开始索引开始是value这个值进行替换mset(key1, value1, key2, value2, ...) // 批量设置键值对msetnx(key1, value1, key2, value2, ...) // 如果key已存在那么设置失败getset(key, newValue) // 获取key的值然后设置新的值getrange(key, startIndex, endIndex) // 获取数据,最后一个值的索引为-1mget(key1, key2, key3, ...) // 批量获取incr(key) // 自增1incrby(key, num) // 指定增减的数量decr(key) // 自减1decrby(key, num) // 指定自增减的数量append(key, value) // 给指定的字符串追加value的值strlen(key) // 获取指定的key对应的值得长度 列表list：Redis的列表list允许用户从序列的两端推入或者弹出元素，列表由多个字符串值组成的有序可重复的序列，是链表结构，类似于Java中的List集合，所以向列表两端添加元素的时间复杂度为0(1)，获取越接近两端的元素速度就越快。这意味着即使是一个有几千万个元素的列表，获取头部或尾部的10条记录也是极快的。List中可以包含的最大元素数量是4294967295。应用场景：1.最新消息排行榜。2.消息队列，以完成多程序之间的消息交换。可以用push操作将任务存在list中（生产者），然后线程在用pop操作将任务取出进行执行。（消费者） 1234567891011lpush(key, value1, value2, ...) // 往list集合中压入元素linsert(key, before/after, value, newValue) // 在指定的元素前或者后插入元素lset(key, index, newValue) // 设置指定下标的值lrem(key, count, value) // 删除count个于value相同的元素,count&gt;0从开始位置进行删除,count&lt;0从末尾开始删除,count=0删除所有的ltrim(key, startIndex, endIndex) // 删除指定范围内以外的元素,保留指定范围内的元素lpop(key) // 从list的头部删除元素lindex(key, index) // 返回指定索引处的元素llen(key) // 返回列表的长度rpush(key, value) // 从末尾压入元素rpop(key) // 从末尾删除元素rpoplpush(key1, key2) // 从key1链表中弹出最后一个元素然后压入到key2链表中 散列hash：Redis中的散列hash可以看成具有String key和String value的map容器，可以将多个key-value存储到一个key中。每一个Hash可以存储4294967295个键值对。hash特别适用于存储对象, 将一个对象存在hash中相对于存储在String可以节省更多的内存。应用场景：例如存储、读取、修改用户属性（name，age，pwd等） 12345678910111213hset(key, field, value) // 给指定的key添加key-value元素hget(key, field) // 获取指定的key中filed字段的值hsetnx(key, field, value) // 如果key不存在进行插入,如果key和field都存在不进行插入hexists(key, field) // 判断指定的key中是否存在field这个字段hlen(key) // 获取指定的key中字段的数量hdel(key, field1, field2, ...) // 删除指定的key中的指定的字段的和对应的值hincrby(key, filed, count) // 给指定的key的field的字段添加或者减去count这个值hgetall(key) // 获取key中所有的键值对,返回的是一个键一个值hkeys(key) // 获取指定的key中所有的字段hmget(key, field1, field2, ...) // 获取指定的key中指定字段的值hmset(key, field1, value1, field2, value2, ...) // 同时设置多个键值对数据hvals(key) // 获取指定的key中所有的valuehincrbyfloat(key field value) // 给指定的字段添加浮点数的值 集合set：Redis的集合set是无序不可重复的，和列表一样，在执行插入和删除和判断是否存在某元素时，效率是很高的。集合最大的优势在于可以进行交集并集差集操作。Set可包含的最大元素数量是4294967295。应用场景：1.利用交集求共同好友。2.利用唯一性，可以统计访问网站的所有独立IP。3.好友推荐的时候根据tag求交集，大于某个threshold（临界值的）就可以推荐。 1234567891011121314sadd(key, member1, member2, ...) // 添加元素scard(key) // 获取成员的数量sismember(key, member) // 判断是否存在member这个成员smembers(key) // 获取所有的成员spop(key) // 随机弹出一个成员srandmember(key, [count]) // 随机获取一个或者多个成员 srem(key, member1, member2, ...) // 删除一个或者多个成员,如果成员不存在则忽略smove(source, desition, member) // 移动一个成员到指定的set中sdiff(first-key, key1, key2, ...) // 返回给定集合之间的差集。不存在的集合 key 将视为空集sdiffstore(destionset, key1, key2, ...) // 把获取到的差集保存到目标set中sinter(key1, key2, ...) // 获取交集sinterstore(destionset, key1, key2, ...) // 把获取到的交集存储到目标set中sunion(key1, key2, ...) // 获取并集sunionstore(destionset, key1, key2, ...) // 把获取到的并集存储到目标set中 有序集合sorted set：Redis的有序集合sorted set和集合set一样，都是字符串的集合，都不允许重复的成员出现在一个set中。他们之间差别在于有序集合中每一个成员都会有一个double类型的分数(score)与之关联，Redis正是通过分数来为集合中的成员进行从小到大的排序。尽管有序集合中的成员必须是唯一的，但是分数(score)却可以重复。应用场景：可以用于一个大型在线游戏的积分排行榜，每当玩家的分数发生变化时，可以执行zadd更新玩家分数(score)，此后在通过zrange获取几分top ten的用户信息。 1234567891011121314zadd(key, score1, member1, score2, member2, ...) // 用于将一个或多个成员元素及其分数值加入到有序集当中。如果某个成员已经是有序集的成员，那么更新这个成员的分数值，并通过重新插入这个成员元素，来保证该成员在正确的位置上。分数值可以是整数值或双精度浮点数。如果有序集合 key 不存在，则创建一个空的有序集并执行 ZADD 操作。当 key 存在但不是有序集类型时，返回一个错误。注意： 在 Redis 2.4 版本以前， ZADD 每次只能添加一个元素。zcard(key) // 计算元素个数zincrby(key, number, member) // 给指定的member的分数添加或者减去number这个值，当 key 不存在，或分数不是 key 的成员时，ZINCRBY key number member 等同于 ZADD key number member 。当 key 不是有序集类型时，返回一个错误。分数值可以是整数值或双精度浮点数。zcount(key, min, max) // 获取分数在min和max之间的成员和数量; 默认是闭区间; 想不包含可以: (min (maxzrange(key, start, stop, [WITHSCORES]) // 返回指定排名之间的成员(结果是分数由低到高)，排名以0开始zrevrange(key, start, stop, [WITHSCORES]) // 返回指定排名之间的成员(结果是分数由高到低)zrangebyscore(key, min, max, [withscores], [limit offset count]) // 根据分数的范围获取成员(按照分数: 从低到高)zrevrangebyscore(key, max, min, [withscores], [limit offset count]) // 根据分数的范围获取成员(从高到低)zrank(key, member) // 返回一个成员的排名(从低到高的顺序)zrevrank(key, member) // 返回一个成员的排名(从高到低)zscore(key, member) // 获取一个成员的分数zrem(key, member1, member2...) // 删除指定的成员zremrangebyrank(key, start, stop) // 根据排名进行删除zremrangebyscore(key, min, max) // 根据分数的范围进行删除 对key的通用操作1234567891011121314151617keys(pattern) // 根据指定的规则返回符合条件的keydel(key1, key2, ...) // 删除指定的keyexists(key) // 判断是否存在指定的keymove(key, db) // 将指定的key移入到指定的数据库中，redis默认存在16个库select 10:表示选择9号库rename(key, newkey) // 对key进行重命名renamenx(key, newkey) // 仅当 newkey 不存在时，将 key 改名为 newkeytype(key) // 返回key的类型expire(key, second) // 给指定的key设置失效时间expireat(key, timestamp) // 以时间戳的形式设置key的失效时间pexpireat(key, timestamp) // 以毫秒为单位设置key的失效时间persist(key) // 移除key的失效时间ttl(key) // 以秒为单位返回key的剩余时间(返回-2表示key不存在, 返回-1表示永远不过时)pttl(key) // 一毫秒为单位返回key的失效时间randomkey() // 随机返回一个keydump(key) // 序列化给定 keyflushdb() // 删除当前选择数据库中的所有keyflushall() // 删除所有数据库中的所有key Jedis API]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Jedis方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Java泛型]]></title>
    <url>%2Fblog%2F2016%2F07%2F31%2F2016-07-31-%E5%85%B3%E4%BA%8EJava%E6%B3%9B%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[泛型概念的提出（为什么需要泛型）？首先，我们看下下面这段简短的代码: 1234567891011121314public class GenericTest &#123; public static void main(String[] args) &#123; List list = new ArrayList(); list.add("dijia478"); list.add("corn"); list.add(100); for (int i = 0; i &lt; list.size(); i++) &#123; String name = (String) list.get(i); // 1 System.out.println("name:" + name); &#125; &#125;&#125; 定义了一个List类型的集合，先向其中加入了两个字符串类型的值，随后加入一个Integer类型的值。这是完全允许的，因为此时list默认的类型为Object类型。在之后的循环中，由于忘记了之前在list中也加入了Integer类型的值或其他编码原因，很容易出现类似于//1中的错误。因为编译阶段正常，而运行时会出现“java.lang.ClassCastException”异常。因此，导致此类错误编码过程中不易发现。 在如上的编码过程中，我们发现主要存在两个问题： 1.当我们将一个对象放入集合中，集合不会记住此对象的类型，当再次从集合中取出此对象时，改对象的编译类型变成了Object类型，但其运行时类型任然为其本身类型。 2.因此，//1处取出集合元素时需要人为的强制类型转化到具体的目标类型，且很容易出现“java.lang.ClassCastException”异常。 那么有没有什么办法可以使集合能够记住集合内元素各类型，且能够达到只要编译时不出现问题，运行时就不会出现“java.lang.ClassCastException”异常呢？答案就是使用泛型。 什么是泛型？泛型，即“参数化类型”。一提到参数，最熟悉的就是定义方法时有形参，然后调用此方法时传递实参。那么参数化类型怎么理解呢？顾名思义，就是将类型由原来的具体的类型参数化，类似于方法中的变量参数，此时类型也定义成参数形式（可以称之为类型形参），然后在使用/调用时传入具体的类型（类型实参）。 看着好像有点复杂，首先我们看下上面那个例子采用泛型的写法。 123456789101112131415161718192021public class GenericTest &#123; public static void main(String[] args) &#123; /* List list = new ArrayList(); list.add("dijia478"); list.add("corn"); list.add(100); */ List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add("dijia478"); list.add("corn"); //list.add(100); // 1 提示编译错误 for (int i = 0; i &lt; list.size(); i++) &#123; String name = list.get(i); // 2 System.out.println("name:" + name); &#125; &#125;&#125; 采用泛型写法后，在//1处想加入一个Integer类型的对象时会出现编译错误，通过List\，直接限定了list集合中只能含有String类型的元素，从而在//2处无须进行强制类型转换，因为此时，集合能够记住元素的类型信息，编译器已经能够确认它是String类型了。 结合上面的泛型定义，我们知道在List\中，String是类型实参，也就是说，相应的List接口中肯定含有类型形参。且get()方法的返回结果也直接是此形参类型（也就是对应的传入的类型实参）。下面就来看看List接口的的具体定义： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public interface List&lt;E&gt; extends Collection&lt;E&gt; &#123; int size(); boolean isEmpty(); boolean contains(Object o); Iterator&lt;E&gt; iterator(); Object[] toArray(); &lt;T&gt; T[] toArray(T[] a); boolean add(E e); boolean remove(Object o); boolean containsAll(Collection&lt;?&gt; c); boolean addAll(Collection&lt;? extends E&gt; c); boolean addAll(int index, Collection&lt;? extends E&gt; c); boolean removeAll(Collection&lt;?&gt; c); boolean retainAll(Collection&lt;?&gt; c); void clear(); boolean equals(Object o); int hashCode(); E get(int index); E set(int index, E element); void add(int index, E element); E remove(int index); int indexOf(Object o); int lastIndexOf(Object o); ListIterator&lt;E&gt; listIterator(); ListIterator&lt;E&gt; listIterator(int index); List&lt;E&gt; subList(int fromIndex, int toIndex);&#125; 我们可以看到，在List接口中采用泛型化定义之后，\中的E表示类型形参，可以接收具体的类型实参，并且此接口定义中，凡是出现E的地方均表示相同的接受自外部的类型实参。自然的，ArrayList作为List接口的实现类，其定义形式是： 123456789101112131415161718public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable &#123; public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; &#125; public E get(int index) &#123; rangeCheck(index); checkForComodification(); return ArrayList.this.elementData(offset + index); &#125; //...省略掉其他具体的定义过程&#125; 由此，我们从源代码角度明白了为什么//1处加入Integer类型对象编译错误，且//2处get()到的类型直接就是String类型了。 自定义泛型接口、泛型类和泛型方法从上面的内容中，大家已经明白了泛型的具体运作过程。也知道了接口、类和方法也都可以使用泛型去定义，以及相应的使用。是的，在具体使用时，可以分为泛型接口、泛型类和泛型方法。 自定义泛型接口、泛型类和泛型方法与上述Java源码中的List、ArrayList类似。如下，我们看一个最简单的泛型类和方法定义： 123456789101112131415161718192021222324252627public class GenericTest &#123; public static void main(String[] args) &#123; Box&lt;String&gt; name = new Box&lt;String&gt;("corn"); System.out.println("name:" + name.getData()); &#125;&#125;class Box&lt;T&gt; &#123; private T data; public Box() &#123; &#125; public Box(T data) &#123; this.data = data; &#125; public T getData() &#123; return data; &#125;&#125; 在泛型接口、泛型类和泛型方法的定义过程中，我们常见的如T、E、K、V等形式的参数常用于表示泛型形参，由于接收来自外部使用时候传入的类型实参。那么对于不同传入的类型实参，生成的相应对象实例的类型是不是一样的呢？ 1234567891011121314public class GenericTest &#123; public static void main(String[] args) &#123; Box&lt;String&gt; name = new Box&lt;String&gt;("corn"); Box&lt;Integer&gt; age = new Box&lt;Integer&gt;(712); System.out.println("name class:" + name.getClass()); // com.dijia478.Box System.out.println("age class:" + age.getClass()); // com.dijia478.Box System.out.println(name.getClass() == age.getClass()); // true &#125;&#125; 由此，我们发现，在使用泛型类时，虽然传入了不同的泛型实参，但并没有真正意义上生成不同的类型，传入不同泛型实参的泛型类在内存上只有一个，即还是原来的最基本的类型（本实例中为Box），当然，在逻辑上我们可以理解成多个不同的泛型类型。 究其原因，在于Java中的泛型这一概念提出的目的，导致其只是作用于代码编译阶段，在编译过程中，对于正确检验泛型结果后，会将泛型的相关信息擦出，也就是说，成功编译过后的class文件中是不包含任何泛型信息的。泛型信息不会进入到运行时阶段。 对此总结成一句话：泛型类型在逻辑上看以看成是多个不同的类型，实际上都是相同的基本类型。 类型通配符接着上面的结论，我们知道，Box\和Box\实际上都是Box类型，现在需要继续探讨一个问题，那么在逻辑上，类似于Box\和Box\是否可以看成具有父子关系的泛型类型呢？ 为了弄清这个问题，我们继续看下下面这个例子: 1234567891011121314151617181920public class GenericTest &#123; public static void main(String[] args) &#123; Box&lt;Number&gt; name = new Box&lt;Number&gt;(99); Box&lt;Integer&gt; age = new Box&lt;Integer&gt;(712); getData(name); //The method getData(Box&lt;Number&gt;) in the type GenericTest is //not applicable for the arguments (Box&lt;Integer&gt;) getData(age); // 1 &#125; public static void getData(Box&lt;Number&gt; data)&#123; System.out.println("data :" + data.getData()); &#125;&#125; 我们发现，在代码//1处出现了错误提示信息：The method getData(Box\) in the t ype GenericTest is not applicable for the arguments (Box\)。显然，通过提示信息，我们知道Box\在逻辑上不能视为Box\的父类。那么，原因何在呢？ 1234567891011121314151617181920212223242526272829303132333435363738public class GenericTest &#123; public static void main(String[] args) &#123; Box&lt;Integer&gt; a = new Box&lt;Integer&gt;(712); Box&lt;Number&gt; b = a; // 1 Box&lt;Float&gt; f = new Box&lt;Float&gt;(3.14f); b.setData(f); // 2 &#125; public static void getData(Box&lt;Number&gt; data) &#123; System.out.println("data :" + data.getData()); &#125;&#125;class Box&lt;T&gt; &#123; private T data; public Box() &#123; &#125; public Box(T data) &#123; setData(data); &#125; public T getData() &#123; return data; &#125; public void setData(T data) &#123; this.data = data; &#125;&#125; 这个例子中，显然//1和//2处肯定会出现错误提示的。在此我们可以使用反证法来进行说明。 假设Box\在逻辑上可以视为Box\的父类，那么//1和//2处将不会有错误提示了，那么问题就出来了，通过getData()方法取出数据时到底是什么类型呢？Integer? Float? 还是Number？且由于在编程过程中的顺序不可控性，导致在必要的时候必须要进行类型判断，且进行强制类型转换。显然，这与泛型的理念矛盾，因此，在逻辑上Box\不能视为Box\的父类。 好，那我们回过头来继续看“类型通配符”中的第一个例子，我们知道其具体的错误提示的深层次原因了。那么如何解决呢？总部能再定义一个新的函数吧。这和Java中的多态理念显然是违背的，因此，我们需要一个在逻辑上可以用来表示同时是Box\和Box\的父类的一个引用类型，由此，类型通配符应运而生。 类型通配符一般是使用 ? 代替具体的类型实参。注意了，此处是类型实参，而不是类型形参！且Box\&lt;?&gt;在逻辑上是Box\、Box\…等所有Box\&lt;具体类型实参&gt;的父类。由此，我们依然可以定义泛型方法，来完成此类需求。 123456789101112131415161718public class GenericTest &#123; public static void main(String[] args) &#123; Box&lt;String&gt; name = new Box&lt;String&gt;("corn"); Box&lt;Integer&gt; age = new Box&lt;Integer&gt;(712); Box&lt;Number&gt; number = new Box&lt;Number&gt;(314); getData(name); getData(age); getData(number); &#125; public static void getData(Box&lt;?&gt; data) &#123; System.out.println("data :" + data.getData()); &#125;&#125; 有时候，我们还可能听到类型通配符上限和类型通配符下限。具体有是怎么样的呢？ 在上面的例子中，如果需要定义一个功能类似于getData()的方法，但对类型实参又有进一步的限制：只能是Number类及其子类。此时，需要用到类型通配符上限。 1234567891011121314151617181920212223242526public class GenericTest &#123; public static void main(String[] args) &#123; Box&lt;String&gt; name = new Box&lt;String&gt;("corn"); Box&lt;Integer&gt; age = new Box&lt;Integer&gt;(712); Box&lt;Number&gt; number = new Box&lt;Number&gt;(314); getData(name); getData(age); getData(number); //getUpperNumberData(name); // 1 getUpperNumberData(age); // 2 getUpperNumberData(number); // 3 &#125; public static void getData(Box&lt;?&gt; data) &#123; System.out.println("data :" + data.getData()); &#125; public static void getUpperNumberData(Box&lt;? extends Number&gt; data)&#123; System.out.println("data :" + data.getData()); &#125;&#125; 此时，显然，在代码//1处调用将出现错误提示，而//2 //3处调用正常。 类型通配符上限通过形如Box\&lt;? extends Number&gt;形式定义，相对应的，类型通配符下限为Box\&lt;? super Number&gt;形式，其含义与类型通配符上限正好相反，在此不作过多阐述了。 话外篇本文中的例子主要是为了阐述泛型中的一些思想而简单举出的，并不一定有着实际的可用性。另外，一提到泛型，相信大家用到最多的就是在集合中，其实，在实际的编程过程中，自己可以使用泛型去简化开发，且能很好的保证代码质量。并且还要注意的一点是，Java中没有所谓的泛型数组一说。 对于泛型，最主要的还是需要理解其背后的思想和目的。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>泛型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2Fblog%2F2016%2F05%2F13%2F2016-05-13-Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[一、系统目录结构 约定俗成： bin (binaries)存放二进制可执行文件 etc (etcetera)存放系统配置文件 usr (unix shared resources)用于存放共享的系统资源 home 存放用户文件的根目录 root 超级用户目录 sbin (super user binaries)存放二进制可执行文件，只有root才能访问 dev (devices)用于存放设备文件 lib (library)存放跟文件系统中的程序运行所需要的共享库及内核模块 mnt (mount)系统管理员安装临时文件系统的安装点 boot 存放用于系统引导时使用的各种文件 tmp (temporary)用于存放各种临时文件 var (variable)用于存放运行时需要改变数据的文件 二、基本命令加粗命令为本人比较常用的命令（会经常更新） 1.目录操作相对路径和绝对路径说明： 现在在/home/dijia478/下./a/b.txt和a/b.txt都表示相对路径，当前目录下的a文件夹下的b.txt文件/home/dijia478/a/b.txt表示绝对路径，根目录下的home文件夹下的dijia478文件夹下的a文件夹下的b.txt文件 cd ./a 切换到当前目录的a文件夹cd .. 切换到上一层目录cd / 切换到系统根目录cd ~ 切换到用户主目录cd - 切换到上一个所在目录pwd 显示当前所在目录的绝对路径 2.查看文件列表ls /path/ 显示该目录所有文件或文件夹名ls -a 显示所有文件或文件夹名（包含隐藏的）ls -l 按列表显示所有文件或文件夹，缩写成ll ll -h 友好的显示文件大小（显示成K，MB，GB） 3.创建和删除文件夹mkdir app 创建app文件夹mkdir –p app2/test 级联创建aap2以及test文件夹 rmdir app 删除app目文件夹（需要是空文件夹） 4.文件操作rm a.txt 删除a.txt文件，删除需要用户确认，y/nrm -f a.txt 不询问，直接删除a.txt文件rm -r a 递归删除a文件夹（无论是否有内容）rm -rf a 不询问递归删除a文件夹（慎用）rm -rf * 删除当前目录下所有内容（最好别用）rm -rf /\ no 作 no die（Linux系统就玩完了）* cp a.txt b.txt 将a.txt复制为b.txt文件cp -r dir/ ../ 将dir文件夹及子目录和文件复制到上一层目录中 mv a.txt ../ 将a.txt文件移动到上一层目录中mv a.txt b.txt 将a.txt文件重命名为b.txt touch a.txt 创建一个空的a.txt文件echo “good good study” &gt; a.txt 把”&gt;”左边的输出内容放到右边的文件里去，如果存在就覆盖，如果不存在就创建vim a.txt 用文本编辑器编辑一个文件，如果不存在就创建 5.文件打包归档和压缩tar -cvf file.tar dirpath filepath 将dir文件夹和file文件在当前目录下打包成file.tartar –xvf file.tar 解包到当前目录 gzip file.tar 压缩文件或文件夹gzip –d file.tar.gz 解压文件或文件夹 tar -czvf file.tar.gz dirpath filepath 将dir文件夹和file文件在当前目录下打包并压缩成file.tar.gztar -xzvf file.tar.gz 解压并解包到当前目录下tar -xzvf file.tar.gz -C /home/dijia478/ 解压并解包到/home/dijia478/目录下 常用参数：-c：创建一个新tar文件-v：显示运行过程的信息-f：指定文件名-z：调用gzip压缩命令进行压缩-t：查看压缩文件的内容-x：解开tar文件 zip test.txt.zip test.txt 也是打包并压缩unzip test.txt.zip 解包并解包 .rpm结尾的包，用rpm -ivh 命令安装 6.查看文本文件cat a.txt 一次性显示整个文件内容more a.txt 可以分页看（翻页：空格,往回翻：b ，退出： q或者 Ctrl+C）less a.txt 不仅可以分页，还可以方便地搜索，回翻等操作（翻页：空格,往回翻：↑,往下翻:↓,退出：q或者 Ctrl+C） tail -10 a.txt 查看文件的尾部的10行tail -f user.log 实时刷新显示文件的尾部，这条命令对于观察调试程序的运行非常重要 head -20 a.txt 查看文件的头部20行注：ctrl+c 结束查看 7.搜索查找命令grep ‘haha’ ./* 打印当前目录下所有文件中含有’haha’的地方（支持正则表达式）grep -c ‘haha’ ./* 显示匹配到的行数grep -r ‘haha’ ./* 对子目录也进行遍历搜索grep -l ‘haha’ ./* 只显示命中的文件名grep -n ‘haha’ ./* 显示命中的行号grep -ld skip ‘haha’ ./* 显示命中的文件名，不要搜索子目录 参数含义：-r 递归搜索子目录-l 只列出有匹配行的文件名-n 列出匹配行的行号-d skip 不搜索子文件夹 常用grep跟其他命令组合使用来查找我们关心的信息（管道）示例：service –status-all | grep ‘httpd’ 在当前系统所有服务中查找’httpd’netstat -nltp | grep ‘22’ 查找监听’22’端口的服务程序ps –ef | grep java 查找系统中当前运行的java进程 find ./ -name ‘*.txt’ 查找以.txt结尾的文件（会遍历当前目录）find ./ -name ‘install*’ 查找以install开头的文件或文件夹find ./ -type f 查找普通文件find ./ -type l 查找连接文件(快捷方式) 8.文本命令> 重定向输出，覆盖原有内容；>&gt; 重定向输出，又追加功能；cat /etc/passwd &gt; a.txt 将密码文件输出定向到a.txt中cat /etc/passwd &gt;&gt; a.txt 输出并且追加ifconfig &gt; ifconfig.txt 保存ip信息到文件中 wc -l a.txt 统计文本行数wc -w a.txt统计文本单词数wc -m a.txt统计文本字符数wc -c a.txt统计文本字节数 vim编辑器，这里全都很常用vim filepath打开文件按Esc键切换到命令行模式切换到插入模式：i在当前位置生前插入I在当前行首插入a在当前位置后插入A在当前行尾插入o在当前行之后插入一行O在当前行之前插入一行dd删除整行7 dd向上删除7行U回退(类似于windows 中 ctrl + z)R替换:（冒号）切换到底行模式:q退出:wq保存并退出（shift + zz也可以保存）:q!不保存退出 9.其他常用命令echo $JAVA_HOME 输出变量JAVA_HOME的值 rpm -qa | grep tomcat 查看当前系统是否安装tomcatrpm -e tomcat 卸载tomcatrpm 的其他附加命令–force 强制操作 如强制安装删除等；–requires 显示该包的依赖关系；–nodeps 忽略依赖关系并继续操作； whoami查询当前登陆的用户名which ls查询ls命令的$PATH路径 mkdir test &amp;&amp; cd test只有在 &amp;&amp; 左边的命令返回真（命令返回值 $? == 0），&amp;&amp; 右边的命令才会被执行。只要有一个命令返回假（命令返回值 $? == 1），后面的命令就不会被执行。 三、用户管理命令1.添加用户基本示例：useradd user001 passwd 123456需要设置密码 参数手册：-u 指定组ID（uid）-g 指定所属的组名（gid）-G 指定多个组，用逗号“，”分开（Groups）-c 用户描述（comment）-d 指定用户目录-e 失效时间（expire date） 2.删除用户userdel user002这样删除的时候，用户的主目录会被保留userdel -r user002删除用户的同时删除用户的主目录 3.修改用户属性指令：usermod参数：-l 修改用户名 （login）usermod -l a b（b改为a）-g 修改组 usermod -g sys tom-d 修改用户的宿主目录-G 添加多个组 usermod -G sys,root tom-L 锁定用户账号密码（Lock）-U 解锁用户账号（Unlock）示例：usermod -l user002 user001将user001的登陆名改为user002usermod -g root user002将user002的组改为root组usermod -G hello1,hello2 user002给user002添加两个组hello1,hello2usermod -d /home/dijia478 user002将user002的主目录改成/home/dijia478（要事先创建dijia478目录，并且拷入环境变量文件） 4.用户组管理用户组相关属性：每个用户至少属于一个用户组（创建新用户时如果不指定所属组，则会自动创建并归属到一个跟用户名同名的组）每个用户组可以包含多个用户同一个用户组的用户享有该组共有的权限用户组管理操作命令：groupadd java创建用户组groupdel hello2删除用户组groupmod –n newname oldname修改用户组名称groups user002查看用所属的组 5.用户及用户组相关配置文件用户配置信息存放位置：保存用户信息的文件：/etc/passwd保存密码的文件：/etc/shadow passwd文件示例：user002:x:500:500:user002:/home/user002:/bin/bashpasswd文件各字段含义：account:password:UID:GID:GECOS:directory:shell shadow文件示例：user002:$1$vRug41$UUxYzdP0i6s6wtUPieGDQ/:18617:0:99999:7:::shadow文件各字段含义： 用户名 登陆系统的用户名 密码 加密密码 最后一次修改时间 用户最后一次修改密码距现在的天数，从1970-1-1起 最小时间间隔 两次修改密码之间的最小天数 最大时间间隔 密码有效天数 警告时间 从系统警告到密码失效的天数 账号闲置时间 账号闲置时间 失效时间 密码失效的天数 标志 标志 用户组配置信息存放位置：保存用户组的文件：/etc/group保存用户组密码的文件：/etc/gshadow（设置组管理员时才有用） 6.其他的用户管理命令id user002查看一个用户的UID和GIDsu user001切换到user001用户su - user001切换到user001用户，并且将环境也切换到user001用户的环境（推荐使用）exis退出当前shell（会退出当前登录用户） 四、网络配置管理1.网卡地址配置检查网络连通性:ping 192.168.25.168 测试当前服务器和指定ip是否能进行网络通信 查看ip地址ifconfig 查看所有网络设备的地址信息ifconfig eth0 查看指定的eth0以太网卡的地址信息 修改ip地址ifconfig eth0 192.168.2.150 netmask 255.255.255.0 通过setup修改网络配置在root权限下执行setup指令可以打开一个带菜单的伪图形界面来修改网络配置 通过配置文件修改ip地址vi /etc/sysconfig/network-scripts/ifcfg-eth0 修改该配置文件即可改ip地址 2.主机名配置管理查看主机名在shell提示符的@后有显示或者用hostname指令打印出主机名 修改主机名vi /etc/sysconfig/network 修改其中的hostname配置项:HOSTNAME=newname修改完成之后要重启服务器才能生效要想立即生效，可以执行指令 hostname newname，然后注销重登陆 主机名-IP映射，服务器本地映射服务器网络寻址时默认是现在本地的hosts文件中查找IP映射，通过修改hosts来映射局域网内部的主机名非常方便实现方法，将局域网内的每一台主机的“hostnamip”写入每一台主机的hosts文件中：vi /etc/hosts192.168.2.150 user001-server-01192.168.2.151 user001-server-02192.168.2.152 user001-server-03 3.网络服务启动与停止列出系统所有应用服务状态：service –status-all查看指定服务运行状态：service servicename status启动服务：service servicename start停止服务：service servicename stop 列出所有服务的随机自起配置：chkconfig –list关闭服务的随机自起：chkconfig servicename off开启服务的随机自起：chkconfig servicename on 常用示例：重启网络服务：service network restart停止httpd：service httpd stop启动httpd：service httpd start关闭防火墙服务：service iptables stop关闭防火墙自动启动：chkconfig iptables off 4.查看网络连接信息指令：netstatnetstat常用示例：netstat -natpnetstat -nltpnetstat -naup netstat -an | grep 3306 查询3306端口占用情况 常用参数解释：-a 显示所有连接和监听端口-l 只显示监听进程-t (tcp)仅显示tcp相关选项-u (udp)仅显示udp相关选项-n 拒绝显示别名，能显示数字的全部转化成数字。-p 显示建立相关链接的程序名 lsof命令其实要比netstat强大常用参数：-i 显示所有连接-i 6 仅获取IPv6流量-iTCP 仅显示TCP连接-i:8080 显示与8080端口相关的网络信息-i@192.168.128.128 显示指定到192.168.128.128的连接-i@192.168.128.128:22 显示基于主机与端口的连接-i -sTCP:LISTEN 找出正等候连接的端口-i -sTCP:ESTABLISHED 找出已建立的连接-u 用户名 显示指定用户打开了什么-u ^用户名 显示除指定用户以外的其它所有用户所做的事情-p 10075 查看指定进程ID已打开的内容其他用法：kill -9 ‘lsof -t -u 用户名’ 杀死指定用户所做的一切事情lsof /var/log/messages/ 显示与指定目录或文件交互的所有一切lsof -u dijia478 -i @1.1.1.1 显示dijia478连接到1.1.1.1所做的一切 五、常用系统管理命令1.磁盘/内存使用信息查看df -h 查看磁盘空间状态信息du -sh * 查看指定目录下所有子目录和文件的汇总大小free 查看内存使用状况 2.进程管理top 查看实时刷新的系统进程信息ps -ef 查看系统中当前瞬间的进程信息快照ps -ef | grep myshell 搜索myshell进程的信息kill -9 pid 杀掉指定pid的进程（-9 表示强制杀死） 3.sudo权限的配置root用户因为具有不受限制的权限，使用不慎可能对系统造成不可估量的损害，因而，生产实际中，轻易不要使用su去切换到root的身份如果普通用户需要使用一些系统级管理命令，可以使用sudo来执行，比如 sudo vim /etc/profile给普通用户赋予sudo权限，配置方法如下：例如，要给hadoop用户赋予sudo任何指令（或某条指定的命令）的权利，则编辑sudoers文件 vim /etc/sudoers在其中加入需要赋予权限的用户&gt; root ALL=(ALL) ALL#让hadoop用户可以用root身份执行所有指令hadoop ALL=(ALL) ALL#让user002用户可以用root身份执行useradd,passwd命令user002 ALL=(root) /usr/sbin/useradd, /usr/bin/passwd 检查是否生效:[root@user001-server-01 user002]# sudo -lU user002User user002 is not allowed to run sudo on user001-server-01. 4.修改系统的默认启动级别vim /etc/inittab &gt; # 0 - halt (Do NOT set initdefault to this)# 1 - Single user mode# 2 - Multiuser, without NFS (The same as 3, if you do not have networking)# 3 - Full multiuser mode# 4 - unused# 5 - X11# 6 - reboot (Do NOT set initdefault to this)#id:3:initdefault:~ 用level 3 就启动全功能状态的命令行界面，5是图形界面。不要设置其他的，容易作死。在命令行模式下，用startx可以手动启动图形界面(在服务器上操作) 5.文件权限管理 Linux三种文件类型：普通文件： 包括文本文件、数据文件、可执行的二进制程序文件等。目录文件： Linux系统把目录看成是一种特殊的文件，利用它构成文件系统的树型结构。设备文件： Linux系统把每一个设备都看成是一个文件文件类型标识：普通文件（-）目录（d）符号链接（l）* 进入etc可以查看，相当于快捷方式字符设备文件（c）块设备文件（s）套接字（s）命名管道（p） u 表示“用户（user）”，即文件或目录的所有者。g 表示“同组（group）用户”，即与文件属主有相同组ID的所有用户。o 表示“其他（others）用户”。a 表示“所有（all）用户”。它是系统默认值。操作符号可以是：+ 添加某个权限。- 取消某个权限。= 赋予给定权限并取消其他所有权限（如果有的话）。设置mode所表示的权限可用下述字母的任意组合：r 可读。w 可写。x 可执行。 文件权限管理：chmod u+rwx a.txt 为a.txt添加所属用户的rwx权限chmod 755 a.txt 为a.txt设置所属用户rwx权限，所属组rx权限，其他用户rx权限（r-4，w-2，x-1）chmod u=rwx,g=rx,o=rx a.txt （u代表所属用户 g代表所属组的成员用户 o代表其他用户）chown user001:hello1 a.txt 将a.txt的所有者改成user001用户，所属组改成hello1组（需要root权限）chown -R user001:hello1 dir 将dir文件夹的所有者改成user001用户，所属组改成hello1组（需要root权限） 6.其他系统管理命令date “+%Y%m%d”按格式显示当前系统时间date -s “2020-01-01 10:10:10”设置系统时间clear清屏幕(只是滚到上面看不到了)uname 显示系统信息uname -a 显示本机详细信息。依次为：内核名称(类别)，主机名，内核版本号，内核版本，内核编译日期，硬件名，处理器类型，硬件平台类型，操作系统名称]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA常用快捷键]]></title>
    <url>%2Fblog%2F2016%2F04%2F09%2F2016-04-09-IDEA%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[一、按键位排序 Ctrl 键位 说明 Ctrl + / 注释 Ctrl + B 进入光标所在方法或变量的定义处（等效Ctrl + 左键单击） Ctrl + D 复制光标选中内容（可以更改设置为复制光标选中所在行） Ctrl + E 显示最近打开的文件 Ctrl + F 在当前文件进行文本查找 Ctrl + H 在右侧显示当前类或接口的继承或实现结构 Ctrl + I 选择可实现的方法 Ctrl + J 插入自定义的动态代码模板 Ctrl + N 打开指定类 Ctrl + O 选择可重写的方法 Ctrl + P 方法参数类型提示（参考Ctrl + Shift + P） Ctrl + Q 快速查看文档内容 Ctrl + R 在当前文件进行文本替换 Ctrl + S 全部保存 Ctrl + U 前往当前光标所在的方法的父类的方法 / 接口定义 Ctrl + W 递进式选择代码块 Ctrl + Y 删除当前行 Ctrl + F1 在光标所在的错误代码处显示错误信息 Ctrl + F3 调转到所选中的词的下一个引用位置 Ctrl + F4 关闭当前编辑文件 Ctrl + F7 搜索当前文件中使用的地方（参考Alt + F7） Ctrl + F8 在 Debug 模式下，设置光标当前行为断点，如果当前已经是断点则去掉断点 Ctrl + F12 弹出当前文件结构层（所有方法和成员），可以在弹出的层上直接输入，进行筛选 Ctrl + 空格 自动完成名称，建议将空格修改为逗号（参考Ctrl + Shift + Space，Ctrl + Alt + Space） Alt 键位 说明 Alt + Q 显示当前方法的定义声明 Alt + 1 快速打开或隐藏工程面板 Alt + F1 在各个面板定位当前元素 Alt + F3 选中或输入文本,按F3逐个向下查找相同内容 Alt + F7 在下方显示全项目中使用的地方，会显示结构（参考Ctrl + F7，类似Ctrl + Alt + F7） Alt + Enter 快速修复错误 Alt + Insert 生成构造方法，getter/setter，hashcode等代码，在左侧目录用上可以生成文件 Alt + 左/右 切换试图 Alt + 上/下 切换方法 Shift 键位 说明 Shift + F2 快速定位到下一个警告位置（等效F2） Shift + F3 在查找模式下，定位到上一个匹配处（参考F3） Shift + F6 重命名 Shift + F7 在 Debug 模式下，智能步入。断点所在行上有多个方法调用，会弹出进入哪个方法 Shift + F8 在 Debug 模式下，跳出当前方法 Shift + F9 等效于点击工具栏的 Debug 按钮 Shift + F10 等效于点击工具栏的 Run 按钮 Shift + ESC 隐藏最后一个打开的工具窗口 Shift + Tab 取消缩进 Shift + Enter 向下插入新行（参考Ctrl + Alt + Enter） Ctrl + Alt 键位 说明 Ctrl + Alt + C 提取作为常量 Ctrl + Alt + F 提取作为成员变量 Ctrl + Alt + H 调用层次（参考Ctrl + Shift + H） Ctrl + Alt + L 格式化代码 Ctrl + Alt + M 抽取选中代码变成方法 Ctrl + Alt + O 优化导入，就是将无效的import去除 Ctrl + Alt + P 提取作为所在方法的参数 Ctrl + Alt + S IDEA设置 Ctrl + Alt + T 对选中的代码进行环绕包裹，if,try等 Ctrl + Alt + V 快速引入变量，提取作为局部变量 Ctrl + Alt + F7 弹出选中元素在全项目中使用的地方，不显示结构（类似Alt + F7） Ctrl + Alt + F12 弹出当前文件的各级本地磁盘目录，可以快速打开 Ctrl + Alt + Enter 向上插入新行（参考Shift + Enter） Ctrl + Alt + Home 弹出跟当前文件有关联的文件弹出层 Ctrl + Alt + 空格 类名或接口名提示（包括jar包里的，还会自动导包，参考Ctrl + 空格，Ctrl + Shift + 空格） Ctrl + Alt + 左/右 退回到上一个浏览的位置（和Ctrl + Shift + Backspace不同） Ctrl + Shift 键位 说明 Ctrl + Shift + / 代码块注释 Ctrl + Shift + 加号 展开所有代码 Ctrl + Shift + 减号 折叠所有代码 Ctrl + Shift + B 跳转到当前元素类型的声明处 Ctrl + Shift + C 复制当前文件磁盘路径到剪贴板 Ctrl + Shift + F 在指定范围查找内容（搜全项目并不保险，有时候会搜不到，参考Ctrl + F） Ctrl + Shift + H 显示方法层次结构（参考Ctrl + Alt + H） Ctrl + Shift + I 快速查看当前元素的声明定义 Ctrl + Shift + N 打开指定文件 Ctrl + Shift + P 方法返回值类型提示（参考Ctrl + P） Ctrl + Shift + R 在指定范围查找内容（参考Ctrl + R） Ctrl + Shift + U 切换大小写 Ctrl + Shift + V 选择粘贴板缓存内容并粘贴 Ctrl + Shift + W 递进式取消选择代码块 Ctrl + Shift + Z 取消撤销 Ctrl + Shift + F7 选中文本，高亮显示所有相同内容（类似Alt + F3） Ctrl + Shift + 空格 智能代码提示（所期望类型的变量和方法列表供选择，参考Ctrl + 空格，Ctrl + Alt + 空格） Ctrl + Shift + Enter 自动结束代码（会在代码后面加上分号完成当前语句） Ctrl + Shift + Backspace 退回到上次修改的地方（和Ctrl + Alt + 左/右不同） Ctrl + Shift + 上/下 移动当前代码（以代码块为单位） Alt + Shift 键位 说明 Alt + Shift + 上/下 移动当前代码（以行为单位） Ctrl + Shift + Alt 键位 说明 Ctrl + Shift + Alt + V 无格式黏贴 Ctrl + Shift + Alt + N 全局查找变量或方法 Ctrl + Shift + Alt + S 项目结构设置 其它 键位 说明 F2 快速定位到下一个警告位置（等效Shift + F2） F3 在查找模式下，定位到下一个匹配处（参考Shift + F3） F4 编辑源（类似Ctrl + B） F7 在 Debug 模式下，进入下一步，如果当前行断点是一个方法，则进入当前方法体内，如果该方法体还有方法，则不会进入该内嵌的方法中（步入） F8 在 Debug 模式下，进入下一步，如果当前行断点是一个方法，则不进入当前方法体内（跳过） F9 在 Debug 模式下，恢复程序运行，但是如果该断点下面代码还有断点则停在下一个断点上 F11 添加书签 F12 回到前一个工具窗口 Tab 缩进 ESC 从工具窗口进入代码文件窗口 连按两次Shift 查找任何地方（搜全项目并不保险，有时候会搜不到） 二、按功能排序 三、设置Eclipse键位模式既然要学习IDEA，那么本人建议最好还是不要设置成Eclipse键位模式，因为这样你还是不熟悉IDEA。当然如果你是初学，但又急需要投入生产环境，那么可以先用Eclipse键位模式进行开发，大部分的快捷键和Eclipse是一样的。具体设置方法： 先按Ctrl + Alt + S，打开IDEA设置，在如图位置处设置快捷键风格]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM内存区域划分]]></title>
    <url>%2Fblog%2F2016%2F03%2F01%2F2016-03-01-JVM%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E5%88%92%E5%88%86%2F</url>
    <content type="text"><![CDATA[JVM内存由于Java程序是交由JVM执行的，所以我们在谈Java内存区域划分的时候事实上是指JVM内存区域划分。在讨论JVM内存区域划分之前，先来看一下Java程序具体执行的过程： 如上图所示，首先Java源代码文件(.java后缀)会被Java编译器编译为字节码文件(.class后缀)，然后由JVM中的类加载器加载各个类的字节码文件，加载完毕之后，交由JVM执行引擎执行。在整个程序执行过程中，JVM会用一段空间来存储程序执行期间需要用到的数据和相关信息，这段空间一般被称作为Runtime Data Area（运行时数据区），也就是我们常说的JVM内存。因此，在Java中我们常常说到的内存管理就是针对这段空间进行管理（如何分配和回收内存空间） Java虚拟机在执行Java程序时会将其管理的内存按照用于划分为若干个不同的数据区域，这些区域有着各自不同的生命周期。根据《JAVA虚拟机规范》，Java虚拟机管理的内存会包含以下几个区域。其中可以分为共享内存区以及线程隔离数据区两个部分。 在JVM规范中虽然规定了程序在执行期间运行时数据区应该包括这几部分，但是至于具体如何实现并没有做出规定，不同的虚拟机厂商可以有不同的实现方式。 JVM内存划分1.程序计数器也有称作为PC寄存器，用来指示执行哪条指令 当前线程所执行的字节码行号指示器 字节码解释器工作依赖计数器控制完成 通过执行线程行号记录，让线程轮流切换各条线程之间计数器互不影响 线程私有，生命周期与线程相同，随JVM启动而生，JVM关闭而死 线程执行Java方法时，记录其正在执行的虚拟机字节码指令地址 线程执行Nativan方法时，计数器记录为空（Undefined） 唯一在Java虚拟机规范中没有规定任何OutOfMemoryError情况区域 由于程序计数器中存储的数据所占空间的大小不会随程序的执行而发生改变，因此，对于程序计数器是不会发生内存溢出现象(OutOfMemory) 有两个线程，其中一个线程可以暂停使用，让其他线程运行，然后等自己获得cpu资源时，又能从暂停的地方开始运行，那么为什么能够记住暂停的位置的，这就依靠了程序计数器， 通过这个例子，大概了解一下程序计数器的功能。 2.虚拟机栈虚拟机栈（Java Vitual Machine Stack）也称作Java栈，也就是我们常常所说的栈。线程私有，它的生命周期和线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧，用来存放存储： 局部变量表(Local Variables) 操作数栈(Operand Stack) 指向当前方法所属的类的运行时常量池的引用(Reference to runtime constant pool) 方法返回地址(Return Address) 一些额外的附加信息 当线程执行一个方法时，就会随之创建一个对应的栈帧，并将建立的栈帧压栈。当方法执行完毕之后，便会将栈帧出栈。 因此可知，线程当前执行的方法所对应的栈帧必定位于Java栈的顶部。对于所有的程序设计语言来说，栈这部分空间对程序员来说是不透明的。下图表示了一个Java栈的模型： 分别来作说明： 局部变量表，用来存储方法中的局部变量（包括在方法中声明的非静态变量以及函数形参）。对于基本数据类型的变量，则直接存储它的值，对于引用类型的变量，则存的是指向对象的引用。需要注意的是，局部变量表所需的内存空间在编译期内完成分配，当进入一个方法时，这个方法在栈帧中分配多大的内存空间是完全确定的，在程序执行期间局部变量表的大小是不会改变的。 操作数栈，栈最典型的一个应用就是用来对表达式求值。想想一个线程执行方法的过程中，实际上就是不断执行语句的过程，而归根到底就是进行计算的过程。因此可以这么说，程序中的所有计算过程都是在借助于操作数栈来完成的。 指向运行时常量池的引用（运行时常量池的概念在方法区部分会谈到），因为在方法执行的过程中有可能需要用到类中的常量，所以必须要有一个引用指向运行时常量。 方法返回地址，当一个方法执行完毕之后，要返回之前调用它的地方，因此在栈帧中必须保存一个方法返回地址。 由于每个线程正在执行的方法可能不同，因此每个线程都会有一个自己的Java栈，互不干扰。 -Xss参数设置栈容量 例：-Xss1m 虚拟机栈和本地方法栈的大小 = 线程允许最大内存 - 最大堆容量 - 最大方法区容量在多线程时，给每个线程分配的栈越大，越容易出现异常 注意这个区域可能出现的两种异常： StackOverflowError，当前线程请求的栈深度大于虚拟机所允许的深度时，会抛出这个异常。制造这种异常很简单：将一个函数反复递归自己，最终会出现栈溢出错误（StackOverflowError）。 OutOfMemoryError ，当虚拟机栈可以动态扩展时（当前大部分虚拟机都可以），如果无法申请足够多的内存就会抛出 内存溢出错误（OutOfMemoryError）。 3.本地方法栈本地方法栈与Java栈的作用和原理非常相似。区别只不过是Java栈是为执行Java方法服务的，而本地方法栈则是为执行本地方法（Native Method）服务的。与虚拟机栈一样，本地方法栈也会抛出 StackOverflowError 和 OutOfMemoryError 异常。在JVM规范中，并没有对本地方发展的具体实现方法以及数据结构作强制规定，虚拟机可以自由实现它。在HotSopt虚拟机中直接就把本地方法栈和虚拟机栈合二为一。 以上说的JVM三个区域都是线程不共享的，也就是这部分内存，每个线程独有，不会让别的线程访问到，接下来的两个就是线程共享了，也就会出现线程安全问题。 4.堆堆可以说是虚拟机中最大一块内存了。它是所有线程所共享的内存区域，在JVM中只有一个堆。Java中的堆是用来存储对象本身的以及数组（当然，数组引用是存放在Java栈中的）。当然，随着 JIT 编译器的发展，所有对象在堆上分配渐渐变得不那么 “绝对” 了。 同时堆也是垃圾收集器管理的主要区域。因此很多时候被称为”GC堆”。由于现在的收集器基本上采用的都是分代收集算法，所以 Java 堆可以细分为：新生代和老年代。在细致分就是把新生代分为：Eden 空间、From Survivor 空间、To Survivor 空间。当堆无法再扩展时，会抛出 OutOfMemoryError 异常。 5.方法区方法区在JVM中也是一个非常重要的区域，它与堆一样，是被线程共享的区域。在方法区中，存储了每个类的信息（包括类的名称、方法信息、字段信息）、静态变量、常量以及编译器编译后的代码等。 在Class文件中除了类的字段、方法、接口等描述信息外，还有一项是常量池（Constant Pool Table）,用于存放编译期生成的各种字面量（比如int i=3，这个3就是字面量的意思）和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 在方法区中有一个非常重要的部分就是运行时常量池(Runtime Constant Pool)，它是每一个类或接口的常量池的运行时表示形式，在类和接口被加载到JVM后，对应的运行时常量池就被创建出来。运行时常量池相对于Class文件常量池的一个重要特征是具备动态性：并非Class文件常量池中的内容才能进入运行时常量池，在运行期间也可将新的常量放入运行时常量池中，比如String.intern()方法，这个方法的作用就是：先从方法区的运行时常量池中查找看是否有该值，如果有，则返回该值的引用，如果没有，那么就会将该值加入运行时常量池中。 在JVM规范中，没有强制要求方法区必须实现垃圾回收。很多人习惯将方法区称为“永久代”，是因为HotSpot虚拟机以永久代来实现方法区，从而JVM的垃圾收集器可以像管理堆区一样管理这部分区域，从而不需要专门为这部分设计垃圾回收机制。 不过自从JDK7之后，Hotspot虚拟机便将运行时常量池从永久代移除了，在Java 堆中开辟了一块区域存放运行时常量池。在Java7之前，HotSpot虚拟机中将GC分代收集扩展到了方法区，使用永久代来实现了方法区。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载。而在Java8中，已经彻底没有了永久代，将方法区直接放在一个与堆不相连的本地内存区域，这个区域被叫做元空间。 常用JVM 参数选项： 堆设置 -Xms :初始堆大小 -Xmx :最大堆大小 -XX:NewSize=n :设置年轻代大小 -XX:NewRatio=n: 设置年轻代和年老代的比值。如:为3，表示年轻代与年老代比值为1：3，年轻代占整个年轻代年老代和的1/4 -XX:SurvivorRatio=n :年轻代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：3，表示Eden：Survivor=3：2，一个Survivor区占整个年轻代的1/5 -XX:PermSize=n :初始永久代大小 -XX:MaxPermSize=n :最大永久代大小 收集器设置 -XX:+UseSerialGC :设置串行收集器 -XX:+UseParallelGC :设置并行收集器 -XX:+UseParalledlOldGC :设置并行年老代收集器 -XX:+UseConcMarkSweepGC :设置并发收集器 垃圾回收统计信息 -XX:+PrintGC -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:filename 并行收集器设置 -XX:ParallelGCThreads=n :设置并行收集器收集时使用的CPU数。并行收集线程数。 -XX:MaxGCPauseMillis=n :设置并行收集最大暂停时间 -XX:GCTimeRatio=n :设置垃圾回收时间占程序运行时间的百分比。公式为1/(1+n) 并发收集器设置 -XX:+CMSIncrementalMode :设置为增量模式。适用于单CPU情况。 -XX:ParallelGCThreads=n :设置并发收集器年轻代收集方式为并行收集时，使用的CPU数。并行收集线程数。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[详解Spring的IoC原理]]></title>
    <url>%2Fblog%2F2016%2F02%2F09%2F2016-02-09-%E8%AF%A6%E8%A7%A3Spring%E7%9A%84IoC%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[IoC理论的背景我们都知道，在采用面向对象方法设计的软件系统中，它的底层实现都是由N个对象组成的，所有的对象通过彼此的合作，最终实现系统的业务逻辑。 如果我们打开机械式手表的后盖，就会看到与上面类似的情形，各个齿轮分别带动时针、分针和秒针顺时针旋转，从而在表盘上产生正确的时间。 图1中描述的就是这样的一个齿轮组，它拥有多个独立的齿轮，这些齿轮相互啮合在一起，协同工作，共同完成某项任务。我们可以看到，在这样的齿轮组中，如果有一个齿轮出了问题，就可能会影响到整个齿轮组的正常运转。 齿轮组中齿轮之间的啮合关系,与软件系统中对象之间的耦合关系非常相似。对象之间的耦合关系是无法避免的，也是必要的，这是协同工作的基础。现在，伴随着工业级应用的规模越来越庞大，对象之间的依赖关系也越来越复杂，经常会出现对象之间的多重依赖性关系，因此，架构师和设计师对于系统的分析和设计，将面临更大的挑战。对象之间耦合度过高的系统，必然会出现牵一发而动全身的情形。 耦合关系不仅会出现在对象与对象之间，也会出现在软件系统的各模块之间，以及软件系统和硬件系统之间。如何降低系统之间、模块之间和对象之间的耦合度，是软件工程永远追求的目标之一。为了解决对象之间的耦合度过高的问题，软件专家Michael Mattson提出了IoC理论，用来实现对象之间的“解耦”，目前这个理论已经被成功地应用到实践当中，很多的J2EE项目均采用了IoC框架产品Spring。 什么是控制反转(IoC)IoC是Inversion of Control的缩写，多数书籍翻译成“控制反转”，还有些书籍翻译成为“控制反向”或者“控制倒置”。 1996年，Michael Mattson在一篇有关探讨面向对象框架的文章中，首先提出了IoC 这个概念。对于面向对象设计及编程的基本思想，前面我们已经讲了很多了，不再赘述，简单来说就是把复杂系统分解成相互合作的对象，这些对象类通过封装以后，内部实现对外部是透明的，从而降低了解决问题的复杂度，而且可以灵活地被重用和扩展。IoC理论提出的观点大体是这样的：借助于“第三方”实现具有依赖关系的对象之间的解耦，如下图： 大家看到了吧，由于引进了中间位置的“第三方”，也就是IoC容器，使得A、B、C、D这4个对象没有了耦合关系，齿轮之间的传动全部依靠“第三方”了，全部对象的控制权全部上缴给“第三方”IoC容器，所以，IoC容器成了整个系统的关键核心，它起到了一种类似“粘合剂”的作用，把系统中的所有对象粘合在一起发挥作用，如果没有这个“粘合剂”，对象与对象之间会彼此失去联系，这就是有人把IoC容器比喻成“粘合剂”的由来。 我们再来做个试验：把上图中间的IoC容器拿掉，然后再来看看这套系统： 我们现在看到的画面，就是我们要实现整个系统所需要完成的全部内容。这时候，A、B、C、D这4个对象之间已经没有了耦合关系，彼此毫无联系，这样的话，当你在实现A的时候，根本无须再去考虑B、C和D了，对象之间的依赖关系已经降低到了最低程度。所以，如果真能实现IoC容器，对于系统开发而言，这将是一件多么美好的事情，参与开发的每一成员只要实现自己的类就可以了，跟别人没有任何关系！ 我们再来看看，控制反转(IoC)到底为什么要起这么个名字？我们来对比一下： 软件系统在没有引入IoC容器之前，如图1所示，对象A依赖于对象B，那么对象A在初始化或者运行到某一点的时候，自己必须主动去创建对象B或者使用已经创建的对象B。无论是创建还是使用对象B，控制权都在自己手上。 软件系统在引入IoC容器之后，这种情形就完全改变了，如图3所示，由于IoC容器的加入，对象A与对象B之间失去了直接联系，所以，当对象A运行到需要对象B的时候，IoC容器会主动创建一个对象B注入到对象A需要的地方。 通过前后的对比，我们不难看出来：对象A获得依赖对象B的过程，由主动行为变为了被动行为，控制权颠倒过来了，这就是“控制反转”这个名称的由来。 IoC的别名：依赖注入(DI)2004年，Martin Fowler探讨了同一个问题，既然IoC是控制反转，那么到底是“哪些方面的控制被反转了呢？”，经过详细地分析和论证后，他得出了答案：“获得依赖对象的过程被反转了”。控制被反转之后，获得依赖对象的过程由自身管理变为了由IoC容器主动注入。于是，他给“控制反转”取了一个更合适的名字叫做“依赖注入（Dependency Injection）”。他的这个答案，实际上给出了实现IoC的方法：注入。所谓依赖注入，就是由IoC容器在运行期间，动态地将某种依赖关系注入到对象之中。 所以，依赖注入(DI)和控制反转(IoC)是从不同的角度的描述的同一件事情，就是指通过引入IoC容器，利用依赖关系注入的方式，实现对象之间的解耦。 我们举一个生活中的例子，来帮助理解依赖注入的过程。大家对USB接口和USB设备应该都很熟悉吧，USB为我们使用电脑提供了很大的方便，现在有很多的外部设备都支持USB接口。 现在，我们利用电脑主机和USB接口来实现一个任务：从外部USB设备读取一个文件。 电脑主机读取文件的时候，它一点也不会关心USB接口上连接的是什么外部设备，而且它确实也无须知道。它的任务就是读取USB接口，挂接的外部设备只要符合USB接口标准即可。所以，如果我给电脑主机连接上一个U盘，那么主机就从U盘上读取文件；如果我给电脑主机连接上一个外置硬盘，那么电脑主机就从外置硬盘上读取文件。挂接外部设备的权力由我作主，即控制权归我，至于USB接口挂接的是什么设备，电脑主机是决定不了，它只能被动的接受。电脑主机需要外部设备的时候，根本不用它告诉我，我就会主动帮它挂上它想要的外部设备，你看我的服务是多么的到位。这就是我们生活中常见的一个依赖注入的例子。在这个过程中，我就起到了IoC容器的作用。 通过这个例子,依赖注入的思路已经非常清楚：当电脑主机读取文件的时候，我就把它所要依赖的外部设备，帮他挂接上。整个外部设备注入的过程和一个被依赖的对象在系统运行时被注入另外一个对象内部的过程完全一样。 我们把依赖注入应用到软件系统中，再来描述一下这个过程： 对象A依赖于对象B,当对象 A需要用到对象B的时候，IoC容器就会立即创建一个对象B送给对象A。IoC容器就是一个对象制造工厂，你需要什么，它会给你送去，你直接使用就行了，而再也不用去关心你所用的东西是如何制成的，也不用关心最后是怎么被销毁的，这一切全部由IoC容器包办。 在传统的实现中，由程序内部代码来控制组件之间的关系。我们经常使用new关键字来实现两个组件之间关系的组合，这种实现方式会造成组件之间耦合。IoC很好地解决了该问题，它将实现组件间关系从程序内部提到外部容器，也就是说由容器在运行期将组件间的某种依赖关系动态注入组件中。 IoC为我们带来了什么好处我们还是从USB的例子说起，使用USB外部设备比使用内置硬盘，到底带来什么好处？ 第一、USB设备作为电脑主机的外部设备，在插入主机之前，与电脑主机没有任何的关系，只有被我们连接在一起之后，两者才发生联系，具有相关性。所以，无论两者中的任何一方出现什么的问题，都不会影响另一方的运行。这种特性体现在软件工程中，就是可维护性比较好，非常便于进行单元测试，便于调试程序和诊断故障。代码中的每一个Class都可以单独测试，彼此之间互不影响，只要保证自身的功能无误即可，这就是组件之间低耦合或者无耦合带来的好处。 第二、USB设备和电脑主机的之间无关性，还带来了另外一个好处，生产USB设备的厂商和生产电脑主机的厂商完全可以是互不相干的人，各干各事，他们之间唯一需要遵守的就是USB接口标准。这种特性体现在软件开发过程中，好处可是太大了。每个开发团队的成员都只需要关心实现自身的业务逻辑，完全不用去关心其它的人工作进展，因为你的任务跟别人没有任何关系，你的任务可以单独测试，你的任务也不用依赖于别人的组件，再也不用扯不清责任了。所以，在一个大中型项目中，团队成员分工明确、责任明晰，很容易将一个大的任务划分为细小的任务，开发效率和产品质量必将得到大幅度的提高。 第三、同一个USB外部设备可以插接到任何支持USB的设备，可以插接到电脑主机，也可以插接到DV机，USB外部设备可以被反复利用。在软件工程中，这种特性就是可复用性好，我们可以把具有普遍性的常用组件独立出来，反复利用到项目中的其它部分，或者是其它项目，当然这也是面向对象的基本特征。显然，IoC不仅更好地贯彻了这个原则，提高了模块的可复用性。符合接口标准的实现，都可以插接到支持此标准的模块中。 第四、同USB外部设备一样，模块具有热插拔特性。IoC生成对象的方式转为外置方式，也就是把对象生成放在配置文件里进行定义，这样，当我们更换一个实现子类将会变得很简单，只要修改配置文件就可以了，完全具有热插拨的特性。 IoC容器的技术剖析IoC中最基本的技术就是“反射(Reflection)”编程，目前.Net C#、Java和PHP5等语言均支持，其中PHP5的技术书籍中，有时候也被翻译成“映射”。有关反射的概念和用法，大家应该都很清楚，通俗来讲就是根据给出的类名（字符串方式）来动态地生成对象。这种编程方式可以让对象在生成时才决定到底是哪一种对象。反射的应用是很广泛的，很多的成熟的框架，比如象Java中的Hibernate、Spring框架，.Net中 NHibernate、Spring.Net框架都是把“反射”做为最基本的技术手段。 反射技术其实很早就出现了，但一直被忽略，没有被进一步的利用。当时的反射编程方式相对于正常的对象生成方式要慢至少得10倍。现在的反射技术经过改良优化，已经非常成熟，反射方式生成对象和通常对象生成方式，速度已经相差不大了，大约为1-2倍的差距。 我们可以把IoC容器的工作模式看做是工厂模式的升华，可以把IoC容器看作是一个工厂，这个工厂里要生产的对象都在配置文件中给出定义，然后利用编程语言的的反射编程，根据配置文件中给出的类名生成相应的对象。从实现来看，IoC是把以前在工厂方法里写死的对象生成代码，改变为由配置文件来定义，也就是把工厂和对象生成这两者独立分隔开来，目的就是提高灵活性和可维护性。 使用IoC框架应该注意什么使用IoC框架产品能够给我们的开发过程带来很大的好处，但是也要充分认识引入IoC框架的缺点，做到心中有数，杜绝滥用框架。 第一、软件系统中由于引入了第三方IoC容器，生成对象的步骤变得有些复杂，本来是两者之间的事情，又凭空多出一道手续，所以，我们在刚开始使用IoC框架的时候，会感觉系统变得不太直观。所以，引入了一个全新的框架，就会增加团队成员学习和认识的培训成本，并且在以后的运行维护中，还得让新加入者具备同样的知识体系。 第二、由于IoC容器生成对象是通过反射方式，在运行效率上有一定的损耗。如果你要追求运行效率的话，就必须对此进行权衡。 第三、具体到IoC框架产品(比如：Spring)来讲，需要进行大量的配制工作，比较繁琐，对于一些小的项目而言，客观上也可能加大一些工作成本。 第四、IoC框架产品本身的成熟度需要进行评估，如果引入一个不成熟的IoC框架产品，那么会影响到整个项目，所以这也是一个隐性的风险。 我们大体可以得出这样的结论：一些工作量不大的项目或者产品，不太适合使用IoC框架产品。另外，如果团队成员的知识能力欠缺，对于IoC框架产品缺乏深入的理解，也不要贸然引入。最后，特别强调运行效率的项目或者产品，也不太适合引入IoC框架产品，像WEB2.0网站就是这种情况。]]></content>
      <categories>
        <category>Spring</category>
        <category>IoC</category>
      </categories>
      <tags>
        <tag>Spring-IoC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[九种内部排序算法的Java实现及其性能测试]]></title>
    <url>%2Fblog%2F2015%2F12%2F03%2F2015-12-03-%E4%B9%9D%E7%A7%8D%E5%86%85%E9%83%A8%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E7%9A%84Java%E5%AE%9E%E7%8E%B0%E5%8F%8A%E5%85%B6%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[九种内部排序算法的Java实现及其性能测试9种内部排序算法性能比较第九种为java.util.Arrays.sort（改进的快速排序方法） 100000的随机数据集 200000的随机数据集 500000的随机数据集 结论：归并排序和堆排序维持O(nlgn)的复杂度，速率差不多，表现优异。固定基准的快排表现很是优秀。而通过使用一个循环完成按增量分组后的直接插入的希尔排序，测试效果显著。冒泡，选择，直接插入都很慢，而冒泡效率是最低。 1.插入排序[稳定]适用于小数组,数组已排好序或接近于排好序速度将会非常快 复杂度：O(n^2) - O(n) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 1234567891011121314151617public void insertionSort(int[] a) &#123; if (null == a || a.length &lt; 2) &#123; return; &#125; for (int i = 1; i &lt; a.length; i++) &#123; // 暂存当前值 int temp = a[i]; int j = i - 1; while (j &gt;= 0 &amp;&amp; temp &lt; a[j]) &#123; // 后移 a[j + 1] = a[j]; j--; &#125; // 当前值归位 a[j + 1] = temp; &#125;&#125; 2.希尔排序(缩小增量排序)[不稳定]复杂度 平均 O(n^1.3) 最好O(n) 最差O(n^s)[1&lt;s&lt;2] 空间O(1) 内循环通过模拟并行的方式完成分组的内部直接插入排序，而不是一个一个分组分组的排，在10w的随机数据20w的随机数据均表现优异。 123456789101112131415161718public void shellSort(int[] a) &#123; if (null == a || a.length &lt; 2) &#123; return; &#125; for (int d = a.length / 2; d &gt; 0; d /= 2) &#123; // 从1B开始先和1A比较 然后2A与2B...然后再1C向前与同组的比较 for (int i = d; i &lt; a.length; i++) &#123; // 内部直接插入 int temp = a[i]; int j = i - d; while (j &gt;= 0 &amp;&amp; temp &lt; a[j]) &#123; a[j + d] = a[j]; j -= d; &#125; a[j + d] = temp; &#125; &#125;&#125; 3.冒泡排序[稳定]复杂度：O(n^2) - O(n) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 1234567891011121314151617181920public void bubbleSort(int[] a) &#123; if (null == a || a.length &lt; 2) &#123; return; &#125; boolean flag; for (int i = 0; i &lt; a.length - 1; i++) &#123; flag = false; for (int j = 0; j &lt; a.length - 1 - i; j++) &#123; if (a[j] &gt; a[j + 1]) &#123; int temp = a[j]; a[j] = a[j + 1]; a[j + 1] = temp; flag = true; &#125; &#125; if (false == flag) &#123; return; &#125; &#125;&#125; 4.选择排序[不稳定]原理：每次从无序序列选取最小的 复杂度：O(n^2) - O(n^2) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 123456789101112131415161718public void selectSort(int[] a) &#123; if (null == a || a.length &lt; 2) &#123; return; &#125; for (int i = 0; i &lt; a.length; i++) &#123; int k = i; for (int j = i + 1; j &lt; a.length; j++) &#123; if (a[j] &lt; a[k]) &#123; k = j; &#125; &#125; if (k != i) &#123; int temp = a[k]; a[k] = a[i]; a[i] = temp; &#125; &#125;&#125; 5.归并排序[稳定]原理：采用分治法 复杂度：O(nlogn) - O(nlgn) - O(nlgn) - O(n)[平均 - 最好 - 最坏 - 空间复杂度] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** 排序 */public void mergeSort(int[] a, int low, int high) &#123; int mid = (low + high) / 2; if (low &lt; high) &#123; // 左边排序 mergeSort(a, low, mid); // 右边排序 mergeSort(a, mid + 1, high); // 有序序列合并 merge(a, low, mid, high); &#125;&#125;/** 合并 */private void merge(int a[], int low, int mid, int high) &#123; // 临时数组 int[] temp = new int[high - low + 1]; // 左指针 int i = low; // 右指针 int j = mid + 1; // 临时数组索引 int k = 0; while (i &lt;= mid &amp;&amp; j &lt;= high) &#123; if (a[i] &lt; a[j]) &#123; temp[k++] = a[i++]; &#125; else &#123; temp[k++] = a[j++]; &#125; &#125; // 把左边剩余的数移入数组 while (i &lt;= mid) &#123; temp[k++] = a[i++]; &#125; // 把右边剩余的数移入数组 while (j &lt;= high) &#123; temp[k++] = a[j++]; &#125; // 注意这里是low + t for (int t = 0; t &lt; temp.length; t++) &#123; a[low + t] = temp[t]; &#125;&#125; 6.快速排序[不稳定]原理：分治+递归 复杂度：O(nlgn) - O(nlgn) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 栈空间0(lgn) - O(n) 1234567891011121314151617181920212223242526272829/** 固定基准 */public void quickSort(int[] a, int low, int high) &#123; if (null == a || a.length &lt; 2) &#123; return; &#125; if (low &lt; high) &#123; int mid = partition(a, low, high); quickSort(a, low, mid - 1); quickSort(a, mid + 1, high); &#125;&#125;private int partition(int[] a, int low, int high) &#123; int pivot = a[low]; while (low &lt; high) &#123; // 注意等于，否则死循环 while (low &lt; high &amp;&amp; a[high] &gt;= pivot) &#123; high--; &#125; a[low] = a[high]; // 注意等于，否则死循环 while (low &lt; high &amp;&amp; a[low] &lt;= pivot) &#123; low++; &#125; a[high] = a[low]; &#125; a[low] = pivot; return low;&#125; 7.堆排序[不稳定]堆一般指二叉堆。 复杂度：O(nlogn) - O(nlgn) - O(nlgn) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 大顶堆实现从小到大的升序排列，小顶堆一般用于构造优先队列 123456789101112131415161718192021222324252627282930313233343536373839404142434445public void heapSort(int[] a) &#123; if (null == a || a.length &lt; 2) &#123; return; &#125; buildMaxHeap(a); for (int i = a.length - 1; i &gt;= 0; i--) &#123; int temp = a[0]; a[0] = a[i]; a[i] = temp; adjustHeap(a, i, 0); &#125;&#125;/** 建堆 */private void buildMaxHeap(int[] a) &#123; int mid = a.length / 2; for (int i = mid; i &gt;= 0; i--) &#123; adjustHeap(a, a.length, i); &#125;&#125;/** 递归调整堆 */private void adjustHeap(int[] a, int size, int parent) &#123; int left = 2 * parent + 1; int right = 2 * parent + 2; int largest = parent; if (left &lt; size &amp;&amp; a[left] &gt; a[parent]) &#123; largest = left; &#125; if (right &lt; size &amp;&amp; a[right] &gt; a[largest]) &#123; largest = right; &#125; if (parent != largest) &#123; int temp = a[parent]; a[parent] = a[largest]; a[largest] = temp; adjustHeap(a, size, largest); &#125;&#125; 8.基数排序[稳定]原理：分配加收集 复杂度： O(d(n+r)) r为基数d为位数 空间复杂度O(n+r) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** 基数排序 */public void radixSort(int[] a, int begin, int end, int digit) &#123; // 基数 final int radix = 10; // 桶中的数据统计 int[] count = new int[radix]; int[] bucket = new int[end - begin + 1]; // 按照从低位到高位的顺序执行排序过程 for (int i = 1; i &lt;= digit; i++) &#123; // 清空桶中的数据统计 for (int j = 0; j &lt; radix; j++) &#123; count[j] = 0; &#125; // 统计各个桶将要装入的数据个数 for (int j = begin; j &lt;= end; j++) &#123; int index = getDigit(a[j], i); count[index]++; &#125; // count[i]表示第i个桶的右边界索引 for (int j = 1; j &lt; radix; j++) &#123; count[j] = count[j] + count[j - 1]; &#125; // 将数据依次装入桶中 // 这里要从右向左扫描，保证排序稳定性 for (int j = end; j &gt;= begin; j--) &#123; int index = getDigit(a[j], i); bucket[count[index] - 1] = a[j]; count[index]--; &#125; // 取出，此时已是对应当前位数有序的表 for (int j = 0; j &lt; bucket.length; j++) &#123; a[j] = bucket[j]; &#125; &#125;&#125;/** 获取x的第d位的数字，其中最低位d=1 */private int getDigit(int x, int d) &#123; String div = "1"; while (d &gt;= 2) &#123; div += "0"; d--; &#125; return x / Integer.parseInt(div) % 10;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[静态代码块和构造代码块和构造方法执行顺序]]></title>
    <url>%2Fblog%2F2015%2F11%2F12%2F2015-11-12-%E9%9D%99%E6%80%81%E4%BB%A3%E7%A0%81%E5%9D%97%E5%92%8C%E6%9E%84%E9%80%A0%E4%BB%A3%E7%A0%81%E5%9D%97%E5%92%8C%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[代码块在Java中，使用{}括起来的代码被称为代码块。 分类根据其位置和声明的不同，可以分为 局部代码块：局部位置,用于限定变量的生命周期。 构造代码块：在类中的成员位置,用{}括起来的代码。每次调用构造方法执行前，都会先执行构造代码块。作用：可以把多个构造方法中的共同代码放到一起，对对象进行初始化。 静态代码块：在类中的成员位置,用{}括起来的代码,只不过它用static修饰了。作用：一般是对类进行初始化。 代码执行顺序单个类中: 静态代码块(静态成员变量) -&gt; main方法 -&gt; 构造代码块(成员变量) -&gt; 构造方法静态代码块：只执行一次构造代码块：在每一次创建对象时执行 涉及父类和子类的初始化过程：a.初始化父类中的静态成员变量和静态代码块(按次序)b.初始化子类中的静态成员变量和静态代码块(按次序)c.初始化父类的普通成员变量和构造代码块(按次序)，再执行父类的构造方法(注意父类构造方法中的子类方法覆盖)d.初始化子类的普通成员变量和构造代码块(按次序)，再执行子类的构造方法 看程序写结果12345678910111213141516171819202122232425262728293031323334class Test &#123; static &#123; System.out.println("Test 静态代码块"); &#125; &#123; System.out.println("Test 构造代码块"); &#125; public Test() &#123; System.out.println("Test 构造方法"); &#125; &#125;public class TestDemo &#123; static &#123; System.out.println("TestDemo 静态代码块"); &#125; &#123; System.out.println("TestDemo 构造代码块"); &#125; public static void main(String[] args) &#123; System.out.println("TestDemo main方法"); Test t1 = new Test(); Test t2 = new Test(); &#125; &#125; 其运行结果是： TestDemo 静态代码块TestDemo main方法Test 静态代码块Test 构造代码块Test 构造方法Test 构造代码块Test 构造方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class Test extends MyClass&#123; &#123; System.out.println("Test 构造代码块"); &#125; Person person = new Person("Test"); static &#123; System.out.println("Test 静态代码块"); &#125; public Test() &#123; System.out.println("Test 构造方法"); &#125; public static void main(String[] args) &#123; new Test(); &#125;&#125;class Person &#123; static &#123; System.out.println("Person 静态代码块"); &#125; &#123; System.out.println("Person 构造代码块"); &#125; public Person(String str) &#123; System.out.println("Person 构造方法，参数是" + str); &#125; &#125;class MyClass &#123; static Person person = new Person("MyClass"); static &#123; System.out.println("MyClass 静态代码块"); &#125; &#123; System.out.println("MyClass 构造代码块"); &#125; public MyClass() &#123; System.out.println("MyClass 构造方法"); &#125; &#125; 其运行结果是： Person 静态代码块Person 构造代码块Person 构造方法，参数是MyClassMyClass 静态代码块Test 静态代码块MyClass 构造代码块MyClass 构造方法Test 构造代码块Person 构造代码块Person 构造方法，参数是TestTest 构造方法]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>执行顺序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git快速入门和常用命令]]></title>
    <url>%2Fblog%2F2015%2F10%2F03%2F2015-10-03-Git%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%92%8C%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[简易图解git流程概念 Workspace：工作区 Index / Stage：暂存区 Repository：本地仓库区（用HEAD指向最后一次commit的结果） Remote：远程仓库 一、快速入门本地初始化一个项目首先，你需要执行下面两条命令，作为 git 的基础配置，作用是告诉 git 你是谁，你输入的信息将出现在你创建的提交中。 12git config --global user.name "你的名字或昵称"git config --global user.email "你的邮箱" 然后在你的需要初始化版本库的文件夹中执行： 12git init git remote add origin 你的项目地址 # 项目地址形式为:http://git.oschina.net/xxx/xxx.git或者 git@git.oschina.net:xxx/xxx.git 这样就完成了一次版本你的初始化。 如果你想克隆一个项目，只需要执行： 1git clone 项目地址 完成第一次提交进入你已经初始化好的或者克隆项目的目录，然后执行： 1234git pull origin master # 从远程仓库获取最新版本并合并，因为你提交前可能已经有别人提交过了，你需要合并最新版本才能提交到远程仓库git add .git commit -m "这是我第一次提交的说明"git push origin master 然后如果需要账号密码的话就输入账号密码，这样就完成了一次提交。 二、基本命令配置123456789# 显示当前的Git配置git config --list# 编辑Git配置文件git config -e [--global]# 设置提交代码时的用户信息git config --global user.name "[name]"git config --global user.email "[email address]" 获取与创建本地项目12git init # 在当前目录新建一个Git代码库git init 目录 # 新建一个目录，将其初始化为Git代码库 1git clone 项目地址URL # 拷贝一个Git仓库到本地 默认情况下，Git 会按照你提供的 URL 所指示的项目的名称创建你的本地项目目录。 通常就是该 URL 最后一个 / 之后的项目名称。如果你想要一个不一样的名字， 你可以在该命令后加上你想要的名称。URL也可以是你的本地仓库地址，这样就是创建了一个本地仓库的克隆版本。 增加/删除文件到Index你可以计划改动（把它们添加到缓存区），使用如下命令：git add [filename]git add . 这是 git 基本工作流程的第一步； 123456789101112131415161718192021222324252627# 添加指定文件到暂存区git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录git add [dir]# 添加当前目录的所有文件到暂存区git add .# 添加每个变化前，都会要求确认# 对于同一个文件的多处变化，可以实现分次提交git add -p# 删除工作区文件，并且将这次删除放入暂存区git rm [file1] [file2] ...# 把文件从暂存区域移除，但保留在当前工作目录中git rm --cached [file]# 如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -fgit rm -f [file]# 递归删除整个目录中的所有子目录和文件git rm –r * # 改名文件，并且将这个改名放入暂存区git mv [file-original] [file-renamed] 代码提交使用如下命令以实际提交改动：git commit -m “代码提交信息”现在，你的改动已经提交到了 HEAD，但是还没到你的远端仓库。 123456789101112131415161718# 提交暂存区到仓库区，并填写备注信息git commit -m [message]# 提交暂存区的指定文件到仓库区git commit [file1] [file2] ... -m [message]# 提交工作区自上次commit之后的变化，直接到仓库区git commit -a# 提交时显示所有diff信息git commit -v# 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息git commit --amend -m [message]# 重做上一次commit，并包括指定文件的新变化git commit --amend [file1] [file2] ... 同步远程仓库你的改动现在已经在本地仓库的 HEAD 中了。执行如下命令以将这些改动提交到远端仓库：git push origin master可以把 master 换成你想要推送的任何分支。 如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加：git remote add origin [server]如此你就能够将你的改动推送到所添加的服务器上去了。 1234567891011121314151617181920212223# 下载远程仓库的所有变动git fetch [remote]# 查看当前配置有哪些远程仓库git remote -v# 显示某个远程仓库的信息git remote show [remote]# 增加一个新的远程仓库，并命名git remote add [shortname] [url]# 取回远程仓库的变化，并与本地分支合并git pull [remote] [branch]# 上传本地指定分支到远程仓库git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突git push [remote] --force# 推送所有分支到远程仓库git push [remote] --all 要更新你的本地仓库至最新改动，执行：git pull以在你的工作目录中 获取（fetch） 并 合并（merge） 远端的改动。要合并其他分支到你的当前分支（例如 master），执行：git merge [branch]两种情况下，git 都会尝试去自动合并改动。不幸的是，自动合并并非次次都能成功，并可能导致 冲突（conflicts）。 这时候就需要你修改这些文件来人肉合并这些 冲突（conflicts） 了。改完之后，你需要执行如下命令以将它们标记为合并成功：git add [filename]在合并改动之前，也可以使用如下命令查看：git diff {source_branch} {target_branch} 三、查看信息1234567891011121314151617181920212223242526272829303132333435# 列出当前目录所有还没有被git管理的文件和被git管理且被修改但还未提交(git commit)的文件,-s参数可简化显示git status# 显示所有提交过的用户，按提交次数排序git shortlog -sn# 显示指定文件是什么人在什么时间修改过git blame [file]# 显示暂存区和工作区的差异git diff# 显示暂存区和上一个commit的差异git diff --cached [file]# 显示工作区与当前分支最新commit之间的差异git diff HEAD# 显示两次提交之间的差异git diff [first-branch]...[second-branch]# 显示今天你写了多少行代码git diff --shortstat "@&#123;0 day ago&#125;"# 显示某次提交的元数据和内容变化git show [commit]# 显示某次提交发生变化的文件git show --name-only [commit]# 显示某次提交时，某个文件的内容git show [commit]:[filename]# 显示当前分支的最近几次提交git reflog 四、撤销假如你做错事，你可以使用如下命令替换掉本地改动：git checkout –[filename]此命令会使用 HEAD 中的最新内容替换掉你的工作目录中的文件。已添加到缓存区的改动，以及新文件，都不受影响。 假如你想要丢弃你所有的本地改动与提交，可以到服务器上获取最新的版本并将你本地主分支指向到它：git fetch origingit reset –hard origin/master 12345678910111213141516171819202122232425262728293031# 恢复暂存区的指定文件到工作区git checkout [file]# 恢复某个commit的指定文件到暂存区和工作区git checkout [commit] [file]# 恢复暂存区的所有文件到工作区git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变git reset [file]# 重置暂存区与工作区，与上一次commit保持一致git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支git revert [commit]# 暂时将未提交的变化移除，稍后再移入git stashgit stash pop 五、分支管理分支是用来将特性开发绝缘开来的。在你创建仓库的时候，master 是“默认的”。在其他分支上进行开发，完成后再将它们合并到主分支上。创建一个叫做“feature_x”的分支，并切换过去：git checkout -b feature_x切换回主分支：git checkout master再把新建的分支删掉：git branch -d feature_x除非你将分支推送到远端仓库，不然该分支就是 不为他人所见的：git push origin [branch] 123456789101112131415161718192021222324252627282930313233343536373839404142# 列出所有本地分支git branch# 列出所有远程分支git branch -r# 列出所有本地分支和远程分支git branch -a# 新建一个分支，但依然停留在当前分支git branch [branch-name]# 新建一个分支，并切换到该分支git checkout -b [branch]# 新建一个分支，指向指定commitgit branch [branch] [commit]# 新建一个分支，与指定的远程分支建立追踪关系git branch --track [branch] [remote-branch]# 切换到指定分支，并更新工作区git checkout [branch-name]# 切换到上一个分支git checkout -# 建立追踪关系，在现有分支与指定的远程分支之间git branch --set-upstream [branch] [remote-branch]# 合并指定分支到当前分支git merge [branch]# 选择一个commit，合并进当前分支git cherry-pick [commit]# 删除分支git branch -d [branch-name]# 删除远程分支git push origin --delete [branch-name]git branch -dr [remote/branch] 六、历史记录123456789101112131415161718192021222324252627282930313233343536373839404142# 列出历史提交记录git log# 显示commit历史，以及每次commit发生变更的文件git log --stat# 查看历史记录的简洁的版本git log --oneline # 查看历史中什么时候出现了分支、合并git log --oneline --graph # 逆向显示所有日志git log --reverse --oneline # 查看历史中什么时候出现了分支、合并git log --oneline --graph# 显示指定文件相关的每一次diffgit log -p [file]# 搜索提交历史，根据关键词git log -S [keyword]# 显示某个commit之后的所有变动，每个commit占据一行git log [tag] HEAD --pretty=format:%s# 显示某个commit之后的所有变动，其"提交说明"必须符合搜索条件git log [tag] HEAD --grep feature# 显示某个文件的版本历史，包括文件改名git log --follow [file]git whatchanged [file]# 显示过去5次提交git log -5 --pretty --oneline# 查找指定用户的提交日志，看5条git log --author=用户名 --oneline -5 # 查看3周前且在10月01日之后的所有提交，--no-merges 选项以隐藏合并提交git log --oneline --before=&#123;3.weeks.ago&#125; --after=&#123;2015-10-01&#125; --no-merges 七、标签在软件发布时创建标签，是被推荐的。这是个旧有概念，在 SVN 中也有。可以执行如下命令以创建一个叫做 1.0.0 的标签：git tag 1.0.0 1b2e1d63ff1b2e1d63ff 是你想要标记的提交 ID 的前 10 位字符。使用如下命令获取提交 ID：git log你也可以用该提交 ID 的少一些的前几位，只要它是唯一的。 123456789101112131415161718192021222324252627# 列出所有taggit tag# 新建一个tag在当前commit# -a 选项意为"创建一个带注解的标签"。 不用 -a 选项也可以执行的，但它不会记录这标签是啥时候打的，谁打的，也不会让你添加个标签的注解。 我推荐一直创建带注解的标签git tag -a [tag]# 新建一个tag在指定commitgit tag [tag] [commit]# 删除本地taggit tag -d [tag]# 删除远程taggit push origin :refs/tags/[tagName]# 查看tag信息git show [tag]# 提交指定taggit push [remote] [tag]# 提交所有taggit push [remote] --tags# 新建一个分支，指向某个taggit checkout -b [branch] [tag] Git有commit，为什么还要引入tag？“请把上周一的那个版本打包发布，commit号是6a5819e…”“一串乱七八糟的数字不好找！”如果换一个办法：“请把上周一的那个版本打包发布，版本号是v1.2”“好的，按照tag v1.2查找commit就行！”所以，tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。 八、http(s)方式如何自动记住密码https 方式每次都要输入密码，按照如下设置即可输入一次就不用再手输入密码的困扰而且又享受 https 带来的极速 按照以下设置记住密码十五分钟： 1git config --global credential.helper cache 如果你想自定义记住的时间，可以这样： 1git config credential.helper 'cache --timeout=3600' # 这里记住的是一个小时，如需其他时间，请修改3600为你想修改的时间，单位是秒 你也可以设置长期记住密码： 1git config --global credential.helper store 或修改仓库的地址带上你的账号密码 1http://yourname:password@git.oschina.net/name/project.git # 注意，在码云上使用邮箱时，请对@符号使用%40替换 如果你原本使用的 ssh 地址想更换成 http(s) 地址，可以执行以下命令: 1234# 删除原本的ssh仓库地址git remote rm origin # origin 代表你原本ssh地址的仓库的别名# 新增http地址的仓库git remote add origin http://git.oschina.net/username/project.git 九、版本回退回退远程仓库的版本先在本地切换到远程仓库要回退的分支对应的本地分支，然后本地回退至你需要的版本，然后执行： 1git push &lt;仓库名&gt; &lt;分支名&gt; -f 以当前版本为基础，回退指定个commit首先，确认你当前的版本需要回退多少个版本，然后计算出你要回退的版本数量，执行如下命令 1git reset HEAD~X # X代表你要回退的版本数量，是数字！！！！ 需要注意的是，如果你是合并过分支，那么被合并分支带过来的 commit 并不会被计入回退数量中，而是只计算一个，所以如果需要一次回退多个 commit，不建议使用这种方法 回退到和远程版本一样有时候，当发生错误修改需要放弃全部修改时，可以以远程分支作为回退点退回到与远程分支一样的地方，执行的命令如下 1git reset --hard origin/master # origin代表你远程仓库的名字，master代表分支名]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git分支的学习笔记整理]]></title>
    <url>%2Fblog%2F2015%2F09%2F21%2F2015-09-21-Git%E5%88%86%E6%94%AF%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[Git 分支几乎每一种版本控制系统都以某种形式支持分支。使用分支意味着你可以从开发主线上分离开来，然后在不影响主线的同时继续工作。在很多版本控制系统中，这是个昂贵的过程，常常需要创建一个源代码目录的完整副本，对大型项目来说会花费很长时间。有人把 Git 的分支模型称为“必杀技特性”，而正是因为它，将 Git 从版本控制系统家族里区分出来。Git 有何特别之处呢？Git 的分支可谓是难以置信的轻量级，它的新建操作几乎可以在瞬间完成，并且在不同分支间切换起来也差不多一样快。和许多其他版本控制系统不同，Git 鼓励在工作流程中频繁使用分支与合并，哪怕一天之内进行许多次都没有关系。理解分支的概念并熟练运用后，你才会意识到为什么 Git 是一个如此强大而独特的工具，并从此真正改变你的开发方式。 何谓分支Git 保存的不是文件差异或者变化量，而只是一系列文件快照。 在 Git 中提交时，会保存一个提交（commit）对象，该对象包含一个指向暂存内容快照的指针，包含本次提交的作者等相关附属信息，包含零个或多个指向该提交对象的父对象指针：首次提交是没有直接祖先的，普通提交有一个祖先，由两个或多个分支合并产生的提交则有多个祖先。 为直观起见，我们假设在工作目录中有三个文件，准备将它们暂存后提交。暂存操作会对每一个文件计算校验和（SHA-1 哈希字串），然后把当前版本的文件快照保存到 Git 仓库中（Git 使用 blob 类型的对象存储这些快照），并将校验和加入暂存区域： 12$ git add README test.rb LICENSE$ git commit -m 'initial commit of my project' 当使用 git commit 新建一个提交对象前，Git 会先计算每一个子目录（本例中就是项目根目录）的校验和，然后在 Git 仓库中将这些目录保存为树（tree）对象。之后 Git 创建的提交对象，除了包含相关提交信息以外，还包含着指向这个树对象（项目根目录）的指针，如此它就可以在将来需要的时候，重现此次快照的内容了。 现在，Git 仓库中有五个对象：三个表示文件快照内容的 blob 对象；一个记录着目录树内容及其中各个文件对应 blob 对象索引的 tree 对象；以及一个包含指向 tree 对象（根目录）的索引和其他提交信息元数据的 commit 对象。概念上来说，仓库中的各个对象保存的数据和相互关系看起来如图 1 所示： 作些修改后再次提交，那么这次的提交对象会包含一个指向上次提交对象的指针（即下图中的 parent 对象）。两次提交后，仓库历史会变成图 2 的样子： 现在来谈分支。Git 中的分支，其实本质上仅仅是个指向 commit 对象的可变指针。Git 会使用 master 作为分支的默认名字。在若干次提交后，你其实已经有了一个指向最后一次提交对象的 master 分支，它在每次提交的时候都会自动向前移动。 那么，Git 又是如何创建一个新的分支的呢？答案很简单，创建一个新的分支指针。比如新建一个 testing 分支，可以使用 git branch 命令： 1$ git branch testing 这会在当前 commit 对象上新建一个分支指针（见图 4）。 那么，Git 是如何知道你当前在哪个分支上工作的呢？其实答案也很简单，它保存着一个名为 HEAD 的特别指针。请注意它和你熟知的许多其他版本控制系统（比如 Subversion 或 CVS）里的 HEAD 概念大不相同。在 Git 中，它是一个指向你正在工作中的本地分支的指针（将 HEAD 想象为当前分支的别名）。运行 git branch 命令，仅仅是建立了一个新的分支，但不会自动切换到这个分支中去，所以在这个例子中，我们依然还在 master 分支里工作（参考图 5）。 要切换到其他分支，可以执行 git checkout 命令。我们现在转换到新建的 testing 分支： 1$ git checkout testing 这样 HEAD 就指向了 testing 分支（见图 6）。 这样的实现方式会给我们带来什么好处呢？好吧，现在不妨再提交一次： 12$ vim test.rb$ git commit -a -m 'made a change' 图 7 展示了提交后的结果。 非常有趣，现在 testing 分支向前移动了一格，而 master 分支仍然指向原先 git checkout 时所在的 commit 对象。现在我们回到 master 分支看看： 1$ git checkout master 图 8 显示了结果。 这条命令做了两件事。它把 HEAD 指针移回到 master 分支，并把工作目录中的文件换成了 master 分支所指向的快照内容。也就是说，现在开始所做的改动，将始于本项目中一个较老的版本。它的主要作用是将 testing 分支里作出的修改暂时取消，这样你就可以向另一个方向进行开发。 我们作些修改后再次提交： 12$ vim test.rb$ git commit -a -m 'made other changes' 现在我们的项目提交历史产生了分叉（如图 9 所示），因为刚才我们创建了一个分支，转换到其中进行了一些工作，然后又回到原来的主分支进行了另外一些工作。这些改变分别孤立在不同的分支里：我们可以在不同分支里反复切换，并在时机成熟时把它们合并到一起。而所有这些工作，仅仅需要 branch 和 checkout 这两条命令就可以完成。 由于 Git 中的分支实际上仅是一个包含所指对象校验和（40 个字符长度 SHA-1 字串）的文件，所以创建和销毁一个分支就变得非常廉价。说白了，新建一个分支就是向一个文件写入 41 个字节（外加一个换行符）那么简单，当然也就很快了。 这和大多数版本控制系统形成了鲜明对比，它们管理分支大多采取备份所有项目文件到特定目录的方式，所以根据项目文件数量和大小不同，可能花费的时间也会有相当大的差别，快则几秒，慢则数分钟。而 Git 的实现与项目复杂度无关，它永远可以在几毫秒的时间内完成分支的创建和切换。同时，因为每次提交时都记录了祖先信息（即 parent 对象），将来要合并分支时，寻找恰当的合并基础（即共同祖先）的工作其实已经自然而然地摆在那里了，所以实现起来非常容易。Git 鼓励开发者频繁使用分支，正是因为有着这些特性作保障。 分支的新建与合并现在让我们来看一个简单的分支与合并的例子，实际工作中大体也会用到这样的工作流程： 开发某个网站。 为实现某个新的需求，创建一个分支。 在这个分支上开展工作。 假设此时，你突然接到一个电话说有个很严重的问题需要紧急修补，那么可以按照下面的方式处理： 返回到原先已经发布到生产服务器上的分支。 为这次紧急修补建立一个新分支，并在其中修复问题。 通过测试后，回到生产服务器所在的分支，将修补分支合并进来，然后再推送到生产服务器上。 切换到之前实现新需求的分支，继续工作。 分支的新建与切换首先，我们假设你正在项目中愉快地工作，并且已经提交了几次更新（见图 10）。 现在，你决定要修补问题追踪系统上的 #53 问题。顺带说明下，Git 并不同任何特定的问题追踪系统打交道。这里为了说明要解决的问题，才把新建的分支取名为 iss53。要新建并切换到该分支，运行 git checkout 并加上 -b 参数： 12$ git checkout -b iss53Switched to a new branch 'iss53' 这相当于执行下面这两条命令： 12$ git branch iss53$ git checkout iss53 图 11 示意该命令的执行结果。 接着你开始尝试修复问题，在提交了若干次更新后，iss53 分支的指针也会随着向前推进，因为它就是当前分支（换句话说，当前的 HEAD 指针正指向 iss53，见图 12）： 12$ vim index.html$ git commit -a -m 'added a new footer [issue 53]' 现在你就接到了那个网站问题的紧急电话，需要马上修补。有了 Git ，我们就不需要同时发布这个补丁和 iss53 里作出的修改，也不需要在创建和发布该补丁到服务器之前花费大力气来复原这些修改。唯一需要的仅仅是切换回 master 分支。 不过在此之前，留心你的暂存区或者工作目录里，那些还没有提交的修改，它会和你即将检出的分支产生冲突从而阻止 Git 为你切换分支。切换分支的时候最好保持一个清洁的工作区域。稍后会介绍几个绕过这种问题的办法（分别叫做 stashing 和 commit amending）。目前已经提交了所有的修改，所以接下来可以正常转换到 master 分支： 12$ git checkout masterSwitched to branch 'master' 此时工作目录中的内容和你在解决问题 #53 之前一模一样，你可以集中精力进行紧急修补。这一点值得牢记：Git 会把工作目录的内容恢复为检出某分支时它所指向的那个提交对象的快照。它会自动添加、删除和修改文件以确保目录的内容和你当时提交时完全一样。 接下来，你得进行紧急修补。我们创建一个紧急修补分支 hotfix 来开展工作，直到搞定（见图 13）： 123456$ git checkout -b hotfixSwitched to a new branch 'hotfix'$ vim index.html$ git commit -a -m 'fixed the broken email address'[hotfix 3a0874c] fixed the broken email address 1 files changed, 1 deletion(-) 有必要作些测试，确保修补是成功的，然后回到 master 分支并把它合并进来，然后发布到生产服务器。用 git merge 命令来进行合并： 123456$ git checkout master$ git merge hotfixUpdating f42c576..3a0874cFast-forward README | 1 - 1 file changed, 1 deletion(-) 请注意，合并时出现了“Fast forward”的提示。由于当前 master 分支所在的提交对象是要并入的 hotfix 分支的直接上游，Git 只需把 master 分支指针直接右移。换句话说，如果顺着一个分支走下去可以到达另一个分支的话，那么 Git 在合并两者时，只会简单地把指针右移，因为这种单线的历史分支不存在任何需要解决的分歧，所以这种合并过程可以称为快进（Fast forward）。 现在最新的修改已经在当前 master 分支所指向的提交对象中了，可以部署到生产服务器上去了（见图 14）。 在那个超级重要的修补发布以后，你想要回到被打扰之前的工作。由于当前 hotfix 分支和 master 都指向相同的提交对象，所以 hotfix 已经完成了历史使命，可以删掉了。使用 git branch 的 -d 选项执行删除操作： 12$ git branch -d hotfixDeleted branch hotfix (was 3a0874c). 现在回到之前未完成的 #53 问题修复分支上继续工作（图 15）： 123456$ git checkout iss53Switched to branch 'iss53'$ vim index.html$ git commit -a -m 'finished the new footer [issue 53]'[iss53 ad82d7a] finished the new footer [issue 53] 1 file changed, 1 insertion(+) 值得注意的是之前 hotfix 分支的修改内容尚未包含到 iss53 中来。如果需要纳入此次修补，可以用 git merge master 把 master 分支合并到 iss53；或者等 iss53 完成之后，再将 iss53 分支中的更新并入 master。 分支的合并在问题 #53 相关的工作完成之后，可以合并回 master 分支。实际操作同前面合并 hotfix 分支差不多，只需回到 master 分支，运行 git merge 命令指定要合并进来的分支： 123456$ git checkout master$ git merge iss53Auto-merging READMEMerge made by the 'recursive' strategy. README | 1 + 1 file changed, 1 insertion(+) 请注意，这次合并操作的底层实现，并不同于之前 hotfix 的并入方式。因为这次你的开发历史是从更早的地方开始分叉的。由于当前 master 分支所指向的提交对象（C4）并不是 iss53 分支的直接祖先，Git 不得不进行一些额外处理。就此例而言，Git 会用两个分支的末端（C4 和 C5）以及它们的共同祖先（C2）进行一次简单的三方合并计算。图 16 用红框标出了 Git 用于合并的三个提交对象： 这次，Git 没有简单地把分支指针右移，而是对三方合并后的结果重新做一个新的快照，并自动创建一个指向它的提交对象（C6）（见图 17）。这个提交对象比较特殊，它有两个祖先（C4 和 C5）。 值得一提的是 Git 可以自己裁决哪个共同祖先才是最佳合并基础；这和 CVS 或 Subversion（1.5 以后的版本）不同，它们需要开发者手工指定合并基础。所以此特性让 Git 的合并操作比其他系统都要简单不少。 既然之前的工作成果已经合并到 master 了，那么 iss53 也就没用了。你可以就此删除它，并在问题追踪系统里关闭该问题。 1$ git branch -d iss53 遇到冲突时的分支合并有时候合并操作并不会如此顺利。如果在不同的分支中都修改了同一个文件的同一部分，Git 就无法干净地把两者合到一起（逻辑上说，这种问题只能由人来裁决。）。如果你在解决问题 #53 的过程中修改了 hotfix 中修改的部分，将得到类似下面的结果： 1234$ git merge iss53Auto-merging index.htmlCONFLICT (content): Merge conflict in index.htmlAutomatic merge failed; fix conflicts and then commit the result. Git 作了合并，但没有提交，它会停下来等你解决冲突。要看看哪些文件在合并时发生冲突，可以用 git status查阅： 1234567891011$ git statusOn branch masterYou have unmerged paths. (fix conflicts and run "git commit")Unmerged paths: (use "git add &lt;file&gt;..." to mark resolution) both modified: index.htmlno changes added to commit (use "git add" and/or "git commit -a") 任何包含未解决冲突的文件都会以未合并（unmerged）的状态列出。Git 会在有冲突的文件里加入标准的冲突解决标记，可以通过它们来手工定位并解决这些冲突。可以看到此文件包含类似下面这样的部分： 1234567&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD&lt;div id="footer"&gt;contact : email.support@github.com&lt;/div&gt;=======&lt;div id="footer"&gt; please contact us at support@github.com&lt;/div&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53 可以看到 ======= 隔开的上半部分，是 HEAD（即 master 分支，在运行 merge 命令时所切换到的分支）中的内容，下半部分是在 iss53 分支中的内容。解决冲突的办法无非是二者选其一或者由你亲自整合到一起。比如你可以通过把这段内容替换为下面这样来解决： 123&lt;div id="footer"&gt;please contact us at email.support@github.com&lt;/div&gt; 这个解决方案各采纳了两个分支中的一部分内容，而且我还删除了 &lt;&lt;&lt;&lt;&lt;&lt;&lt;，======= 和 &gt;&gt;&gt;&gt;&gt;&gt;&gt; 这些行。在解决了所有文件里的所有冲突后，运行 git add 将把它们标记为已解决状态（注：实际上就是来一次快照保存到暂存区域。）。因为一旦暂存，就表示冲突已经解决。如果你想用一个有图形界面的工具来解决这些问题，不妨运行 git mergetool，它会调用一个可视化的合并工具并引导你解决所有冲突： 12345678910111213$ git mergetoolThis message is displayed because 'merge.tool' is not configured.See 'git mergetool --tool-help' or 'git help config' for more details.'git mergetool' will now attempt to use one of the following tools:opendiff kdiff3 tkdiff xxdiff meld tortoisemerge gvimdiff diffuse diffmerge ecmerge p4merge araxis bc3 codecompare vimdiff emergeMerging:index.htmlNormal merge conflict for 'index.html': &#123;local&#125;: modified file &#123;remote&#125;: modified fileHit return to start merge resolution tool (opendiff): 如果不想用默认的合并工具（Git 为我默认选择了 opendiff，因为我在 Mac 上运行了该命令），你可以在上方”merge tool candidates”里找到可用的合并工具列表，输入你想用的工具名。 退出合并工具以后，Git 会询问你合并是否成功。如果回答是，它会为你把相关文件暂存起来，以表明状态为已解决。 再运行一次 git status 来确认所有冲突都已解决： 123456$ git statusOn branch masterChanges to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) modified: index.html 如果觉得满意了，并且确认所有冲突都已解决，也就是进入了暂存区，就可以用 git commit 来完成这次合并提交。提交的记录差不多是这样： 12345678910Merge branch 'iss53'Conflicts: index.html## It looks like you may be committing a merge.# If this is not correct, please remove the file# .git/MERGE_HEAD# and try again.# 如果想给将来看这次合并的人一些方便，可以修改该信息，提供更多合并细节。比如你都作了哪些改动，以及这么做的原因。有时候裁决冲突的理由并不直接或明显，有必要略加注解。 分支的管理到目前为止，你已经学会了如何创建、合并和删除分支。除此之外，我们还需要学习如何管理分支，在日后的常规工作中会经常用到下面介绍的管理命令。 git branch 命令不仅仅能创建和删除分支，如果不加任何参数，它会给出当前所有分支的清单： 1234$ git branch iss53* master testing 注意看 master 分支前的 * 字符：它表示当前所在的分支。也就是说，如果现在提交更新，master 分支将随着开发进度前移。若要查看各个分支最后一个提交对象的信息，运行 git branch -v： 1234$ git branch -v iss53 93b412c fix javascript issue* master 7a98805 Merge branch 'iss53' testing 782fd34 add scott to the author list in the readmes 要从该清单中筛选出你已经（或尚未）与当前分支合并的分支，可以用 --merged 和 --no-merged 选项（Git 1.5.6 以上版本）。比如用 git branch --merged 查看哪些分支已被并入当前分支（也就是说哪些分支是当前分支的直接上游。）： 123$ git branch --merged iss53* master 之前我们已经合并了 iss53，所以在这里会看到它。一般来说，列表中没有 * 的分支通常都可以用 git branch -d 来删掉。原因很简单，既然已经把它们所包含的工作整合到了其他分支，删掉也不会损失什么。 另外可以用 git branch --no-merged 查看尚未合并的工作： 12$ git branch --no-merged testing 它会显示还未合并进来的分支。由于这些分支中还包含着尚未合并进来的工作成果，所以简单地用 git branch -d 删除该分支会提示错误，因为那样做会丢失数据： 123$ git branch -d testingerror: The branch 'testing' is not fully merged.If you are sure you want to delete it, run 'git branch -D testing'. 不过，如果你确实想要删除该分支上的改动，可以用大写的删除选项 -D 强制执行，就像上面提示信息中给出的那样。 利用分支进行开发的工作流程现在我们已经学会了新建分支和合并分支，可以（或应该）用它来做点什么呢？我会介绍一些利用分支进行开发的工作流程。而正是由于分支管理的便捷，才衍生出了这类典型的工作模式，你可以根据项目的实际情况选择一种用用看。 长期分支由于 Git 使用简单的三方合并，所以就算在较长一段时间内，反复多次把某个分支合并到另一分支，也不是什么难事。也就是说，你可以同时拥有多个开放的分支，每个分支用于完成特定的任务，随着开发的推进，你可以随时把某个特性分支的成果并到其他分支中。 许多使用 Git 的开发者都喜欢用这种方式来开展工作，比如仅在 master 分支中保留完全稳定的代码，即已经发布或即将发布的代码。与此同时，他们还有一个名为 develop 或 next 的平行分支，专门用于后续的开发，或仅用于稳定性测试 — 当然并不是说一定要绝对稳定，不过一旦进入某种稳定状态，便可以把它合并到 master 里。这样，在确保这些已完成的特性分支（短期分支，比如之前的 iss53 分支）能够通过所有测试，并且不会引入更多错误之后，就可以并到主干分支中，等待下一次的发布。 本质上我们刚才谈论的，是随着提交对象不断右移的指针。稳定分支的指针总是在提交历史中落后一大截，而前沿分支总是比较靠前（见图 18）。 或者把它们想象成工作流水线，或许更好理解一些，经过测试的提交对象集合被遴选到更稳定的流水线（见图 19）。 你可以用这招维护不同层次的稳定性。某些大项目还会有个 proposed（建议）或 pu（proposed updates，建议更新）分支，它包含着那些可能还没有成熟到进入 next 或 master 的内容。这么做的目的是拥有不同层次的稳定性：当这些分支进入到更稳定的水平时，再把它们合并到更高层分支中去。再次说明下，使用多个长期分支的做法并非必需，不过一般来说，对于特大型项目或特复杂的项目，这么做确实更容易管理。 特性分支在任何规模的项目中都可以使用特性（Topic）分支。一个特性分支是指一个短期的，用来实现单一特性或与其相关工作的分支。可能你在以前的版本控制系统里从未做过类似这样的事情，因为通常创建与合并分支消耗太大。然而在 Git 中，一天之内建立、使用、合并再删除多个分支是常见的事。 我们在上节的例子里已经见过这种用法了。我们创建了 iss53 和 hotfix 这两个特性分支，在提交了若干更新后，把它们合并到主干分支，然后删除。该技术允许你迅速且完全的进行语境切换 — 因为你的工作分散在不同的流水线里，每个分支里的改变都和它的目标特性相关，浏览代码之类的事情因而变得更简单了。你可以把作出的改变保持在特性分支中几分钟，几天甚至几个月，等它们成熟以后再合并，而不用在乎它们建立的顺序或者进度。 现在我们来看一个实际的例子。请看图 20，由下往上，起先我们在 master 工作到 C1，然后开始一个新分支 iss91 尝试修复 91 号缺陷，提交到 C6 的时候，又冒出一个解决该问题的新办法，于是从之前 C4 的地方又分出一个分支 iss91v2，干到 C8 的时候，又回到主干 master 中提交了 C9 和 C10，再回到 iss91v2 继续工作，提交 C11，接着，又冒出个不太确定的想法，从 master 的最新提交 C10 处开了个新的分支 dumbidea 做些试验。 现在，假定两件事情：我们最终决定使用第二个解决方案，即 iss91v2 中的办法；另外，我们把 dumbidea 分支拿给同事们看了以后，发现它竟然是个天才之作。所以接下来，我们准备抛弃原来的 iss91 分支（实际上会丢弃 C5 和 C6），直接在主干中并入另外两个分支。最终的提交历史将变成图 21 这样： 请务必牢记这些分支全部都是本地分支，这一点很重要。当你在使用分支及合并的时候，一切都是在你自己的 Git 仓库中进行的 — 完全不涉及与服务器的交互。 远程分支远程分支（remote branch）是对远程仓库中的分支的索引。它们是一些无法移动的本地分支；只有在 Git 进行网络交互时才会更新。远程分支就像是书签，提醒着你上次连接远程仓库时上面各分支的位置。 我们用 (远程仓库名)/(分支名) 这样的形式表示远程分支。比如我们想看看上次同 origin 仓库通讯时 master 分支的样子，就应该查看 origin/master 分支。如果你和同伴一起修复某个问题，但他们先推送了一个 iss53 分支到远程仓库，虽然你可能也有一个本地的 iss53 分支，但指向服务器上最新更新的却应该是 origin/iss53 分支。 可能有点乱，我们不妨举例说明。假设你们团队有个地址为 git.ourcompany.com 的 Git 服务器。如果你从这里克隆，Git 会自动为你将此远程仓库命名为 origin，并下载其中所有的数据，建立一个指向它的 master 分支的指针，在本地命名为 origin/master，但你无法在本地更改其数据。接着，Git 建立一个属于你自己的本地 master分支，始于 origin 上 master 分支相同的位置，你可以就此开始工作（见图 22）： 如果你在本地 master 分支做了些改动，与此同时，其他人向 git.ourcompany.com 推送了他们的更新，那么服务器上的 master 分支就会向前推进，而与此同时，你在本地的提交历史正朝向不同方向发展。不过只要你不和服务器通讯，你的 origin/master 指针仍然保持原位不会移动（见图 23）。 可以运行 git fetch origin 来同步远程服务器上的数据到本地。该命令首先找到 origin 是哪个服务器（本例为 git.ourcompany.com），从上面获取你尚未拥有的数据，更新你本地的数据库，然后把 origin/master 的指针移到它最新的位置上（见图 24）。 为了演示拥有多个远程分支（在不同的远程服务器上）的项目是如何工作的，我们假设你还有另一个仅供你的敏捷开发小组使用的内部服务器 git.team1.ourcompany.com。可以用 git remote add 命令把它加为当前项目的远程分支之一。我们把它命名为 teamone，以便代替完整的 Git URL 以方便使用（见图 25）。 现在你可以用 git fetch teamone 来获取小组服务器上你还没有的数据了。由于当前该服务器上的内容是你 origin 服务器上的子集，Git 不会下载任何数据，而只是简单地创建一个名为 teamone/master 的远程分支，指向 teamone 服务器上 master 分支所在的提交对象 31b8e（见图 26）。 推送本地分支要想和其他人分享某个本地分支，你需要把它推送到一个你拥有写权限的远程仓库。你创建的本地分支不会因为你的写入操作而被自动同步到你引入的远程服务器上，你需要明确地执行推送分支的操作。换句话说，对于无意分享的分支，你尽管保留为私人分支好了，而只推送那些协同工作要用到的特性分支。 如果你有个叫 serverfix 的分支需要和他人一起开发，可以运行 git push (远程仓库名) (分支名)： 1234567$ git push origin serverfixCounting objects: 20, done.Compressing objects: 100% (14/14), done.Writing objects: 100% (15/15), 1.74 KiB, done.Total 15 (delta 5), reused 0 (delta 0)To git@github.com:schacon/simplegit.git * [new branch] serverfix -&gt; serverfix 这里其实走了一点捷径。Git 自动把 serverfix 分支名扩展为 refs/heads/serverfix:refs/heads/serverfix，意为“取出我在本地的 serverfix 分支，推送到远程仓库的 serverfix 分支中去”。以后有机会了再介绍 refs/heads/ 的细节，不过一般使用的时候都可以省略它。也可以运行 git push origin serverfix:serverfix 来实现相同的效果，它的意思是“上传我本地的 serverfix 分支到远程仓库中去，仍旧称它为 serverfix 分支”。通过此语法，你可以把本地分支推送到某个命名不同的远程分支：若想把远程分支叫作 awesomebranch，可以用 git push origin serverfix:awesomebranch 来推送数据。 接下来，当你的协作者再次从服务器上获取数据时，他们将得到一个新的远程分支 origin/serverfix，并指向服务器上 serverfix 所指向的版本： 1234567$ git fetch originremote: Counting objects: 20, done.remote: Compressing objects: 100% (14/14), done.remote: Total 15 (delta 5), reused 0 (delta 0)Unpacking objects: 100% (15/15), done.From git@github.com:schacon/simplegit * [new branch] serverfix -&gt; origin/serverfix 值得注意的是，在 fetch 操作下载好新的远程分支之后，你仍然无法在本地编辑该远程仓库中的分支。换句话说，在本例中，你不会有一个新的 serverfix 分支，有的只是一个你无法移动的 origin/serverfix 指针。 如果要把该远程分支的内容合并到当前分支，可以运行 git merge origin/serverfix。如果想要一份自己的 serverfix 来开发，可以在远程分支的基础上分化出一个新的分支来： 123$ git checkout -b serverfix origin/serverfixBranch serverfix set up to track remote branch serverfix from origin.Switched to a new branch 'serverfix' 这会切换到新建的 serverfix 本地分支，其内容同远程分支 origin/serverfix 一致，这样你就可以在里面继续开发了。 跟踪远程分支从远程分支 checkout 出来的本地分支，称为 跟踪分支 (tracking branch)。跟踪分支是一种和某个远程分支有直接联系的本地分支。在跟踪分支里输入 git push，Git 会自行推断应该向哪个服务器的哪个分支推送数据。同样，在这些分支里运行 git pull 会获取所有远程索引，并把它们的数据都合并到本地分支中来。 在克隆仓库时，Git 通常会自动创建一个名为 master 的分支来跟踪 origin/master。这正是 git push 和 git pull 一开始就能正常工作的原因。当然，你可以随心所欲地设定为其它跟踪分支，比如 origin 上除了 master之外的其它分支。刚才我们已经看到了这样的一个例子：git checkout -b [分支名] [远程名]/[分支名]。如果你有 1.6.2 以上版本的 Git，还可以用 --track 选项简化： 123$ git checkout --track origin/serverfixBranch serverfix set up to track remote branch serverfix from origin.Switched to a new branch 'serverfix' 要为本地分支设定不同于远程分支的名字，只需在第一个版本的命令里换个名字： 123$ git checkout -b sf origin/serverfixBranch sf set up to track remote branch serverfix from origin.Switched to a new branch 'sf' 现在你的本地分支 sf 会自动将推送和抓取数据的位置定位到 origin/serverfix 了。 删除远程分支如果不再需要某个远程分支了，比如搞定了某个特性并把它合并进了远程的 master 分支（或任何其他存放稳定代码的分支），可以用这个非常无厘头的语法来删除它：git push [远程名] :[分支名]。如果想在服务器上删除 serverfix 分支，运行下面的命令： 123$ git push origin :serverfixTo git@github.com:schacon/simplegit.git - [deleted] serverfix 咚！服务器上的分支没了。你最好特别留心这一页，因为你一定会用到那个命令，而且你很可能会忘掉它的语法。有种方便记忆这条命令的方法：记住我们不久前见过的 git push [远程名] [本地分支]:[远程分支] 语法，如果省略 [本地分支]，那就等于是在说“在这里提取空白然后把它变成[远程分支]”。 分支的衍合把一个分支中的修改整合到另一个分支的办法有两种：merge 和 rebase（rebase 的翻译暂定为“衍合”，大家知道就可以了）。 基本的衍合操作请回顾之前有关合并的一节（见图 27），你会看到开发进程分叉到两个不同分支，又各自提交了更新。 之前介绍过，最容易的整合分支的方法是 merge 命令，它会把两个分支最新的快照（C3 和 C4）以及二者最新的共同祖先（C2）进行三方合并，合并的结果是产生一个新的提交对象（C5）。如图 28 所示： 其实，还有另外一个选择：你可以把在 C3 里产生的变化补丁在 C4 的基础上重新打一遍。在 Git 里，这种操作叫做衍合（rebase）。有了 rebase 命令，就可以把在一个分支里提交的改变移到另一个分支里重放一遍。 在上面这个例子中，运行： 1234$ git checkout experiment$ git rebase masterFirst, rewinding head to replay your work on top of it...Applying: added staged command 它的原理是回到两个分支最近的共同祖先，根据当前分支（也就是要进行衍合的分支 experiment）后续的历次提交对象（这里只有一个 C3），生成一系列文件补丁，然后以基底分支（也就是主干分支 master）最后一个提交对象（C4）为新的出发点，逐个应用之前准备好的补丁文件，最后会生成一个新的合并提交对象（C3’），从而改写 experiment 的提交历史，使它成为 master 分支的直接下游，如图 29 所示： 现在回到 master 分支，进行一次快进合并（见图 30）： 现在的 C3’ 对应的快照，其实和普通的三方合并，即上个例子中的 C5 对应的快照内容一模一样了。虽然最后整合得到的结果没有任何区别，但衍合能产生一个更为整洁的提交历史。如果视察一个衍合过的分支的历史记录，看起来会更清楚：仿佛所有修改都是在一根线上先后进行的，尽管实际上它们原本是同时并行发生的。 一般我们使用衍合的目的，是想要得到一个能在远程分支上干净应用的补丁 — 比如某些项目你不是维护者，但想帮点忙的话，最好用衍合：先在自己的一个分支里进行开发，当准备向主项目提交补丁的时候，根据最新的 origin/master 进行一次衍合操作然后再提交，这样维护者就不需要做任何整合工作（实际上是把解决分支补丁同最新主干代码之间冲突的责任，化转为由提交补丁的人来解决），只需根据你提供的仓库地址作一次快进合并，或者直接采纳你提交的补丁。 请注意，合并结果中最后一次提交所指向的快照，无论是通过衍合，还是三方合并，都会得到相同的快照内容，只不过提交历史不同罢了。衍合是按照每行的修改次序重演一遍修改，而合并是把最终结果合在一起。 有趣的衍合衍合也可以放到其他分支进行，并不一定非得根据分化之前的分支。以图 31 的历史为例，我们为了给服务器端代码添加一些功能而创建了特性分支 server，然后提交 C3 和 C4。然后又从 C3 的地方再增加一个 client 分支来对客户端代码进行一些相应修改，所以提交了 C8 和 C9。最后，又回到 server 分支提交了 C10。 假设在接下来的一次软件发布中，我们决定先把客户端的修改并到主线中，而暂缓并入服务端软件的修改（因为还需要进一步测试）。这个时候，我们就可以把基于 client 分支而非 server 分支的改变（即 C8 和 C9），跳过 server 直接放到 master 分支中重演一遍，但这需要用 git rebase 的 --onto 选项指定新的基底分支 master： 1$ git rebase --onto master server client 这好比在说：“取出 client 分支，找出 client 分支和 server 分支的共同祖先之后的变化，然后把它们在 master 上重演一遍”。是不是有点复杂？不过它的结果如图 32 所示，非常酷（虽然 client 里的 C8, C9 在 C3 之后，但这仅表明时间上的先后，而非在 C3 修改的基础上进一步改动，因为 server 和 client 这两个分支对应的代码应该是两套文件，虽然这么说不是很严格，但应理解为在 C3 时间点之后，对另外的文件所做的 C8，C9 修改，放到主干重演。）： 现在可以快进 master 分支了（见图 33）： 12$ git checkout master$ git merge client 现在我们决定把 server 分支的变化也包含进来。我们可以直接把 server 分支衍合到 master，而不用手工切换到 server 分支后再执行衍合操作 — git rebase [主分支] [特性分支] 命令会先取出特性分支 server，然后在主分支 master 上重演： 1$ git rebase master server 于是，server 的进度应用到 master 的基础上，如图 34 所示： 然后就可以快进主干分支 master 了： 12$ git checkout master$ git merge server 现在 client 和 server 分支的变化都已经集成到主干分支来了，可以删掉它们了。最终我们的提交历史会变成图 35 的样子： 12$ git branch -d client$ git branch -d server 衍合的风险呃，奇妙的衍合也并非完美无缺，要用它得遵守一条准则： 一旦分支中的提交对象发布到公共仓库，就千万不要对该分支进行衍合操作。 如果你遵循这条金科玉律，就不会出差错。否则，人民群众会仇恨你，你的朋友和家人也会嘲笑你，唾弃你。 在进行衍合的时候，实际上抛弃了一些现存的提交对象而创造了一些类似但不同的新的提交对象。如果你把原来分支中的提交对象发布出去，并且其他人更新下载后在其基础上开展工作，而稍后你又用 git rebase 抛弃这些提交对象，把新的重演后的提交对象发布出去的话，你的合作者就不得不重新合并他们的工作，这样当你再次从他们那里获取内容时，提交历史就会变得一团糟。 下面我们用一个实际例子来说明为什么公开的衍合会带来问题。假设你从一个中央服务器克隆然后在它的基础上搞了一些开发，提交历史类似图 36 所示： 现在，某人在 C1 的基础上做了些改变，并合并他自己的分支得到结果 C6，推送到中央服务器。当你抓取并合并这些数据到你本地的开发分支中后，会得到合并结果 C7，历史提交会变成图 37 这样： 接下来，那个推送 C6 上来的人决定用衍合取代之前的合并操作；继而又用 git push --force 覆盖了服务器上的历史，得到 C4’。而之后当你再从服务器上下载最新提交后，会得到： 下载更新后需要合并，但此时衍合产生的提交对象 C4’ 的 SHA-1 校验值和之前 C4 完全不同，所以 Git 会把它们当作新的提交对象处理，而实际上此刻你的提交历史 C7 中早已经包含了 C4 的修改内容，于是合并操作会把 C7 和 C4’ 合并为 C8（见图 39）: C8 这一步的合并是迟早会发生的，因为只有这样你才能和其他协作者提交的内容保持同步。而在 C8 之后，你的提交历史里就会同时包含 C4 和 C4’，两者有着不同的 SHA-1 校验值，如果用 git log 查看历史，会看到两个提交拥有相同的作者日期与说明，令人费解。而更糟的是，当你把这样的历史推送到服务器后，会再次把这些衍合后的提交引入到中央服务器，进一步困扰其他人（这个例子中，出问题的责任方是那个发布了 C6 后又用衍合发布 C4’ 的人，其他人会因此反馈双重历史到共享主干，从而混淆大家的视听）。 如果把衍合当成一种在推送之前清理提交历史的手段，而且仅仅衍合那些尚未公开的提交对象，就没问题。如果衍合那些已经公开的提交对象，并且已经有人基于这些提交对象开展了后续开发工作的话，就会出现叫人沮丧的麻烦。 本文参考《Pro Git》 一书]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库学习笔记]]></title>
    <url>%2Fblog%2F2015%2F06%2F22%2F2015-06-22-MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[数据库1.事务 说明 ：事务是恢复和并发控制的基本单位，是用户定义的一个操作序列。这些操作要么都做，要么都不做，是一个不可分割的工作单位。通过事务，逻辑相关的一组操作绑定在一起，以便服务器保持数据的完整性。 事务的特性：ACID A:原子性(Atomicity)事务是数据库的逻辑工作单位，事务中包括的诸操作要么全做，要么全不做。C:一致性(Consistency)事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的。 I:隔离性(Isolation)一个事务的执行不能被其他事务干扰。 D:持续性/永久性(Durability)一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。 2.多个条件where 1=1 模糊查询%%通配符 3.手写一段sql语句，具体内容忘了，好像和limit有关 4.存储引擎的区别 InnoDB 是支持事务的存储引擎，其设计目标是面向在线事务处理的应用，其特点是行锁设计，支持外键，支持类似Oracle的非锁定读，默认读取操作不会产生锁。MySQL5.5以后是默认的存储引擎。还提供了插入缓存，二次写，预读等高性能和高可用的功能。 MyISAM 引擎不支持事务，表锁设计，支持全文索引，主要是面向OLAP数据库应用。 NDB是集群存储引擎，其数据全部放在内存中，因此主键查找的速度极快。 Memory将表中的数据存放在内存中，如果数据库重启或者发生奔溃，表中的数据都将消失。它使用于存储临时数据的临时表。默认采用哈希索引 5.sql注入原理 就是通过把SQL命令插入到Web 表单 提交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令 1）猜表名，列名等 2）后台身份验证绕过漏洞 验证绕过漏洞就是’or’=’or’后台绕过漏洞，利用的就是AND和OR的运算规则，从而造成后台脚本逻辑性错误. 防范： 1）永远不要信任用户的输入，要对用户的输入进行校验，可以通过正则表达式，或限制长度，对单引号和双”-“进行转换等。 2）永远不要使用动态拼装SQL，可以使用参数化的SQL或者直接使用存储过程进行数据查询存取。 3）永远不要使用管理员权限的数据库连接，为每个应用使用单独的权限有限的数据库连接。 4）不要把机密信息明文存放，请加密或者hash掉密码和敏感的信息。 5）应用的异常信息应该给出尽可能少的提示，最好使用自定义的错误信息对原始错误信息进行包装，把异常信息存放在独立的表中。 数据库索引索引是一个单独存储在磁盘上的数据库结构，它们包含着对数据表里所有记录的引用指针，使用索引可以提高数据库特定数据的查询速度.索引时在存储引擎中实现的，因此每种存储引擎的索引不一定完全相同,并且每种存储引擎也不一定支持所有索引类型． 索引的存储类型有两种：BTREE和HASH,具体和表的存储引擎有关．MyISAM和InnoDB存储引擎只支持BTREE;MEMORY/HEAD存储索引可以支持HASH和BTREE索引． 索引的优点： 1.通过创建唯一索引，可以保证数据库表中每行数据的唯一性. 2.可以加快数据的查询速度． 3.在实现数据的参考完整性方面，可以加速表和表之间的连接． 索引的缺点： 1.创建索引和维护索引要耗费时间，并且随着数据量的增加耗费时间也增加． 2.索引需要占空间内存． 3.在对表中数据进行增加,删除和修改的时候，索引也需要动态维护，这样降低了数据维护速度． 索引分类： 1.普通索引 2.唯一索引 数据库锁机制数据库锁定机制简单来说就是数据库为了保证数据的一致性而使各种共享资源在被并发访问，访问变得有序所设计的一种规则。MySQL各存储引擎使用了三种类型（级别）的锁定机制：行级锁定，页级锁定和表级锁定。 表级锁定（table-level）：表级别的锁定是MySQL各存储引擎中最大颗粒度的锁定机制。该锁定机制最大的特点是实现逻辑非常简单，带来的系统负面影响最小。所以获取锁和释放锁的速度很快。由于表级锁一次会将整个表锁定，所以可以很好的避免困扰我们的死锁问题。当然，锁定颗粒度大所带来最大的负面影响就是出现锁定资源争用的概率也会最高，致使并大度大打折扣。表级锁分为读锁和写锁。 页级锁定（page-level）：页级锁定的特点是锁定颗粒度介于行级锁定与表级锁之间，所以获取锁定所需要的资源开销，以及所能提供的并发处理能力也同样是介于上面二者之间。另外，页级锁定和行级锁定一样，会发生死锁。 行级锁定（row-level）：行级锁定最大的特点就是锁定对象的颗粒度很小，也是目前各大数据库管理软件所实现的锁定颗粒度最小的。由于锁定颗粒度很小，所以发生锁定资源争用的概率也最小，能够给予应用程序尽可能大的并发处理能力而提高一些需要高并发应用系统的整体性能。虽然能够在并发处理能力上面有较大的优势，但是行级锁定也因此带来了不少弊端。由于锁定资源的颗粒度很小，所以每次获取锁和释放锁需要做的事情也更多，带来的消耗自然也就更大了。此外，行级锁定也最容易发生死锁。InnoDB的行级锁同样分为两种，共享锁和排他锁，同样InnoDB也引入了意向锁（表级锁）的概念，所以也就有了意向共享锁和意向排他锁，所以InnoDB实际上有四种锁，即共享锁（S）、排他锁（X）、意向共享锁（IS）、意向排他锁（IX）； 在MySQL数据库中，使用表级锁定的主要是MyISAM，Memory，CSV等一些非事务性存储引擎，而使用行级锁定的主要是Innodb存储引擎和NDBCluster存储引擎，页级锁定主要是BerkeleyDB存储引擎的锁定方式。 而意向锁的作用就是当一个事务在需要获取资源锁定的时候，如果遇到自己需要的资源已经被排他锁占用的时候，该事务可以需要锁定行的表上面添加一个合适的意向锁。如果自己需要一个共享锁，那么就在表上面添加一个意向共享锁。而如果自己需要的是某行（或者某些行）上面添加一个排他锁的话，则先在表上面添加一个意向排他锁。意向共享锁可以同时并存多个，但是意向排他锁同时只能有一个存在。 | | 共享锁（S）| 排他锁（X）| 意向共享锁（IS）| 意向排他锁（IX）|共享锁（S） | 兼容 | 冲突 | 兼容 |冲突排他锁（X） | 冲突 | 冲突 | 冲突 |冲突意向共享锁（IS） | 兼容 | 冲突 | 兼容 |兼容意向排他锁（IX） | 冲突 | 冲突 | 兼容 |兼容 参考地址：http://www.cnblogs.com/ggjucheng/archive/2012/11/14/2770445.html MyISAM 表锁优化建议： 1、缩短锁定时间 2、分离能并行的操作 3、合理利用读写优先级 乐观锁，悲观锁悲观锁:它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制。悲观的缺陷是不论是页锁还是行锁，加锁的时间可能会很长，这样可能会长时间的限制其他用户的访问，也就是说悲观锁的并发访问性不好。 乐观锁（ Optimistic Locking ） :相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则则拒绝更新并返回用户错误的信息，让用户决定如何去做。乐观锁由程序实现，不会存在死锁问题。它适用的场景也相对乐观。但乐观锁不能解决脏读的问题 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。[1] 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。[1] 乐观锁不能解决脏读的问题。 事务隔离机制为什么？ 1.更新丢失 两个事务都同时更新一行数据，一个事务对数据的更新把另一个事务对数据的更新覆盖了。这是因为系统没有执行任何的锁操作，因此并发事务并没有被隔离开来。 2.脏读 一个事务读取到了另一个事务未提交的数据操作结果。这是相当危险的，因为很可能所有的操作都被回滚。 3.不可重复读 不可重复读（Non-repeatable Reads）：一个事务对同一行数据重复读取两次，但是却得到了不同的结果。 4、幻读：幻读与不可重复读类似。它发生在一个事务(T1)读取了几行数据，接着另一个并发事务(T2)插入了一些数据时。在随后的查询中，第一个事务(T1)就会发现多了一些原本不存在的记录 事物隔离级别： 未提交读(READ UNCOMMITTED):允许脏读 不允许更新丢失 提交读(READ COMMITTED):允许不可重复读 但不允许脏读 可重复读(REPEATABLE READ):禁止不可重复读和脏读 但可能出现幻读 可串行化(SERIZLIZABLE):它通过强制事务串行执行，不能并发的执行，避免了前面说的幻读的问题。 总结：隔离级别越高，越能保证事务的完整性和一致性，但对并发性能的影响也就越大。对于多数应用可以把隔离级别设置为ReadCommited，也就是授权读取，能够避免脏读，而保持较好的并发性能。 1脏读 不可重复读 幻读可能性 加锁读 未提交读 YES YES YES NO提交读 NO YES YES NO可重复读 NO NO YES NO可串行化 NO NO NO YES 数据库事务属性事务是由一组SQL语句组成的逻辑处理单元，事务具有以下4个属性，通常简称为事务的ACID属性。原子性（Atomicity）：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。一致性（Consistent）：在事务开始和完成时，数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构（如B树索引或双向链表）也都必须是正确的。隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的，反之亦然。持久性（Durable）：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 数据库事务的几种粒度数据库的索引是如何实现的MyISAM索引实现MyISAM索引使用了B+Tree作为索引结构，叶子结点的data域存放的是数据记录的地址。MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。主索引和辅助索引的存储结构没有任何区别。 InnoDB索引实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这种索引叫做聚集索引。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据。第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。 Memory索引实现Memory索引适用于需要快速访问数据的场景，显示支持哈希索引。内部基于哈希表数据结构实现，只包含哈希值和行指针，对于每一行数据，存储引擎都会对所有的引擎列计算一个哈希码，在哈希表对应位置存放该行数据的指针或地址。为了解决多个hash冲突问题，哈希索引采用了链地址法来解决冲突问题。所以采用链表数组作为存储结构。这种索引结构十分紧凑，且具有很快的查询速度。但也存在一些问题，1.哈希表数据不是按照索引顺序存储的，所以无法用于排序。2.只能支持等值比较查询。3.存在冲突情况下查询速度变慢。https://msdn.microsoft.com/zh-cn/library/dn133190.aspx 数据库连接池原理背景 传统的数据库连接方式是，用户每次请求都要向数据库获取连接，而数据库连接的创建和关闭需要一定的开销。频繁的建立、关闭数据库，会极大的降低系统的性能，增大系统的开销，甚至成为系统的瓶颈。另外使用这种传统的模式，还必须管理数据库的每一个连接，以确保他们能正确关闭，如果出现程序异常而导致某些连接未能关闭。同时无节制的创建连接极易导致数据库服务器内存溢出。 原理 数据库连接池的基本思想就是为数据库连接建立一个“缓冲池”。预先在缓冲池中放入一定数量的连接，当需要建立数据库连接时，只需从“缓冲池”中取出一个，使用完毕之后再放回去。以及一套连接使用、分配、管理策略，使得该连接池中的连接可以得到高效、安全的复用，避免了数据库连接频繁建立、关闭的开销。我们可以通过设定连接池最大连接数来防止系统无尽的与数据库连接。 开源java连接池 现在很多Web服务器(Weblogic, WebSphere, Tomcat)都提供了DataSoruce的实现，即连接池的实现。通常我们把DataSource的实现，按其英文含义称之为数据源，数据源中都包含了数据库连接池的实现。 1.C3P0 :是一个开放源代码的JDBC连接池，它在lib目录中与Hibernate一起发布,包括了实现jdbc3和jdbc2扩展规范说明的Connection 和Statement 池的DataSources 对象。参考网站: http://sourceforge.net/projects/c30/ 2.Proxool :是一个Java SQL Driver驱动程序，提供了对你选择的其它类型的驱动程序的连接池封装。可以非常简单的移植到现存的代码中。完全可配置。快速，成熟，健壮。可以透明地为你现存的JDBC驱动程序增加连接池功能。 参考网站: http://proxool.sourceforge.net 3.Jakarta DBCP :是一个依赖Jakarta commons-pool对象池机制的数据库连接池.DBCP可以直接的在应用程序用使用。参考网站: http://commons.apache.org/proper/commons-dbcp/ 原理: http://www.uml.org.cn/sjjm/201004153.asp实现: http://www.cnblogs.com/lihuiyy/archive/2012/02/14/2351768.html 连接池使用什么数据结构实现（链表） mysql有那些存储引擎，分别有什么特点join操作LEFT JOIN 关键字会从左表 (Persons) 那里返回所有的行，即使在右表 (Orders) 中没有匹配的行。 RIGHT JOIN 关键字会从右表 (Orders) 那里返回所有的行，即使在左表 (Persons) 中没有匹配的行。 FULL JOIN 关键字会从左表 (Persons) 和右表 (Orders) 那里返回所有的行。如果 “Persons” 中的行在表 “Orders” 中没有匹配，或者如果 “Orders” 中的行在表 “Persons” 中没有匹配，这些行同样会列出。 INNER JOIN 关键字在表中存在至少一个匹配时返回行。如果 “Persons” 中的行在 “Orders” 中没有匹配，就不会列出这些行。 三大范式第一范式（无重复的列） 第二范式（属性完全依赖于主键） 定义：满足第一范式前提，当存在多个主键的时候，才会发生不符合第二范式的情况。比如有两个主键，不能存在这样的属性，它只依赖于其中一个主键，这就是不符合。通俗解释：任意一个字段都只依赖表中的同一个字段。 eg:比如不符合第二范式学生证 名称 学生证号 学生证办理时间 借书证名称 借书证号 借书证办理时间 改成2张表如下学生证表学生证 学生证号 学生证办理时间 借书证表借书证 借书证号 借书证办理时间 第三范式（属性不能传递依赖于主属性） 定义：满足第二范式前提，如果某一属性依赖于其他非主键属性，而其他非主键属性又依赖于主键，那么这个属性就是间接依赖于主键，这被称作传递依赖于主属性。 eg:爸爸资料表，不满足第三范式爸爸 儿子 女儿 女儿的小熊 女儿的海绵宝宝 改成 爸爸信息表：爸爸 儿子 女儿女儿信息表女儿 女儿的小熊 女儿的海绵宝宝]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL在Linux上修改root密码]]></title>
    <url>%2Fblog%2F2015%2F06%2F10%2F2015-06-10-MySQL%E4%BF%AE%E6%94%B9root%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[方法一：用set password命令首先，登陆mysql mysql -u root -p 然后执行set password命令set password for root@localhost = password(‘654321’); 上面例子，将root密码更改为654321 方法二：使用mysqladmin格式为：mysqladmin -u用户名 -p旧密码 password 新密码 mysqladmin -uroot -p123456 password “654321” 上面例子，将root密码由123456更改为654321 方法三：更改mysql的user表首先，登陆mysql 12345mysql -uroot -p# 然后操作mysql库的user表，进行updatemysql&gt; use mysql;mysql&gt; update user set password=password(&apos;654321&apos;) where user=&apos;root&apos; and host=&apos;localhost&apos;;mysql&gt; flush privileges; 方法四：忘记密码的情况下首先停止mysql服务 1、service mysqld stop 以跳过授权的方式启动mysql 2、mysqld_safe –skip-grant-tables &amp; 3、mysql -u root 操作mysql库的user表，进行update 12345mysql&gt; use mysql;mysql&gt; update user set password=password(&apos;654321&apos;) where user=&apos;root&apos; and host=&apos;localhost&apos;;mysql&gt; flush privileges;mysql&gt; quit# 重启mysql服务 重启mysql服务service mysqld restart 开启远程mysql连接步骤：1、登入mysql 2、use mysql命令 3、GRANT ALL PRIVILEGES ON . TO ‘root’@’%’ IDENTIFIED BY ‘root’ WITH GRANT OPTION; 4、flush privileges 5、查看select host,user from user]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>修改密码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL多表查询]]></title>
    <url>%2Fblog%2F2015%2F06%2F04%2F2015-06-04-MySQL%E5%A4%9A%E8%A1%A8%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[多表查询的分类1.连接查询交叉连接：cross join（很少用这个） 交叉连接：查询到的是两个表的笛卡尔积。 语法1：select * from 表1 cross join 表2; 语法2：select * from 表1,表2; 内连接：inner join（inner是可以省略的） 显试内连接：在SQL中显示的调用inner join关键字 语法：select * from 表1 inner join 表2 on 关联条件; 隐式内连接：在SQL中没有调用inner join关键字 语法：select * from 表1,表2 where 关联条件; 外连接：outer join（outer是可以省略的） 左外连接： 语法：select * from 表1 left outer join 表2 on 关联条件; 右外连接： 语法：select * from 表1 right outer join 表2 on 关联条件; 2.子查询子查询：一个查询语句条件需要依赖另一个查询语句的结果其实就是sql查询语句的嵌套使用 实例说明1.数据准备这里用班级、学生、课程、和学生选课情况四张表之间的关系来举例说明。 班级表： 有1/2/3/4四个班 学生表： 有9名学生，其中4班没有学生，有2名学生没有班级 课程表： 有语数英三门课程 学生选课表： 每个学生选了几门课，以及得的分数 四张表的外键关系： 2.交叉连接使用cross join关键字 1select * from class cross join student; 不使用cross join关键字（效果和上面的相同） 1select * from class,student; class表有4条数据，student表有9条数据，所以上面两种查询的结果都是36条数据（笛卡尔积） 3.内连接显式内连接12select * from class c inner join student s on c.class_id = s.class_id;-- inner可以省略不写 隐式内连接效果和显式内连接相同 1select * from class c,student s where c.class_id = s.class_id; class表有4条数据，student表有9条数据，但student表的学生没有4班的，还有2个学生没有班级。内连接是取两个表共有的部分，结果是7条数据 4.外连接左外连接12select * from class c left outer join student s on c.class_id = s.class_id;-- outer可以省略不写 左外连接会以左表（class表）为基准，查询出左表的全部内容，即便4班在学生表内没有人，也会将4班的结果列出来 右外连接12select * from class c right outer join student s on c.class_id = s.class_id;-- outer可以省略不写 右外连接会以右表（student表）为基准，查询出右表的全部内容，即便有两名学生没有班级，也会将这两名学生的结果列出来 5.子查询我们可以在where和having子句中使用子查询，将子查询得到的结果作为判断的条件 带in的子查询查询学生生日在1990年3月1日之后的班级的信息 1select * from class where class_id in (select class_id from student where student_birthday &gt; '1990-03-01'); in表示前面的值是否存在子查询结果集中。“in”也可以写成“= any”，”not in”也可以写成”&lt;&gt;all”，子查询结果中有null的时候，null不会用于比较。 带exists的子查询查询学生生日大于1991年1月1日，如果记录存在，前面的SQL语句就执行 1select * from class where exists (SELECT class_id FROM student WHERE student_birthday &gt; '1991-01-01'); exists可以理解为，将主查询的数据，放到子查询中做条件验证，根据验证结果（true 或 false）来决定主查询的数据结果是否得以保留。 带any的子查询查询，班级id大于，任意一个，学生所在班级id，的班级信息 1select * from class where class_id &gt; any (select class_id from student); any可以理解为or，或的意思，只要满足后面结果的其一即可。some和any的效果是相同的，所以也可以写成some。子查询结果中有null的时候，null不会用于比较 &lt;&gt;any是只要不等于其中的任意一个，就成立 带all的子查询查询，班级id大于，任何一个，学生所在班级id，的班级信息 1select * from class where class_id &gt; all (select class_id from student); all可以理解为and，且的意思，要满足后面结果的每一项才可。注意！！all在子查询结果中有null的时候，不会返回数据。我这里演示的时候，将学生表的那两个没有班级的学生，设置了班级id为3，才有的返回结果。&lt;&gt;all的同义词是not in，表示不等于集合中的所有值，这个很容易和&lt;&gt;any搞混，平时多留点心就好了。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>多表查询</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL基础使用规范]]></title>
    <url>%2Fblog%2F2015%2F05%2F26%2F2015-05-26-MySql%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[一、基础规范1、使用InnoDB存储引擎 支持事务、行级锁、并发性能更好、CPU及内存缓存页优化使得资源利用率更高 2、推荐使用utf8mb4字符集 无需转码，无乱码风险, 支持emoji表情以及部分不常见汉字 3、表、字段必须加注释 方便他人理解字段意思。4、不在数据库做计算 禁止使用存储过程、视图、触发器、Event。 在并发量大的情况下，这些功能很可能将数据库拖跨，业务逻辑放到服务层具备更好的扩展性，能够轻易实现“增机器就加性能” 5、禁止存储文件 文件存储在文件系统，数据库里存URI 6、控制单表数据量 单表记录控制在千万级 二、命名规范1、库名、表名、字段名：小写，下划线风格 非唯一索引名idx_xxx，唯一索引名uniq_xxx 2、表必须有主键，例如自增主键 a）主键递增，数据行写入可以提高插入性能 b）主键要选择较短的数据类型，Innodb引擎普通索引都会保存主键的值，较短的数据类型可以有效的减少索引的磁盘空间，提高索引的缓存效率 c）保证实体的完整性，唯一性 3、不要使用外键，如果有外键约束，用应用程序控制 外键会导致表与表之间耦合，update与delete操作都会涉及相关联的表，十分影响sql 的性能，甚至会造成死锁。高并发情况下容易造成数据库性能下降，大数据高并发业务场景数据库使用以性能优先 三、字段设计规范1、把字段定义为NOT NULL并且提供默认值 a）null的列使索引/索引统计/值比较都更加复杂，对MySQL来说更难优化 b）null 这种类型MySQL内部需要进行特殊处理，增加数据库处理记录的复杂性；同等条件下，表中有较多空字段的时候，数据库的处理性能会降低很多 c）null值需要更多的存储空间，无论是表还是索引中每行中的null的列都需要额外的空间来标识 d）对null 的处理时候，只能采用is null或is not null，而不能采用=、in、&lt;、&lt;&gt;、!=、not in这些操作符号。如：where name!=’zhangsan’，如果存在name为null值的记录，查询结果就不会包含name为null值的记录 2、不要使用TEXT、BLOB类型 会浪费更多的磁盘和内存空间，非必要的大量的大字段查询会淘汰掉热数据，导致内存命中率急剧降低，影响数据库性能,如果必须要使用则独立出来一张表，用主键来对应，避免影响其它字段索引效率 3、不要使用小数存储货币 建议使用整数，小数容易导致钱对不上 4、必须使用varchar存储手机号 手机号会去做数学运算么？ 5、为提高效率可以牺牲范式设计，冗余数据 a）不是频繁修改的字段 b）不是 varchar 超长字段，更不能是 text 字段 四、索引设计规范1、禁止在更新十分频繁、区分度不高的属性上建立索引 a）更新会变更B+树，更新频繁的字段建立索引会大大降低数据库性能 b）“性别”这种区分度不大的属性，建立索引是没有什么意义的 2、建立组合索引，必须把区分度高的字段放在最左边 如果 where a=? and b=? ， a 列的几乎接近于唯一值，那么只需要单建 idx_a 索引即可 3、 页面搜索严禁左模糊或者全模糊 索引文件具有 B-Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引, 如果需要请走搜索引擎来解决 五、SQL使用规范1、禁止使用SELECT *，只获取必要的字段，需要显示说明列属性 a）消耗cpu，io，内存，带宽 b）不能有效的利用覆盖索引 c）使用SELECT *容易在增加或者删除字段后出现程序BUG, 不具有扩展性 2、使用INSERT INTO t_xxx VALUES(xxx)，必须显示指定插入的列属性 容易在增加或者删除字段后出现程序BUG 3、务必请使用“同类型”进行比较，否则可能全表扫面 SELECT name FROM t_user WHERE phone=1333333333 会导致全表扫描. 4、禁止在WHERE条件的上使用函数或者计算 解读：SELECT naem FROM t_user WHERE date(create_datatime)=’2017-12-15’ 会导致全表扫描 推荐的写法是：SELECT name FROM t_user WHERE create_datatime&gt;=’2017-02-15’ and create_datatime &lt; ‘2017-02-16 ‘ 5、禁止负向查询，以及%开头的模糊查询 a）负向查询条件：NOT、!=、&lt;&gt;、!&lt;、!&gt;、NOT IN、NOT LIKE等，会导致全表扫描 b）%开头的模糊查询，会导致全表扫描 6、不要大表使用JOIN查询，禁止大表使用子查询 会产生临时表，消耗较多内存与CPU，极大影响数据库性能 7、OR改写为IN()或者UNION 原因很简单or不会走索引 8、简单的事务 事务就像程序中的锁一样粒度尽可能要小 9、不要一次更新大量数据 数据更新会对行或者表加锁，应该分为多次更新]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK1.7中HashMap底层实现原理]]></title>
    <url>%2Fblog%2F2015%2F03%2F13%2F2015-03-13-JDK1-7%E4%B8%ADHashMap%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[一、数据结构HashMap中的数据结构是数组+单链表的组合，以键值对(key-value)的形式存储元素的，通过put()和get()方法储存和获取对象。 （蓝色方块表示Entry对象，横排红框表示数组table[ ]，纵排绿框表示哈希桶bucket【实际上是一个由Entry组成的链表，新加入的Entry放在链头，最先加入的放在链尾】） 二、实现原理成员变量源码分析： 1234567891011121314151617181920212223242526/** 初始容量，默认16 */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** 最大初始容量，2^30 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** 负载因子，默认0.75，负载因子越小，hash冲突机率越低 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** 初始化一个Entry的空数组 */static final Entry&lt;?,?&gt;[] EMPTY_TABLE = &#123;&#125;;/** 将初始化好的空数组赋值给table，table数组是HashMap实际存储数据的地方，并不在EMPTY_TABLE数组中 */transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE;/** HashMap实际存储的元素个数 */transient int size;/** 临界值（HashMap 实际能存储的大小）,公式为(threshold = capacity * loadFactor) */int threshold;/** 负载因子 */final float loadFactor;/** HashMap的结构被修改的次数，用于迭代器 */transient int modCount; 构造方法源码分析： 123456789101112131415161718192021222324252627282930public HashMap(int initialCapacity, float loadFactor) &#123; // 判断设置的容量和负载因子合不合理 if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); // 设置负载因子，临界值此时为容量大小，后面第一次put时由inflateTable(int toSize)方法计算设置 this.loadFactor = loadFactor; threshold = initialCapacity; init();&#125;public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;public HashMap() &#123; this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR);&#125;public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR); inflateTable(threshold); putAllForCreate(m);&#125; put方法put()源码分析： 12345678910111213141516171819202122232425262728293031public V put(K key, V value) &#123; // 如果table引用指向成员变量EMPTY_TABLE，那么初始化HashMap（设置容量、临界值，新的Entry数组引用） if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; // 若“key为null”，则将该键值对添加到table[0]处，遍历该链表，如果有key为null，则将value替换。没有就创建新Entry对象放在链表表头 // 所以table[0]的位置上，永远最多存储1个Entry对象，形成不了链表。key为null的Entry存在这里 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值 int hash = hash(key); // 搜索指定hash值在对应table中的索引 int i = indexFor(hash, table.length); // 循环遍历table数组上的Entry对象，判断该位置上key是否已存在 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 哈希值相同并且对象相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; // 如果这个key对应的键值对已经存在，就用新的value代替老的value，然后退出！ V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 修改次数+1 modCount++; // table数组中没有key对应的键值对，就将key-value添加到table[i]处 addEntry(hash, key, value, i); return null; &#125; 可以看到，当我们给put()方法传递键和值时，HashMap会由key来调用hash()方法，返回键的hash值，计算Index后用于找到bucket（哈希桶）的位置来储存Entry对象。 如果两个对象key的hash值相同，那么它们的bucket位置也相同，但equals()不相同，添加元素时会发生hash碰撞，也叫hash冲突，HashMap使用链表来解决碰撞问题。 分析源码可知，put()时，HashMap会先遍历table数组，用hash值和equals()判断数组中是否存在完全相同的key对象， 如果这个key对象在table数组中已经存在，就用新的value代替老的value。如果不存在，就创建一个新的Entry对象添加到table[ i ]处。 如果该table[ i ]已经存在其他元素，那么新Entry对象将会储存在bucket链表的表头，通过next指向原有的Entry对象，形成链表结构（hash碰撞解决方案）。 Entry数据结构源码如下（HashMap内部类）： 12345678910111213141516171819static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; /** 指向下一个元素的引用 */ Entry&lt;K,V&gt; next; int hash; /** * 构造方法为Entry赋值 */ Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; ... ...&#125; 形成单链表的核心代码如下： 12345678910111213141516171819202122/** * 将Entry添加到数组bucketIndex位置对应的哈希桶中，并判断数组是否需要扩容 */void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 如果数组长度大于等于容量×负载因子，并且要添加的位置为null if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; // 长度扩大为原数组的两倍，代码分析见下面扩容机制 resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); &#125; createEntry(hash, key, value, bucketIndex);&#125;/** * 在链表中添加一个新的Entry对象在链表的表头 */void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; get方法如果两个不同的key的hashcode相同，两个值对象储存在同一个bucket位置，要获取value，我们调用get()方法，HashMap会使用key的hashcode找到bucket位置，因为HashMap在链表中存储的是Entry键值对，所以找到bucket位置之后，会调用key的equals()方法，按顺序遍历链表的每个 Entry，直到找到想获取的 Entry 为止——如果恰好要搜索的 Entry 位于该 Entry 链的最末端（该 Entry 是最早放入该 bucket 中），那HashMap必须循环到最后才能找到该元素。 get()方法源码如下： 1234567891011121314151617181920212223242526272829public V get(Object key) &#123; // 若key为null，遍历table[0]处的链表（实际上要么没有元素，要么只有一个Entry对象），取出key为null的value if (key == null) return getForNullKey(); // 若key不为null，用key获取Entry对象 Entry&lt;K,V&gt; entry = getEntry(key); // 若链表中找到的Entry不为null，返回该Entry中的value return null == entry ? null : entry.getValue();&#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123; if (size == 0) &#123; return null; &#125; // 计算key的hash值 int hash = (key == null) ? 0 : hash(key); // 计算key在数组中对应位置，遍历该位置的链表 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; // 若key完全相同，返回链表中对应的Entry对象 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; // 链表中没找到对应的key，返回null return null;&#125; 三、hash算法我们可以看到在HashMap中要找到某个元素，需要根据key的hash值来求得对应数组中的位置。如何计算这个位置就是hash算法。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量的分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，而不用再去遍历链表。 源码：1234567/** * Returns index for hash code h. */static int indexFor(int h, int length) &#123; // assert Integer.bitCount(length) == 1 : "length must be a non-zero power of 2"; return h &amp; (length-1);&#125; 四、性能问题HashMap有两个参数影响其性能：初始容量和负载因子。均可以通过构造方法指定大小。 容量capacity是HashMap中bucket哈希桶(Entry的链表)的数量，初始容量只是HashMap在创建时的容量，最大设置初始容量是2^30，默认初始容量是16（必须为2的幂），解释一下，当数组长度为2的n次幂的时候，不同的key通过indexFor()方法算得的数组位置相同的几率较小，那么数据在数组上分布就比较均匀，也就是说碰撞的几率小，相对的，get()的时候就不用遍历某个位置上的链表，这样查询效率也就较高了。 负载因子loadFactor是HashMap在其容量自动增加之前可以达到多满的一种尺度，默认值是0.75。 扩容机制：当HashMapde的长度超出了加载因子与当前容量的乘积（默认16*0.75=12）时，通过调用resize方法重新创建一个原来HashMap大小的两倍的newTable数组，最大扩容到2^30+1，并将原先table的元素全部移到newTable里面，重新计算hash，然后再重新根据hash分配位置。这个过程叫作rehash，因为它调用hash方法找到新的bucket位置。 扩容机制源码分析： 123456789101112131415161718void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; // 如果之前的HashMap已经扩充打最大了，那么就将临界值threshold设置为最大的int值 if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 根据新传入的newCapacity创建新Entry数组 Entry[] newTable = new Entry[newCapacity]; // 用来将原先table的元素全部移到newTable里面，重新计算hash，然后再重新根据hash分配位置 transfer(newTable, initHashSeedAsNeeded(newCapacity)); // 再将newTable赋值给table table = newTable; // 重新计算临界值，扩容公式在这儿（newCapacity * loadFactor） threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);&#125; 123456789101112131415void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 扩容问题：数组扩容之后，最消耗性能的点就出现了：原数组中的数据必须重新计算其在新数组中的位置，并放进去，这个操作是极其消耗性能的。所以如果我们已经预知HashMap中元素的个数，那么预设初始容量能够有效的提高HashMap的性能。 重新调整HashMap大小，当多线程的情况下可能产生条件竞争。因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。 五、线程安全HashMap是线程不安全的，在多线程情况下直接使用HashMap会出现一些莫名其妙不可预知的问题。在多线程下使用HashMap，有几种方案： A.在外部包装HashMap，实现同步机制 B.使用Map m = Collections.synchronizedMap(new HashMap(…));实现同步（官方参考方案，但不建议使用，使用迭代器遍历的时候修改映射结构容易出错） D.使用java.util.HashTable，效率最低（几乎被淘汰了） E.使用java.util.concurrent.ConcurrentHashMap，相对安全，效率高（建议使用） 注意一个小问题，HashMap所有集合类视图所返回迭代器都是快速失败的(fail-fast)，在迭代器创建之后，如果从结构上对映射进行修改，除非通过迭代器自身的 remove 或 add 方法，其他任何时间任何方式的修改，迭代器都将抛出 ConcurrentModificationException。。因此，面对并发的修改，迭代器很快就会完全失败。 六、关于JDK1.8的问题JDK1.8的HashMap源码实现和1.7是不一样的，有很大不同，其底层数据结构也不一样，引入了红黑树结构。有网友测试过，JDK1.8HashMap的性能要高于JDK1.7 15%以上，在某些size的区域上，甚至高于100%。随着size的变大，JDK1.7的花费时间是增长的趋势，而JDK1.8是明显的降低趋势，并且呈现对数增长稳定。当一个链表长度大于8的时候，HashMap会动态的将它替换成一个红黑树（JDK1.8引入红黑树大程度优化了HashMap的性能），这会将时间复杂度从O(n)降为O(logn)。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java类加载器]]></title>
    <url>%2Fblog%2F2015%2F01%2F07%2F2015-01-07-Java%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%2F</url>
    <content type="text"><![CDATA[类加载器工作机制类加载器就是寻找类的节码文件并构造出类在JVM内部表示对象的组件。负责将.class文件加载到内存中，并为之生成对应的Class对象。在Java中，类加载器把一个类装入JVM中，要经过以下步骤： [1.] 加载：（loading）通过类的全限定名获取二进制字节流（并没有指明要从一个Class文件中获取，可以从其他渠道，譬如：网络、动态生成、数据库等），将二进制字节流转换成方法区中的运行时数据结构，在内存中创建一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。加载阶段和连接阶段（linking）的部分内容（如一部分字节码文件格式验证动作）是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始，但这些夹在加载阶段之中进行的动作，仍然属于连接阶段的内容，这两个阶段的开始时间仍然保持着固定的先后顺序。 [2.] 连接：（linking）执行下面的验证、准备和解析步骤，其中解析步骤是可以选择的。 &gt; [2.1]验证：检查导入类或接口的二进制数据的正确性，（文件格式验证，元数据验证，字节码验证，符号引用验证）。这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用-Xverifynone参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。 [2.2]准备：准备阶段是正式为类的静态成员分配内存并设置默认初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。这时候进行内存分配的仅包括类静态成员变量，而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在堆中。其次，这里所说的初始值“通常情况”下是数据类型的零值。 [2.3]解析：解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。 [3.] 初始化：（initializing）初始化阶段是类加载过程的最后一步，到了初始化阶段，才真正开始执行类中定义的java程序代码。在准备阶段，变量已经赋过一次系统要求的默认初始值，而在初始化阶段，则根据程序猿通过程序制定的主管计划去初始化类变量和其他资源。 JVM三种预定义类型类加载器类装载工作由ClassLoader及其子类负责，ClassLoader是一个重要的Java运行时系统组件，它负责在运行时查找和装入Class字节码文件。JVM在运行时会产生三个ClassLoader： Bootstrap ClassLoader（根加载器），也被称为引导类加载器，负责Java核心类或-Xbootclasspath选项指定的jar包加载到内存中，在JDK中JRE的lib目录下rt.jar文件中加载文件，比如System,String等。它不是由java实现的，而是由C++编写，并不继承自 java.lang.ClassLoader 所以无法通过程序获取根加载器。 Extension ClassLoader（扩展类加载器），负责JRE的扩展目录中jar包的加载。主要将JDK中JRE的lib/ext目录中的类库或者由系统变量-Djava.ext.dir指定位置中的类库加载到内存中。 System/Application ClassLoader（系统类加载器），也称为应用程序类加载器，负责将JVM启动时来自java命令的class文件，或classpass环境变量所指定的jar包和类路径加载到内存中。可以通过ClassLoader的静态方法getSystemLoader()来获取系统类加载器。 这三个类加载器之间存在父子层级关系，即Bootstrap ClassLoader是Extension ClassLoader的父加载器，Extension ClassLoader是System ClassLoader的父加载器。默认情况下，使用System ClassLoader装载应用程序的类，我们可以做一个实验： 12345678public class ClassLoaderTest &#123; public static void main(String[] args) &#123; ClassLoader loader = Thread.currentThread().getContextClassLoader(); System.out.println("current loader:"+loader); System.out.println("parent loader:"+loader.getParent()); System.out.println("grandparent loader:"+loader.getParent(). getParent()); &#125;&#125; 运行以上代码，在控制台上将打出以下信息： 1234current loader:sun.misc.Launcher$AppClassLoader@7b7035c6parent loader:sun.misc.Launcher$ExtClassLoader@3da997a// 根加载器在Java中访问不到，所以返回null grandparent loader:null 通过以上的输出信息，我们知道当前的ClassLoader是AppClassLoader(应用程序类加载器)，父ClassLoader是ExtClassLoader(扩展类加载器)，祖父ClassLoader是根类加载器，因为在Java中无法获得它的句柄，所以仅返回null。 全盘负责机制JVM装载类时使用“全盘负责委托机制”。“全盘负责”是指当一个ClassLoader装载一个类的时，除非显式地使用另一个ClassLoader，该类所依赖及引用的类也由这个ClassLoader载入。”委托机制”见下面的”双亲委派机制“。 双亲委派机制如果一个类加载器收到了一个类加载请求，它不会自己去尝试加载这个类，而是把这个请求转交给父类加载器去完成。每一个层次的类加载器都是如此。因此所有的类加载请求都应该传递到最顶层的根类加载器中，只有到父类加载器反馈自己无法完成这个加载请求（在它的搜索范围没有找到这个类）时，子类加载器才会尝试自己去加载。这一点是从安全角度考虑的，试想如果有人编写了一个恶意的基础类（如java.lang.String）并装载到JVM中将会引起多么可怕的后果。但是由于有了“双亲委派机制”，java.lang.String永远是由根加载器来装载的，这样就避免了上述事件的发生。 “双亲委派”机制只是Java推荐的机制，并不是强制的机制。我们可以继承java.lang.ClassLoader类，实现自己的类加载器。如果想保持双亲委派模型，就应该重写findClass(name)方法；如果想破坏双亲委派模型，可以重写loadClass(name)方法。 ClassLoader重要方法在Java中，ClassLoader是一个抽象类，位于java.lang包中。下面对该类的一些重要接口方法进行介绍： Class loadClass(String name)name参数指定类加载器需要装载类的名字，必须使用全限定类名，如com.baobaotao. beans.Car。该方法有一个重载方法loadClass(String name ,boolean resolve)，resolve参数告诉类加载器是否需要解析该类。在初始化类之前，应考虑进行类解析的工作，但并不是所有的类都需要解析，如果JVM只需要知道该类是否存在或找出该类的超类，那么就不需要进行解析。 Class defineClass(String name, byte[] b, int off, int len)将类文件的字节数组转换成JVM内部的java.lang.Class对象。字节数组可以从本地文件系统、远程网络获取。name为字节数组对应的全限定类名。 Class findSystemClass(String name)从本地文件系统载入Class文件，如果本地文件系统不存在该Class文件，将抛出ClassNotFoundException异常。该方法是JVM默认使用的装载机制。 Class findLoadedClass(String name)调用该方法来查看ClassLoader是否已装入某个类。如果已装入，那么返回java.lang.Class对象，否则返回null。如果强行装载已存在的类，将会抛出连接错误。 ClassLoader getParent()获取类加载器的父加载器，除根加载器外，所有的类加载器都有且仅有一个父加载器，ExtClassLoader的父加载器是根加载器，因为根加载器非Java编写，所以无法获得，将返回null。 除JVM默认的三个ClassLoader以外，可以编写自己的第三方类加载器，以实现一些特殊的需求。类文件被装载并解析后，在JVM内将拥有一个对应的java.lang.Class类描述对象，该类的实例都拥有指向这个类描述对象的引用，而类描述对象又拥有指向关联ClassLoader的引用，如图所示。 每一个类在JVM中都拥有一个对应的java.lang.Class对象，它提供了类结构信息的描述。数组、枚举、注解以及基本Java类型（如int、double等），甚至void都拥有对应的Class对象。Class没有public的构造方法。Class对象是在装载类时由JVM通过调用类加载器中的defineClass()方法自动构造的。 类的初始化类什么时候才被初始化 创建类的实例，也就是new一个对象 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射（Class.forName(“com.dijia478.load”)） 初始化一个类的子类（会首先初始化子类的父类） JVM启动时标明的启动类，即用java.exe命令来运行文件名和类名相同的那个主类 只有这6中情况才会导致类的类的初始化。 类的初始化步骤： 如果这个类还没有被加载和连接，那先进行加载和连接 假如这个类存在直接父类，并且这个类还没有被初始化（注意：在一个类加载器中，类只能初始化一次），那就初始化直接的父类（不适用于接口） 加入类中存在初始化语句（如static变量和static块），那就依次执行这些初始化语句。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>类加载器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[request和response中文乱码问题后台处理办法]]></title>
    <url>%2Fblog%2F2014%2F12%2F19%2F2014-12-19-request%E5%92%8Cresponse%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98%E5%90%8E%E5%8F%B0%E5%A4%84%E7%90%86%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[request接收参数的中文乱码的处理：GET：方法一：使用String的构造方法： 1new String(request.getParameter("传过来的name").getBytes("ISO-8859-1"), "UTF-8"); 方法二：修改tomcat7的默认编码方式，server.xml中端口号那项添加配置： 1&lt;Connector connectionTimeout="50000" port="8080" protocol="HTTP/1.1" redirectPort="8443" URIEncoding="UTF-8"/&gt; POST：方法一：设置request的缓冲区的编码： 1request.setCharacterEncoding("UTF-8"); 方法二：使用spring的编码过滤器，在web.xml中添加： 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;CharacterEncoding&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncoding&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; response输出中文的乱码问题：字符流输出中文：方法一： 1234// 设置浏览器字符集编码. response.setHeader("Content-Type","text/html;charset=UTF-8");// 设置response的缓冲区的编码.response.setCharacterEncoding("UTF-8"); 方法二：建议使用： 1response.setContentType("text/html;charset=UTF-8"); 字节流输出中文：（实际中不会用）123456// 使用字节流的方式输出中文：ServletOutputStream outputStream = response.getOutputStream();// 设置浏览器默认打开的时候采用的字符集response.setHeader("Content-Type", "text/html;charset=UTF-8");// 设置中文转成字节数组字符集编码outputStream.write("中文".getBytes("UTF-8"));]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>中文乱码</tag>
      </tags>
  </entry>
</search>
