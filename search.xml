<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Tomcat在Linux服务器上的BIO、NIO、APR模式设置]]></title>
    <url>%2Fblog%2F2018%2F01%2F04%2F2018-01-04-Tomcat%E5%9C%A8Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E7%9A%84BIO%E3%80%81NIO%E3%80%81APR%E6%A8%A1%E5%BC%8F%E8%AE%BE%E7%BD%AE%2F</url>
    <content type="text"><![CDATA[一、BIO、NIO、AIO先了解四个概念同步： 自己亲自出马持银行卡到银行取钱（使用同步IO时，Java自己处理IO读写）。 异步： 委托一小弟拿银行卡到银行取钱，然后给你（使用异步IO时，Java将IO读写委托给OS处理，需要将数据缓冲区地址和大小传给OS(银行卡和密码)，OS需要支持异步IO操作API）。阻塞： ATM排队取款，你只能等待（使用阻塞IO时，Java调用会一直阻塞到读写完成才返回）。 非阻塞：柜台取款，取个号，然后坐在椅子上做其它事，等号广播会通知你办理，没到号你就不能去，你可以不断问大堂经理排到了没有，大堂经理如果说还没到你就不能去（使用非阻塞IO时，如果不能读写Java调用会马上返回，当IO事件分发器会通知可读写时再继续进行读写，不断循环直到读写完成）。 Java对BIO、NIO、AIO的支持Java BIO： 同步并阻塞，服务器实现模式为一个连接一个线程，即客户端有连接请求时服务器端就需要启动一个线程进行处理，如果这个连接不做任何事情会造成不必要的线程开销，当然可以通过线程池机制改善。 Java NIO： 同步非阻塞，服务器实现模式为一个请求一个线程，即客户端发送的连接请求都会注册到多路复用器上，多路复用器轮询到连接有I/O请求时才启动一个线程进行处理。 Java AIO(NIO.2)： 异步非阻塞，服务器实现模式为一个有效请求一个线程，客户端的I/O请求都是由OS先完成了再通知服务器应用去启动线程进行处理 BIO、NIO、AIO适用场景分析BIO方式适用于连接数目比较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4以前的唯一选择，但程序直观简单易理解。 NIO方式适用于连接数目多且连接比较短（轻操作）的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。 AIO方式使用于连接数目多且连接比较长（重操作）的架构，比如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK7开始支持。 二、tomcat三种模式简介BIOBIO(blocking I/O)，顾名思义，即阻塞式I/O操作，表示Tomcat使用的是传统的Java I/O操作(即java.io包及其子包)。Tomcat在默认情况下，就是以bio模式运行的。遗憾的是，就一般而言，bio模式是三种运行模式中性能最低的一种。我们可以通过Tomcat Manager来查看服务器的当前状态。 NIO是Java SE 1.4及后续版本提供的一种新的I/O操作方式(即java.nio包及其子包)。Java nio是一个基于缓冲区、并能提供非阻塞I/O操作的Java API，因此nio也被看成是non-blocking I/O的缩写。它拥有比传统I/O操作(bio)更好的并发运行性能。 APR(Apache Portable Runtime/Apache可移植运行库)，是Apache HTTP服务器的支持库。你可以简单地理解为，Tomcat将以JNI的形式调用Apache HTTP服务器的核心动态链接库来处理文件读取或网络传输操作，从而大大地提高Tomcat对静态文件的处理性能。 Tomcat apr也是在Tomcat上运行高并发应用的首选模式。 三、tomcat三种模式性能比较这里我引用了网友给出的测试结果 四、tomcat模式设置我这里演示的是tomcat7，默认是BIO模式的。而tomcat8是默认NIO模式的。 BIO模式tomcat7默认就是。如果你是tomcat8或9想设置成BIO模式的，那么在tomcat目录里的conf目录里的server.xml文件中修改。找到设置端口号8080的那个标签，主要是修改protocol属性为HTTP/1.1，重启tomcat就会使用BIO模式。tomcat7默认就是这个样子的 123&lt;Connector port="8080" protocol="HTTP/1.1" connectionTimeout="20000" redirectPort="8443" /&gt; NIO模式在和设置BIO模式同样的位置上，修改protocol属性为org.apache.coyote.http11.Http11NioProtocol，重启tomcat就会使用NIO模式。tomcat8以上默认就是这个样子的 123&lt;Connector port="8080" protocol="org.apache.coyote.http11.Http11NioProtocol" connectionTimeout="20000" redirectPort="8443" /&gt; APR模式启用这种模式稍微麻烦一些，除了需要改配置文件，还需要安装一些依赖库，以下就是安装所需的条件： 1. 最新的apr 2. 最新的apr-util 3. tomcat-native.tar.gz(在tomcat/bin/下有相应的安装tar包) 前两个依赖库的官方下载地址：http://apr.apache.org/download.cgi 先改配置文件，和改BIO,NIO模式的位置一样，修改protocol属性为org.apache.coyote.http11.Http11AprProtocol，注意和NIO的很像，但不一样 123&lt;Connector port="8080" protocol="org.apache.coyote.http11.Http11AprProtocol" connectionTimeout="20000" redirectPort="8443" /&gt; 现在先安装apr，在解压好的apr目录下，执行下面的命令，指定apr安装目录： 1./configure --prefix=/usr/local/apr &amp;&amp; make &amp;&amp; make install 再安装apr-util，在解压好的apr-util目录下，执行下面的命令，指定apr目录和apr-util安装目录： 1./configure --with-apr=/usr/local/apr/ --prefix=/usr/local/apr-util &amp;&amp; make &amp;&amp; make install 再安装tomcat-native，这个不用下载，在tomcat/bin/下有相应的安装tar包，在解压好的tomcat-native目录下，执行下面的命令，指定指定apr目录和JAVA_HOME目录： 1./native/configure --with-apr=/usr/local/apr --with-java-home=/usr/share/jdk1.8 &amp;&amp; make &amp;&amp; make install 安装完后记得在 /etc/profile 文件中的JAVA_HOME环境变量后面多加一条APR的环境变量（注意你自己安装的apr目录） 1export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/apr/lib 用source /etc/profile命令让环境变量配置立即生效，启动tomcat，就是APR模式了 五、如何确定自己当前的模式启动tomcat后，可以在tomcat/logs目录下，执行如下命令： 1tail -f catalina.out 我这里就是成功启动APR模式后显示的样子]]></content>
      <categories>
        <category>Tomcat</category>
      </categories>
      <tags>
        <tag>Tomcat设置</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solr集群在Linux上搭建详细步骤]]></title>
    <url>%2Fblog%2F2017%2F12%2F24%2F2017-12-24-Solr%E9%9B%86%E7%BE%A4%E5%9C%A8Linux%E4%B8%8A%E6%90%AD%E5%BB%BA%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[一、Solr集群的系统架构SolrCloud(solr 云)是Solr提供的分布式搜索方案，当你需要大规模，容错，分布式索引和检索能力时使用 SolrCloud。当一个系统的索引数据量少的时候是不需要使用SolrCloud的，当索引量很大，搜索请求并发很高，这时需要使用SolrCloud来满足这些需求。 SolrCloud是基于Solr和Zookeeper的分布式搜索方案，它的主要思想是使用Zookeeper作为集群的配置信息中心。它有几个特色功能： 1）集中式的配置信息 2）自动容错 3）近实时搜索 4）查询时自动负载均衡 1.物理结构三个Solr实例（ 每个实例包括两个Core），组成一个SolrCloud。 2.逻辑结构索引集合包括两个Shard（shard1和shard2），shard1和shard2分别由三个Core组成，其中一个Leader两个Replication，Leader是由zookeeper选举产生，zookeeper控制每个shard上三个Core的索引数据一致，解决高可用问题。 用户发起索引请求分别从shard1和shard2上获取，解决高并发问题。 2.1. collectionCollection在SolrCloud集群中是一个逻辑意义上的完整的索引结构。它常常被划分为一个或多个Shard（分片），它们使用相同的配置信息。 比如：针对商品信息搜索可以创建一个collection。 collection=shard1+shard2+….+shardX 2.2. Core每个Core是Solr中一个独立运行单位，提供 索引和搜索服务。一个shard需要由一个Core或多个Core组成。由于collection由多个shard组成所以collection一般由多个core组成。 2.3. Master或SlaveMaster是master-slave结构中的主结点（通常说主服务器），Slave是master-slave结构中的从结点（通常说从服务器或备服务器）。同一个Shard下master和slave存储的数据是一致的，这是为了达到高可用目的。 2.4. ShardCollection的逻辑分片。每个Shard被化成一个或者多个replication，通过选举确定哪个是Leader。 3.本次演示实现的solr集群架构 Zookeeper作为集群的管理工具。 1、集群管理：容错、负载均衡。 2、配置文件的集中管理 3、集群的入口 需要实现zookeeper 高可用。需要搭建集群。建议是奇数节点。需要三个zookeeper服务器。 搭建solr集群至少需要7台服务器。 这里因环境限制，演示的是搭建伪分布式（在一台虚拟机上，建议内存至少1G）： 需要三个zookeeper节点 需要四个tomcat节点。 本文使用tomcat进行部署，而不使用solr自带的jetty 4.系统环境CentOS-6.7-i386-bin-DVD1 jdk-8u151-linux-i586 apache-tomcat-8.5.24 zookeeper-3.4.10 solr-7.1.0 注意：solr6.0以上版本，官方建议使用jdk8，tomcat8，搭建集群步骤和solr6以下略微有区别。搭建solr集群前，要先关闭iptables防火墙服务 二、 先搭建Zookeeper集群第一步：上传，解压zookeeper12rztar zxf zookeeper-3.4.10.tar.gz 第二步：在zookeeper目录下创建一个data目录1mkdir ./zookeeper-3.4.10/data 第三步：把zookeeper目录下的conf目录下的zoo_sample.cfg文件改名为zoo.cfg12cd ./zookeeper-3.4.10/confmv zoo_sample.cfg zoo.cfg 第四步：把zookeeper目录复制三份先创建目录/usr/local/solr-cloud，这里就是后面集群放置的目录了 1234mkdir /usr/local/solr-cloudcp -r ./zookeeper-3.4.10 /usr/local/solr-cloud/zookeeper01cp -r ./zookeeper-3.4.10 /usr/local/solr-cloud/zookeeper02cp -r ./zookeeper-3.4.10 /usr/local/solr-cloud/zookeeper03 第五步：创建三个myid文件在第四步复制的三个zookeeper目录里的data目录下，分别创建一个myid文件，文件名都叫做“myid”。内容就是每个实例的id。例如1、2、3 123echo 1 &gt;&gt; ./zookeeper01/data/myidecho 2 &gt;&gt; ./zookeeper02/data/myidecho 3 &gt;&gt; ./zookeeper03/data/myid 第六步：修改三个zoo.cfg配置文件就是上面第三步改的文件，第四步复制了三份。这里只演示一个，但三份都要改。前两个红色框框里的（目录和端口号）内容三份配置文件是不一样的，目录就是各自对应的目录，我的端口号分别改为了2181，2182，2183。而最后那个红框里的内容三份配置文件是一样的。 server.1的这个1，就是第五步myid文件的内容。在实际工作中每个zookeeper实例在不同的服务器上，所以后面的ip应该是不同的，我这里是在一台虚拟机上演示，所以ip相同。 第七步：启动每个zookeeper实例这里一个个进目录里启动实在麻烦，我这里在solr-cloud目录下写个脚本来执行 1vi start-zookeeper-all.sh 脚本内容： 123456cd /usr/local/solr-cloud/zookeeper01/bin/./zkServer.sh startcd /usr/local/solr-cloud/zookeeper02/bin/./zkServer.sh startcd /usr/local/solr-cloud/zookeeper03/bin/./zkServer.sh start 写完后保存退出，发现脚本没有执行权限，添加权限： 1chmod u+x start-zookeeper-all.sh 执行脚本： 为了验证，去三个zookeeper的实例里bin目录下分别查看每个实例的状态 （我刚开始是把查看状态的命令写在脚本里的，让它一启动就查看，可每次状态都显示not running，后来想了想，应该是因为脚本执行太快，启动命令执行了但还没启动起来，就去查看状态，所以会显示没有运行） 如果你显示的是这样子的一个领导两个部下（leader和follower不一定是谁，随机的），那么就代表zookeeper集群已经搭建完成 zookeeper集群搭建完成了，下来搭建solr集群 三、solr集群搭建注意，在搭建solr集群前，建议最好有一个solr服务是已经搭建好的，可以简化大量重复的配置操作。 单机solr服务搭建过程参看我的这篇文章： Solr服务在Linux上搭建详细步骤 这个单机solr服务在solr集群搭建第二步和第三步里需要，搭建好一个solr服务后，就可以进行集群搭建了。 第一步：创建四个tomcat实例。每个tomcat运行在不同的端口。8180、8280、8380、8480在搭建单机solr服务第二步的时候，tomcat8已经上传解压好了，所以直接复制用就行，复制到搭建zookeeper集群第四步时创建的/usr/local/solr-cloud/目录下。 1234cp -r apache-tomcat-8.5.24 /usr/local/solr-cloud/tomcat01cp -r apache-tomcat-8.5.24 /usr/local/solr-cloud/tomcat02cp -r apache-tomcat-8.5.24 /usr/local/solr-cloud/tomcat03cp -r apache-tomcat-8.5.24 /usr/local/solr-cloud/tomcat04 在复制过来的四个tomcat目录下的conf里的server.xml，修改每个tomcat的端口号，这里只演示第一个目录的，另外三个都要改，端口要互不冲突 1vi ./tomcat01/conf/server.xml 用/port命令搜索port字符串，按n搜索下一个，有三个地方要改，依次更改为 其他三个tomcat目录里的server.xml配置的端口按照234的顺序全改了啊，这里我就不截图了。 一共要改四个文件，每个文件里改三个地方，这12个端口号要互不冲突。 123vi ./tomcat02/conf/server.xmlvi ./tomcat03/conf/server.xmlvi ./tomcat04/conf/server.xml 第二步：把单机版的solr工程复制到集群中的tomcat里在搭建好的单机solr服务里（这里是另一篇教程，上面说过了），复制solr工程到第一步的4个tomcat目录里，一共是复制4份 1234cp -r /usr/local/solr/tomcat/webapps/solr /usr/local/solr-cloud/tomcat01/webapps/cp -r /usr/local/solr/tomcat/webapps/solr /usr/local/solr-cloud/tomcat02/webapps/cp -r /usr/local/solr/tomcat/webapps/solr /usr/local/solr-cloud/tomcat03/webapps/cp -r /usr/local/solr/tomcat/webapps/solr /usr/local/solr-cloud/tomcat04/webapps/ 第三步：为每个solr实例创建一个对应的solrhome一样在之前搭建的单机solr服务里，把solrhome复制4份出来到solr-cloud目录里 1234cp -r /usr/local/solr/solrhome /usr/local/solr-cloud/solrhome01cp -r /usr/local/solr/solrhome /usr/local/solr-cloud/solrhome02cp -r /usr/local/solr/solrhome /usr/local/solr-cloud/solrhome03cp -r /usr/local/solr/solrhome /usr/local/solr-cloud/solrhome04 现在solr-cloud目录里是有这些目录，检查一下有没有复制错地方的： 123456789101112solrhome01solrhome02solrhome03solrhome04start-zookeeper-all.shtomcat01tomcat02tomcat03tomcat04zookeeper01zookeeper02zookeeper03 第四步：配置solrCloud相关的配置第三步复制好的四个solrhome下，都有一个solr.xml，把其中的ip及端口号配置好。搭建单机版solr服务的时候没有动过solrhome里的这个文件，现在搭建集群了，需要进行修改。需要修改4个solr.xml，我这里还是只演示第一个，另外三个目录里的类比着改，一样的，就是1234的顺序 1vi solrhome01/solr.xml 找到这个地方，第一个红框代表当前节点的ip地址，实际工作中就是会部署4个服务器，一个服务器是一个节点，部署一个solr服务。第二个红框代表当前solr服务实例的端口号，就是所在的tomcat的端口号，就是第一步在tomcat01的server.xml里配置的8180 我的配置完后是这样的： 12&lt;str name="host"&gt;192.168.25.128&lt;/str&gt;&lt;int name="hostPort"&gt;8180&lt;/int&gt; 其他三个solr.xml照着改，由于我是在一台虚拟机搭建的四个solr实例，所以肯定四个实例的ip是一样的，但端口分别是8180，8280，8380，8480 123vi solrhome02/solr.xmlvi solrhome03/solr.xmlvi solrhome04/solr.xml 第五步：修改solr服务的web.xml文件。把solrhome关联起来修改这个文件，和单机版的solr配置是一样的 要注意的是，tomcat01这里的solr服务，是上面第二步复制过来的对吧，是我之前用solr7搭建的单机solr服务，里面的配置和solr4不太一样，具体还是去上面看我提供的单机solr服务搭建教程链接 1vi tomcat01/webapps/solr/WEB-INF/web.xml 找到这个，这里是我之前搭建单机solr服务时，配置的solrhome路径 现在改成集群的solrhome01目录，使他们关联起来 1&lt;env-entry-value&gt;/usr/local/solr-cloud/solrhome01&lt;/env-entry-value&gt; 其他三个tomcat里的web.xml都对应着一改，solrhome02，03，04 123vi tomcat02/webapps/solr/WEB-INF/web.xmlvi tomcat03/webapps/solr/WEB-INF/web.xmlvi tomcat04/webapps/solr/WEB-INF/web.xml 第六步：让zookeeper统一管理配置文件。需要把/conf目录上传到zookeeper现在我们每一个solr都有了自己的solrhome，现在我们要让每一个solr实例的配置文件都一样，这个配置文件需要集中管理，这个时候我们使用zookeeper来统一管理配置文件。所以要将配置文件上传到zookeeper中。那么上传哪些配置文件呢？ 这里注意下managed-schema文件，网上有很多低版本solr，会提到一个collection1/conf下的schema.xml，但是本人并没有找到。好像是从5.0版本开始不使用schema.xml的，这俩其实内容都一样，搞不懂为啥要换个名字，而且内容格式是xml，但是文件名却没有.xml的后辍，这里直接上传整个conf目录就行。 1/usr/local/solr-cloud/solrhome01/configsets/sample_techproducts_configs/conf 知道了要上传什么，那么怎么上传呢？打开最早solr解压出来的原始文件（如果删了那就重新上传解压吧） 在这个目录下有个zkcli.sh脚本文件，执行这个脚本就可以将配置文件上传到zookeeper了，有点难找，我是用find命令给搜出来的。 1/home/dijia478/solr-7.1.0/server/scripts/cloud-scripts 要注意，在执行脚本上传配置文件前，必须先去启动zookeeper集群 12cd /usr/local/solr-cloud/./start-zookeeper-all.sh 然后回来执行脚本，这里脚本的执行命令有点长，主要是参数多，建议复制出来改好后再粘贴上去 12cd /home/dijia478/solr-7.1.0/server/scripts/cloud-scripts./zkcli.sh -zkhost 192.168.25.128:2181,192.168.25.128:2182,192.168.25.128:2183 -cmd upconfig -confdir /usr/local/solr-cloud/solrhome01/configsets/sample_techproducts_configs/conf -confname myconf 我解释下各个参数的含义： 红色框代表zookeeper集群的ip和端口号列表（搭建zookeeper集群的时候配置过的） 绿色框代表要执行的是上传配置文件操作 黄色框代表的是要上传的配置文件目录（低版本不太一样，具体以那两个主要的配置文件所在目录为准，不知到在哪就find命令搜吧） 紫色框代表的是你给上传的配置起的名字，可以改 现在上传完了，那么我们怎么确定是否上传成功呢？去zookeeper集群的一个目录找到bin里zookeeper的客户端脚本 12cd /usr/local/solr-cloud/zookeeper01/bin/./zkCli.sh 运行后，里面会出现一大堆内容，如果你不指定参数，他会默认访问localhost:2181 在最下面执行ls /，查看在根目录下有什么，发现一个configs，再看它里面有什么？ 这就是我们刚才上传的配置了，名字一样的，代表上传成功了 然后用quit命令退出 如果你以后需要修改solr配置的话，只用在刚才那个solrhome01/…/conf目录里改好，改好后再上传一次就行了，就会覆盖原来的配置文件 第七步：修改tomcat/bin目录下的catalina.sh 文件，关联solr和zookeeper现在上传好配置文件了，可是solr和zookeeper还没有建立任何关系，他们也不知道对方在哪里，这个时候需要修改4个tomcat的配置文件，这里只演示tomcat01，其他三个完全相同照着改 12cd tomcat01/binvi catalina.sh 用/JAVA_OPTS搜索红色框框里的这句话（因为低版本加的位置长的不太一样，但这句话的例子是不变的），在这句话下面的位置加（注意位置，不要弄错了）： 在图上位置加上JAVA_OPTS的值（zookeeper集群的ip列表）： 1JAVA_OPTS="-DzkHost=192.168.25.128:2181,192.168.25.128:2182,192.168.25.128:2183" 然后把其他三个tomcat也一改，改的位置和内容是一样的，不用变 123vi tomcat02/bin/catalina.shvi tomcat03/bin/catalina.shvi tomcat04/bin/catalina.sh 这样每个solr实例就通过这个参数和zookeeper集群建立了联系，solr会将自己的状态发送给zookeeper，比如ip地址啊，端口号啊，zookeeper就可以连接到solr了，建立了通信关系 第八步：启动每个tomcat实例。要包装zookeeper集群是启动状态现在需要启动每个tomcat，当然了，这个和启动zookeeper集群一样，要一个一个进去启动，太麻烦了，还是在solr-cloud目录下写个批处理脚本来运行 1vi start-tomcat-all.sh 脚本内容： 1234/usr/local/solr-cloud/tomcat01/bin/startup.sh/usr/local/solr-cloud/tomcat02/bin/startup.sh/usr/local/solr-cloud/tomcat03/bin/startup.sh/usr/local/solr-cloud/tomcat04/bin/startup.sh 保存，退出，还是要为脚本添加执行权限，不然执行不了。添加完后运行脚本，tomcat集群启动有点慢 12chmod u+x start-tomcat-all.sh./start-tomcat-all.sh 如果你想看启动起来没有，可以复制一个SSH渠道（我用的xshell5），去看看tomcat的日志信息（相当于看控制台打印信息），在另一个会话窗口里运行下面的命令： 1tail -f /usr/local/solr-cloud/tomcat01/logs/catalina.out 这里说个小知识点，tomcat8开始，默认启动的是NIO模式，7默认启动的是BIO模式，还可以通过配置设置APR模式启动，至于APR，NIO和BIO的区别，是和tomcat并发性能有关的，高并发的系统应该将tomcat的模式设置成APR模式，会大幅度的提高服务器的处理和响应性能。感兴趣的可以自己百度下。当然这个不用在意，跟本文集群搭建没关系，就是想到了说一下。过几天我再写个配置tomcat三种模式的博客吧。 第九步：访问集群然后用自己的电脑访问下集群，之前单机版是没有红框框出来的两个东西的，出现这个就是OK了，但还没完 我这里还没创建collections，所以什么都没有： 注意下访问地址的输入，要写全。访问地址写成这样会404 第十步：创建新的Collection进行分片处理点击页面的Collections按钮，然后就能添加了（高版本的solr才有，低版本的需要通过地址栏传递参数去设置，比较麻烦） 我这里选择的是名字叫mycollection1，用自己上传的myconf配置文件，有2片shard，每个shard有2个备份节点一主一备 然后回去那个第九步空的页面看下，这样solr集群就搭建ok了！ 第十一步：删除不用的Collection或core删除collection，点这里，然后输入你要删除的collection名称就行 删除core在右边，完了如果要添加下面有add replica（如果工作中你的哪个备份机挂了，就这样删掉挂的服务器，再添加一个好的就行，当然了，在这里添加前，肯定是需要在服务器上部署好solr服务，然后连接zookeeper集群才行的）]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>solr集群搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Solr服务在Linux上搭建详细步骤]]></title>
    <url>%2Fblog%2F2017%2F12%2F20%2F2017-12-20-Solr%E6%9C%8D%E5%8A%A1%E5%9C%A8Linux%E4%B8%8A%E6%90%AD%E5%BB%BA%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4%2F</url>
    <content type="text"><![CDATA[一、系统环境CentOS-6.7-i386-bin-DVD1 jdk-8u151-linux-i586 apache-tomcat-8.5.24.tar solr-7.1.0 注意：solr6.0以上，官方建议使用jdk8，tomcat8。和solr4部署流程有略微差别。部署solr服务前，要先关闭iptables防火墙服务 二、搭建步骤第一步：上传并解压solr7.1这里解压出来的文件夹叫solr-7.1.0，后面有很多步骤需要这个目录，以下简称：solr目录 12rz tar zxf solr-7.1.0.tgz 第二步：上传并解压tomcat8.5，复制一份出来1234rztar zxf apache-tomcat-8.5.24.tar.gzmkdir /usr/local/solrcp -r apache-tomcat-8.5.24 /usr/local/solr/tomcat 第三步：把solr部署到tomcat下注意，这里因为我用的是solr7.1最新版，所以跟网上很多solr4的版本要拷贝*.war文件，然后再启动tomcat解压的操作是不一样的。（这里直接就是解压好的） 复制并重命名solr目录里的server/solr-webapp/webapp文件夹到/usr/local/solr/tomcat/webapps/solr 第四步：把solr目录里的server/lib/目录下的部分jar包，添加到第三步部署的solr工程中。solr目录里的server/lib/ext/下的所有jar包，都复制到 /usr/local/solr/tomcat/webapps/solr/WEB-INF/lib/下，都是些日志相关的jar包 12cd server/lib/ext/cp * /usr/local/solr/tomcat/webapps/solr/WEB-INF/lib/ solr目录里的server/lib/metrics* 开头的5个jar包，复制到 /usr/local/solr/tomcat/webapps/solr/WEB-INF/lib/下（solr4部署没有这个） 12cd server/lib/cp met* /usr/local/solr/tomcat/webapps/solr/WEB-INF/lib/ 第五步：把solr目录里的server/resources/目录下的log4j.properties，添加到第三步部署的solr工程中注意要创建一个classes的目录（solr4部署没有第五步） 123mkdir /usr/local/solr/tomcat/webapps/solr/WEB-INF/classes/cd server/resources/cp log4j.properties /usr/local/solr/tomcat/webapps/solr/WEB-INF/classes/ 第六步：创建一个solrhome将solr目录里的servier/solr文件夹，复制到/usr/local/solr/下，重命名为solrhome，现在/usr/local/solr/目录下就有两个文件夹了，分别是第二步复制过来的tomcat文件夹，和第六步复制过来的solrhome文件夹 12cd servercp -r solr /usr/local/solr/solrhome 第七步：关联已部署的solr和solrhome需要修改第二步复制出来的tomcat目录里，solr工程的web.xml文件 12cd tomcat/webapps/solr/WEB-INF/vi web.xml 找到这个，是被注释的，需要修改value那项 打开注释，修改为上面自己创建的solrhome目录 然后到最下方，将这一段注释掉，不然会报403错误，完成后保存退出（solr4部署不用注释这个） 第八步：启动tomcat12cd tomcat/bin./startup.sh 去自己的电脑上访问下服务器的solr服务 注意下访问地址，直接访问(根据你自己的服务器ip写)192.168.25.128:8080/solr会报404，需要在后面加上/index.xml 如果出现下面的页面，就是solr服务搭建ok了 如果要关闭solr服务，直接关闭tomcat就可以了 12cd tomcat/bin./shutdown.sh 三、关于集群搭建这篇只是单机solr服务的搭建过程，如果需要搭建solr集群，请参考我的这篇博客： Solr集群在Linux上搭建详细步骤]]></content>
      <categories>
        <category>Solr</category>
      </categories>
      <tags>
        <tag>单机solr服务搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux服务器上安装JDK小白教程]]></title>
    <url>%2Fblog%2F2017%2F09%2F07%2F2017-09-07-Linux%E6%9C%8D%E5%8A%A1%E5%99%A8%E4%B8%8A%E5%AE%89%E8%A3%85JDK%E5%B0%8F%E7%99%BD%E6%95%99%E7%A8%8B%2F</url>
    <content type="text"><![CDATA[一、环境VMware12 Pro CentOS-6.7-i386-bin-DVD1 jdk-8u151-linux-i586 二、详细安装步骤前提：需要卸载已经装过的jdk rpm -qa | grep jdk 会显示你所有包含jdk这个字符串的安装包 rpm -e –nodeps 对应的每个包名 会卸载对应的包 之后如果输入java -version命令后显示command not found，就是卸载完了。我之前是装过jdk1.7的，所以我需要卸载 ，从来没装过jdk的直接从下面开始 1. 去官网下载JDK http://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html 需要选中那个图中红色框起来的小圆点，才能下在，意思就是你接收许可协议 因为我的虚拟系统是32位的，所以我下32位JDK，根据你的情况具体看。x86代表32位系统，x64代表64位系统。 不知道服务器是多少位系统的，直接用这个命令： 1getconf LONG_BIT 2. 上传，解压JDK有人可能不知到怎么上传文件到服务器上，这里我推荐个软件，叫lrzsz 这是一个简单的linux服务器上传下载工具，如果你没装过的话，可以装一下，挺好用的 安装前先检查有没有安装lrzsz： 1rpm –q lrzsz 没安装可以使用下列命令进行安装： 1yum -y install lrzsz 之后你就可以用rz命令上传本地文件到服务器的当前目录下了，sz命令后面跟上指定文件目录，可以将其从服务器上下载到本地 好了，用rz命令上传好jdk的tar包后，需要解压，执行如下命令，可能你用的版本和我不一样，注意后面是你自己上传的jdk的包名： 1tar zxf jdk-8u151-linux-i586.tar.gz 3. 移动下位置为了以后方便管理，我把jdk的目录换个位置，重命名一下 1mv jdk1.8.0_151 /usr/share/jdk1.8 4. 配置环境变量用vim打开/etc/profile文件 1vim /etc/profile 在文件最下面添加下面的语句，保存退出（按ESC，然后输入，英文冒号wq英文感叹号（:wq!），按回车） 1234JAVA_HOME=/usr/share/jdk1.8CLASSPATH=$JAVA_HOME/lib/PATH=$PATH:$JAVA_HOME/binexport PATH JAVA_HOME CLASSPATH 执行下面的命令让配置立即生效 1source /etc/profile 现在输入java -version命令，如果显示java version &quot;1.8.0_151&quot;，就是ok了]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>JDK安装</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring框架学习笔记整理-cache篇]]></title>
    <url>%2Fblog%2F2017%2F08%2F28%2F2017-08-28-Spring%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-cache%E7%AF%87%2F</url>
    <content type="text"><![CDATA[关于缓存缓存是实际工作中非常常用的一种提高性能的方法。而在java中，所谓缓存，就是将程序或系统经常要调用的对象存在内存中，再次调用时可以快速从内存中获取对象，不必再去创建新的重复的实例。这样做可以减少系统开销，提高系统效率。在增删改查中，数据库查询占据了数据库操作的80%以上，而非常频繁的磁盘I/O读取操作，会导致数据库性能极度低下。而数据库的重要性就不言而喻了： 数据库通常是企业应用系统最核心的部分 数据库保存的数据量通常非常庞大 数据库查询操作通常很频繁，有时还很复杂 在系统架构的不同层级之间，为了加快访问速度，都可以存在缓存 Spring cache特性与缺憾现在市场上主流的缓存框架有ehcache、redis、memcached。spring cache可以通过简单的配置就可以搭配使用起来。其中使用注解方式是最简单的。 Cache注解 从以上的注解中可以看出，虽然使用注解的确方便，但是缺少灵活的缓存策略， 缓存策略： TTL（Time To Live ） 存活期，即从缓存中创建时间点开始直到它到期的一个时间段（不管在这个时间段内有没有访问都将过期） TTI（Time To Idle） 空闲期，即一个数据多久没被访问将从缓存中移除的时间 项目中可能有很多缓存的TTL不相同，这时候就需要编码式使用编写缓存。 条件缓存根据运行流程，如下@Cacheable将在执行方法之前( #result还拿不到返回值)判断condition，如果返回true，则查缓存； 12@Cacheable(value = "user", key = "#id", condition = "#id lt 10") public User conditionFindById(final Long id) 如下@CachePut将在执行完方法后（#result就能拿到返回值了）判断condition，如果返回true，则放入缓存 12@CachePut(value = "user", key = "#id", condition = "#result.username ne 'liu'") public User conditionSave(final User user) 如下@CachePut将在执行完方法后（#result就能拿到返回值了）判断unless，如果返回false，则放入缓存；（即跟condition相反） 12@CachePut(value = "user", key = "#user.id", unless = "#result.username eq 'liu'") public User conditionSave2(final User user) 如下@CacheEvict， beforeInvocation=false表示在方法执行之后调用（#result能拿到返回值了）；且判断condition，如果返回true，则移除缓存； 12@CacheEvict(value = "user", key = "#user.id", beforeInvocation = false, condition = "#result.username ne 'liu'") public User conditionDelete(final User user) 小试牛刀，综合运用： 12345678910111213141516171819202122232425262728293031323334@CachePut(value = "user", key = "#user.id")public User save(User user) &#123; users.add(user); return user;&#125;@CachePut(value = "user", key = "#user.id")public User update(User user) &#123; users.remove(user); users.add(user); return user;&#125;@CacheEvict(value = "user", key = "#user.id")public User delete(User user) &#123; users.remove(user); return user;&#125;@CacheEvict(value = "user", allEntries = true)public void deleteAll() &#123; users.clear();&#125;@Cacheable(value = "user", key = "#id")public User findById(final Long id) &#123; System.out.println("cache miss, invoke find by id, id:" + id); for (User user : users) &#123; if (user.getId().equals(id)) &#123; return user; &#125; &#125; return null;&#125; 配置ehcache与redis spring cache集成ehcache，spring-ehcache.xml主要内容： 12345&lt;dependency&gt; &lt;groupId&gt;net.sf.ehcache&lt;/groupId&gt; &lt;artifactId&gt;ehcache-core&lt;/artifactId&gt; &lt;version&gt;$&#123;ehcache.version&#125;&lt;/version&gt;&lt;/dependency&gt; 1234567891011121314&lt;!-- Spring提供的基于的Ehcache实现的缓存管理器 --&gt; &lt;!-- 如果有多个ehcacheManager要在bean加上p:shared="true" --&gt;&lt;bean id="ehcacheManager" class="org.springframework.cache.ehcache.EhCacheManagerFactoryBean"&gt; &lt;property name="configLocation" value="classpath:xml/ehcache.xml"/&gt;&lt;/bean&gt; &lt;bean id="cacheManager" class="org.springframework.cache.ehcache.EhCacheCacheManager"&gt; &lt;property name="cacheManager" ref="ehcacheManager"/&gt; &lt;property name="transactionAware" value="true"/&gt;&lt;/bean&gt; &lt;!-- cache注解，和spring-redis.xml中的只能使用一个 --&gt;&lt;cache:annotation-driven cache-manager="cacheManager" proxy-target-class="true"/&gt; spring cache集成redis，spring-redis.xml主要内容： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt; &lt;version&gt;1.8.1.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-pool2&lt;/artifactId&gt; &lt;version&gt;2.4.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 注意需要添加Spring Data Redis等jar包 --&gt;&lt;description&gt;redis配置&lt;/description&gt;&lt;bean id="jedisPoolConfig" class="redis.clients.jedis.JedisPoolConfig"&gt; &lt;property name="maxIdle" value="$&#123;redis.pool.maxIdle&#125;"/&gt; &lt;property name="maxTotal" value="$&#123;redis.pool.maxActive&#125;"/&gt; &lt;property name="maxWaitMillis" value="$&#123;redis.pool.maxWait&#125;"/&gt; &lt;property name="testOnBorrow" value="$&#123;redis.pool.testOnBorrow&#125;"/&gt; &lt;property name="testOnReturn" value="$&#123;redis.pool.testOnReturn&#125;"/&gt;&lt;/bean&gt;&lt;!-- JedisConnectionFactory --&gt;&lt;bean id="jedisConnectionFactory" class="org.springframework.data.redis.connection.jedis.JedisConnectionFactory"&gt; &lt;property name="hostName" value="$&#123;redis.master.ip&#125;"/&gt; &lt;property name="port" value="$&#123;redis.master.port&#125;"/&gt; &lt;property name="poolConfig" ref="jedisPoolConfig"/&gt;&lt;/bean&gt;&lt;bean id="redisTemplate" class="org.springframework.data.redis.core.RedisTemplate" p:connectionFactory-ref="jedisConnectionFactory"&gt; &lt;property name="keySerializer"&gt; &lt;bean class="org.springframework.data.redis.serializer.JdkSerializationRedisSerializer"&gt;&lt;/bean&gt; &lt;/property&gt; &lt;property name="valueSerializer"&gt; &lt;bean class="org.springframework.data.redis.serializer.JdkSerializationRedisSerializer"/&gt; &lt;/property&gt; &lt;property name="hashKeySerializer"&gt; &lt;bean class="org.springframework.data.redis.serializer.JdkSerializationRedisSerializer"/&gt; &lt;/property&gt; &lt;property name="hashValueSerializer"&gt; &lt;bean class="org.springframework.data.redis.serializer.JdkSerializationRedisSerializer"/&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!--spring cache--&gt;&lt;bean id="cacheManager" class="org.springframework.data.redis.cache.RedisCacheManager" c:redisOperations-ref="redisTemplate"&gt; &lt;!-- 默认缓存10分钟 --&gt; &lt;property name="defaultExpiration" value="600"/&gt; &lt;property name="usePrefix" value="true"/&gt; &lt;!-- cacheName 缓存超时配置，半小时，一小时，一天 --&gt; &lt;property name="expires"&gt; &lt;map key-type="java.lang.String" value-type="java.lang.Long"&gt; &lt;entry key="halfHour" value="1800"/&gt; &lt;entry key="hour" value="3600"/&gt; &lt;entry key="oneDay" value="86400"/&gt; &lt;!-- shiro cache keys --&gt; &lt;entry key="authorizationCache" value="1800"/&gt; &lt;entry key="authenticationCache" value="1800"/&gt; &lt;entry key="activeSessionCache" value="1800"/&gt; &lt;/map&gt; &lt;/property&gt;&lt;/bean&gt;&lt;!-- cache注解，和spring-ehcache.xml中的只能使用一个 --&gt;&lt;cache:annotation-driven cache-manager="cacheManager" proxy-target-class="true"/&gt; 项目中注解缓存只能配置一个，所以可以通过以下引入哪个配置文件来决定使用哪个缓存。 12&lt;import resource="classpath:spring/spring-ehcache.xml"/&gt;&lt;!-- &lt;import resource="classpath:spring/spring-redis.xml"/&gt;--&gt; 当然，可以通过其他配置搭配使用两个缓存机制。比如ecache做一级缓存，redis做二级缓存。 好了所有spring的知识点总结就到这里结束了。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring-cache</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring框架学习笔记整理-aop篇]]></title>
    <url>%2Fblog%2F2017%2F08%2F07%2F2017-08-07-Spring%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-aop%E7%AF%87%2F</url>
    <content type="text"><![CDATA[什么是aopAOP（Aspect-OrientedProgramming，面向方面编程），可以说是OOP（Object-Oriented Programing，面向对象编程）的补充和完善。OOP允许你定义从上到下的关系，但并不适合定义从左到右的关系。例如日志功能。日志代码往往水平地散布在所有对象层次中，而与它所散布到的对象的核心功能毫无关系。这种散布在各处的无关的代码被称为横切（cross-cutting）代码，在OOP设计中，它导致了大量代码的重复，而不利于各个模块的重用。而AOP技术则恰恰相反，它利用一种称为“横切”的技术，剖解开封装的对象内部，并将那些影响了多个类的公共行为封装到一个可重用模块，并将其名为“Aspect”，即方面。所谓“方面”，简单地说，就是将那些与业务无关，却为业务模块所共同调用的逻辑或责任封装起来，便于减少系统的重复代码，降低模块间的耦合度，并有利于未来的可操作性和可维护性。 aop使用场景aop框架种类 AspectJ JBoss AOP Spring AOP 使用aop可以做的事情有很多。 性能监控，在方法调用前后记录调用时间，方法执行太长或超时报警。 缓存代理，缓存某方法的返回值，下次执行该方法时，直接从缓存里获取。 软件破解，使用AOP修改软件的验证类的判断逻辑。 记录日志，在方法执行前后记录系统日志。 工作流系统，工作流系统需要将业务代码和流程引擎代码混合在一起执行，那么我们可以使用AOP将其分离，并动态挂接业务。 权限验证，方法执行前验证是否有权限执行当前方法，没有则抛出没有权限执行异常，由业务代码捕捉。 观察一下传统编码方式与使用aop的区别： 核心概念描述AOP常用的一些术语有通知(Adivce)、切点（Pointcut）、连接点（Join point）、切面（Aspect）、引入（Introduction）、织入（Weaving）、通知（Advice）等。 简单例子相比xml配置，基于注解的方式更加简洁方便。 12345678910111213141516171819202122232425@Aspectpublic class TransactionDemo &#123; @Pointcut(value="execution(* cn.dijia478.core.service.*.*.*(..))") public void point()&#123; &#125; @Before(value="point()") public void before()&#123; System.out.println("transaction begin"); &#125; @AfterReturning(value = "point()") public void after()&#123; System.out.println("transaction commit"); &#125; @Around("point()") public void around(ProceedingJoinPoint joinPoint) throws Throwable&#123; System.out.println("transaction begin"); joinPoint.proceed(); System.out.println("transaction commit"); &#125;&#125; 在applicationContext.xml中配置。 12&lt;aop:aspectj-autoproxy /&gt;&lt;bean id = "transactionDemo" class = "cn.dijia478.core.transaction.TransactionDemo" /&gt; Spring aop原理通过前面介绍可以知道：AOP 代理其实是由 AOP 框架动态生成的一个对象，该对象可作为目标对象使用。AOP 代理包含了目标对象的全部方法，但 AOP 代理中的方法与目标对象的方法存在差异：AOP 方法在特定切入点添加了增强处理，并回调了目标对象的方法。 Spring 的 AOP 代理由 Spring 的 IoC 容器负责生成、管理，其依赖关系也由 IoC 容器负责管理。因此，AOP 代理可以直接使用容器中的其他 Bean 实例作为目标，这种关系可由 IoC 容器的依赖注入提供。 aop开发时，其中需要程序员参与的只有 3 个部分： 定义普通业务组件。 定义切入点，一个切入点可能横切多个业务组件。 定义增强处理，增强处理就是在 AOP 框架为普通业务组件织入的处理动作。 为了理清关系，先来个类关系图(找不到更清晰的了)。 两种动态代理方式Spring默认采取的动态代理机制实现AOP，当动态代理不可用时（代理类无接口）会使用CGlib机制。 Spring提供了两种方式来生成代理对象: JDKProxy和Cglib，具体使用哪种方式生成由AopProxyFactory根据AdvisedSupport对象的配置来决定。默认的策略是如果目标类是接口，则使用JDK动态代理技术，否则使用Cglib来生成代理。 JDK动态代理 JDK动态代理主要涉及到java.lang.reflect包中的两个类：Proxy和InvocationHandler。InvocationHandler是一个接口，通过实现该接口定义横切逻辑，并通过反射机制调用目标类的代码，动态将横切逻辑和业务逻辑编制在一起。 Proxy利用InvocationHandler动态创建一个符合某一接口的实例，生成目标类的代理对象。 CGLib动态代理 CGLib全称为Code Generation Library，是一个强大的高性能，高质量的代码生成类库，可以在运行期扩展Java类与实现Java接口，CGLib封装了asm，可以再运行期动态生成新的class。和JDK动态代理相比较：JDK创建代理有一个限制，就是只能为接口创建代理实例，而对于没有通过接口定义业务方法的类，则可以通过CGLib创建动态代理。 知识拓展通过上面的分析，大家是否有种熟悉的感觉，似乎和拦截器、过滤器的功能相似。那么问题来了，aop与拦截器、过滤器是什么关系。 先来回顾一下拦截器与过滤器。如下图一网友的测试，在web.xml中注册了TestFilter1和TestFilter2。然后在spring的配置文件中配置了BaseInterceptor和TestInterceptor。得到的结果如下图所示。从图中可以看出，拦截器和过滤器都横切了业务方法，看似符合aop的思想。 Filter过滤器：拦截web访问url地址。 Interceptor拦截器：拦截以 .action结尾的url，拦截Action的访问。 Spring AOP拦截器：只能拦截Spring管理Bean的访问（业务层Service） aop篇就写到这里，下篇博客将会写Spring cache的内容]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring-aop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring框架学习笔记整理-mvc篇]]></title>
    <url>%2Fblog%2F2017%2F07%2F17%2F2017-07-17-Spring%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-mvc%E7%AF%87%2F</url>
    <content type="text"><![CDATA[Spring MVC简介与运行原理Spring的模型-视图-控制器（MVC）框架是围绕一个DispatcherServlet来设计的，这个Servlet会把请求分发给各个处理器，并支持可配置的处理器映射、视图渲染、本地化、时区与主题渲染等，甚至还能支持文件上传。 (1) Http请求：客户端请求提交到DispatcherServlet。 (2) 寻找处理器：由DispatcherServlet控制器查询一个或多个HandlerMapping，找到处理请求的Controller。 (3) 调用处理器：DispatcherServlet将请求提交到Controller。 (4)(5)调用业务处理和返回结果：Controller调用业务逻辑处理后，返回ModelAndView。 (6)(7)处理视图映射并返回模型： DispatcherServlet查询一个或多个ViewResoler视图解析器，找到ModelAndView指定的视图。 (8) Http响应：视图负责将结果显示到客户端 Spring MVC的主要注解 ContextLoaderListener在讲ContextLoaderListener之前，首先来了解一下web.xml的作用。 一个web项目中可以没有web.xml文件，也就是说，web.xml文件并不是web工程必须的。web.xml文件是用来初始化配置信息：比如Welcome页面、servlet、servlet-mapping、filter、listener、启动加载级别等。当你的web工程没用到这些时，你可以不用web.xml文件来配置你的Application。 当要启动某个web项目时，服务器软件或容器如（tomcat）会第一步加载项目中的web.xml文件，通过其中的各种配置来启动项目，只有其中配置的各项均无误时，项目才能正确启动。web.xml有多项标签，在其加载的过程中顺序依次为：context-param &gt;&gt; listener &gt;&gt; fileter &gt;&gt; servlet。（同类多个节点以出现顺序依次加载） 而Spring MVC启动过程大致分为两个过程： ContextLoaderListener初始化，实例化IOC容器，并将此容器实例注册到ServletContext中。 DispatcherServlet初始化。 其中ContextLoaderListener监听器它实现了ServletContextListener这个接口，在web.xml配置这个监听器，启动容器时，就会默认执行它实现的方法。在ContextLoaderListener中关联了ContextLoader这个类，所以整个加载配置过程由ContextLoader来完成。 ContextLoaderListener在web.xml中的配置： 12345678910&lt;!-- 配置contextConfigLocation初始化参数 --&gt;&lt;context-param&gt; &lt;param-name&gt;contextConfigLocation&lt;/param-name&gt; &lt;param-value&gt;/WEB-INF/applicationContext.xml&lt;/param-value&gt;&lt;/context-param&gt;&lt;!-- 配置ContextLoaderListerner --&gt;&lt;listener&gt; &lt;listener-class&gt;org.springframework.web.context.ContextLoaderListener&lt;/listener-class&gt;&lt;/listener&gt; ServletContextListener 接口有两个方法：contextInitialized和contextDestroyed DispatcherServletSpring MVC框架，与其他很多web的MVC框架一样：请求驱动；所有设计都围绕着一个中央Servlet来展开，它负责把所有请求分发到控制器；同时提供其他web应用开发所需要的功能。不过Spring的中央处理器，DispatcherServlet，能做的比这更多。 下图展示了Spring Web MVC的DispatcherServlet处理请求的工作流。熟悉设计模式的朋友会发现，DispatcherServlet应用的其实就是一个“前端控制器”的设计模式（其他很多优秀的web框架也都使用了这个设计模式）。 流程图 DispatcherServlet在web.xml中的配置 1234567891011&lt;!-- servlet定义 --&gt;&lt;servlet&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;servlet-class&gt;org.springframework.web.servlet.DispatcherServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;1&lt;/load-on-startup&gt;&lt;/servlet&gt; &lt;servlet-mapping&gt; &lt;servlet-name&gt;dispatcher&lt;/servlet-name&gt; &lt;url-pattern&gt;/&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 其中 load-on-startup：表示启动容器时初始化该Servlet； url-pattern：表示哪些请求交给Spring Web MVC处理， / 是用来定义默认servlet映射的。也可以如*.html表示拦截所有以html为扩展名的请求。 在Spring MVC中，每个DispatcherServlet都持有一个自己的上下文对象WebApplicationContext，它又继承了根（root）WebApplicationContext对象中已经定义的所有bean。这些继承的bean可以在具体的Servlet实例中被重载，在每个Servlet实例中你也可以定义其scope下的新bean。 WebApplicationContext继承自ApplicationContext，它提供了一些web应用经常需要用到的特性。它与普通的ApplicationContext不同的地方在于，它支持主题的解析，并且知道它关联到的是哪个servlet（它持有一个该ServletContext的引用） Spring MVC同时提供了很多特殊的注解，用于处理请求和渲染视图等。DispatcherServlet初始化的过程中会默认使用这些特殊bean进行配置。如果你想指定使用哪个特定的bean，你可以在web应用上下文WebApplicationContext中简单地配置它们。 其中，常用的ViewResolver的配置。以jsp作为视图为例 12345&lt;!-- 对模型视图名称的解析,即在模型视图名称添加前后缀 --&gt;&lt;bean class="org.springframework.web.servlet.view.InternalResourceViewResolver"&gt; &lt;property name="prefix" value="/WEB-INF/jsp/" /&gt; &lt;property name="suffix" value=".jsp" /&gt;&lt;/bean&gt; 配置上传文件限制MultipartResolver 12345&lt;!-- 上传限制 --&gt;&lt;bean id="multipartResolver" class="org.springframework.web.multipart.commons.CommonsMultipartResolver"&gt; &lt;!-- 上传文件大小限制为31M，31*1024*1024 --&gt; &lt;property name="maxUploadSize" value="32505856"/&gt;&lt;/bean&gt; applicationContext.xml配置文件中的标签 文件上传前面说到DispatcherServlet中有个特殊的Bean叫MultipartResolver，可用于限制文件的上传大小等。当解析器MultipartResolver完成处理时，请求便会像其他请求一样被正常流程处理。 表单 12345&lt;form method="post" action="/form" enctype="multipart/form-data"&gt; &lt;input type="text" name="name"/&gt; &lt;input type="file" name="file"/&gt; &lt;input type="submit"/&gt;&lt;/form&gt; 控制器 1234567891011@RequestMapping(path = "/form", method = RequestMethod.POST) public String handleFormUpload(@RequestParam("name") String name, @RequestParam("file") MultipartFile file) &#123; if (!file.isEmpty()) &#123; byte[] bytes = file.getBytes(); // store the bytes somewhere return "redirect:uploadSuccess"; &#125; return "redirect:uploadFailure";&#125; 异常处理先来说下常见的异常处理有几种方式，如下图： Spring的处理器异常解析器HandlerExceptionResolver接口的实现负责处理各类控制器执行过程中出现的异常。也是上面提到的，是DispatcherServlet中的特殊bean，可以自定义配置处理。 某种程度上讲，HandlerExceptionResolver与你在web应用描述符web.xml文件中能定义的异常映射（exception mapping）很相像，不过它比后者提供了更灵活的方式。比如它能提供异常被抛出时正在执行的是哪个处理器这样的信息。 HandlerExceptionResolver 提供resolveException接口 1234public interface HandlerExceptionResolver &#123; ModelAndView resolveException( HttpServletRequest request, HttpServletResponse response, Object handler, Exception ex); &#125; 在BaseController中使用 @ExceptionHandler注解处理异常 123456789101112131415161718192021222324252627282930313233343536373839@ExceptionHandler(Exception.class)public Object exceptionHandler(Exception ex, HttpServletResponse response, HttpServletRequest request) throws IOException &#123; String url = ""; String msg = ex.getMessage(); Object resultModel = null; try &#123; if (ex.getClass() == HttpRequestMethodNotSupportedException.class) &#123; url = "admin/common/500"; System.out.println("--------没有找到对应方法---------"); &#125; else if (ex.getClass() == ParameterException.class) &#123;// 自定义的异常 &#125; else if (ex.getClass() == UnauthorizedException.class) &#123; url = "admin/common/unauth"; System.out.println("--------没有权限---------"); &#125; String header = req.getHeader("X-Requested-With"); boolean isAjax = "XMLHttpRequest".equalsIgnoreCase(header); String method = req.getMethod(); boolean isPost = "POST".equalsIgnoreCase(method); if (isAjax || isPost) &#123; return Message.error(msg); &#125; else &#123; ModelAndView view = new ModelAndView(url); view.addObject("error", msg); view.addObject("class", ex.getClass()); view.addObject("method", request.getRequestURI()); return view; &#125; &#125; catch (Exception exception) &#123; logger.error(exception.getMessage(), exception); return resultModel; &#125; finally &#123; logger.error(msg, ex); ex.printStackTrace(); &#125;&#125; 在web.xml中处理异常 12345678910111213141516171819202122232425262728&lt;!-- 默认的错误处理页面 --&gt;&lt;error-page&gt; &lt;error-code&gt;403&lt;/error-code&gt; &lt;location&gt;/403.html&lt;/location&gt;&lt;/error-page&gt;&lt;error-page&gt; &lt;error-code&gt;404&lt;/error-code&gt; &lt;location&gt;/404.html&lt;/location&gt;&lt;/error-page&gt;&lt;!-- 仅仅在调试的时候注视掉,在正式部署的时候不能注释 --&gt;&lt;!-- 这样配置也是可以的，表示发生500错误的时候，转到500.jsp页面处理。 --&gt;&lt;error-page&gt; &lt;error-code&gt;500&lt;/error-code&gt; &lt;location&gt;/500.html&lt;/location&gt; &lt;/error-page&gt; &lt;!-- 这样的配置表示如果jsp页面或者servlet发生java.lang.Exception类型（当然包含子类）的异常就会转到500.jsp页面处理。 --&gt;&lt;error-page&gt; &lt;exception-type&gt;java.lang.Exception&lt;/exception-type&gt; &lt;location&gt;/500.jsp&lt;/location&gt; &lt;/error-page&gt; &lt;error-page&gt; &lt;exception-type&gt;java.lang.Throwable&lt;/exception-type&gt; &lt;location&gt;/500.jsp&lt;/location&gt; &lt;/error-page&gt;&lt;!-- 当error-code和exception-type都配置时，exception-type配置的页面优先级高及出现500错误，发生异常Exception时会跳转到500.jsp--&gt; 来一个问题：HandlerExceptionResolver和web.xml中配置的error-page会有冲突吗？ 解答：如果resolveException返回了ModelAndView，会优先根据返回值中的页面来显示。不过，resolveException可以返回null，此时则展示web.xml中的error-page的500状态码配置的页面。 当web.xml中有相应的error-page配置，则可以在实现resolveException方法时返回null。 API文档中对返回值的解释：return a corresponding ModelAndView to forward to, or null for default processing. OK，关于Spring MVC就先到这里，下篇博客将会整理Spring aop的内容]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>SpringMVC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring框架学习笔记整理-ioc篇]]></title>
    <url>%2Fblog%2F2017%2F05%2F18%2F2017-05-18-Spring%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-ioc%E7%AF%87%2F</url>
    <content type="text"><![CDATA[学过java的都知道：所有的对象都必须创建；或者说：使用对象之前必须先创建。而使用ioc之后，你就可以不再手动创建对象，而是从ioc容器中直接获取对象。 就好像我们无需考虑对象的销毁回收一样，因为java垃圾回收机制帮助我们实现了这个过程；而ioc则是让我们无需考虑对象的创建过程，由ioc容器帮我们实现对象的创建、注入等过程。 控制反转 Spring ioc容器在Spring框架中的核心组件只有三个：Core、Context和Bean。它们构建起了整个Spring的骨骼架构，没有它们就不可能有AOP、Web等特性功能 如果说在三个核心中再选出一个核心，那就非Bean莫属了。可以说，Spring就是面向Bean的编程，Bean在Spring中才是真正的主角。 Spring为何如此流行？你会发现Spring解决了一个非常关键的问题，它可以让你对对象之间的关系转而用配置文件来管理，或者注解，也就是它的依赖注入机制。而这个注入关系在一个叫ioc的容器中管理。ioc容器就是被Bean包裹的对象。Spring正是通过把对象包装在Bean中从而达到管理这些对象及做一些列额外操作的目的。 三大核心组件协同工作BeanSpring的bean实例。身份是演员。 ContextSpring的上下文。身份是导演。 我们知道Bean包装的是Object，而Object必然有数据，如何给这些数据提供生存环境就是Context要解决的问题，对Context来说，他就是要发现每个Bean之间的关系，为他们建立这种关系并且要维护好这种关系。所以Context就是一个Bean关系的集合，这个关系集合又叫ioc容器，一旦建立起这个ioc容器后，Spring就可以为你工作了。 CoreSpring的核心工具包。身份是道具。 建立和维护每个Bean之间的关系所需要的一系列核心工具包。其实就相当于Util包。 BeanFactory与ApplacationContext的区别IOC中最核心的接口是Beanfactory提供IOC的高级服务，而ApplicationContext是建立在BeanFactory基础之上提供抽象的面向应用的服务。 三种注入方式在Spring框架中，依赖注入(DI)的设计模式是用来定义对象彼此间的依赖。使用xml配置bean的情况下，它主要有两种类型： Setter方法注入 构造器注入 当然，有了注解之后，使用注解的方式更加方便快捷。即自动装配功能实现属性自动注入（@Autowire）。 写到这里，让我想起了最近在牛客网上看的一道关于spring的选择题了： 下面有关spring的依赖注入，说法错误的是？A.依赖注入通常有如下两种：设置注入和构造注入B.构造注入可以在构造器中决定依赖关系的注入顺序，优先依赖的优先注入C.当设值注入与构造注入同时存在时，先执行设值注入，再执行构造注入D.设值注入是指IoC容器使用属性的setter方法来注入被依赖的实例。这种注入方式比较简单、直观 牛客网给出的答案是选C，应该是先执行构造注入，后执行设置注入。查看网友评论及答案 Spring原理解析Spring的代码还真是不好读，分得太细了，文字也是难以描述出来，看了别人有关的博客，贴了好多代码，画了好多ER图来描述关键接口或类之间的关系。这么一篇这么长文章下来，大家也未必会认真读代码，看ER图，干脆也不跟风了。就贴了一点在我看来比较关键的代码，嘿嘿。 context的初始化过程当运行ApplicationContext ctx = new ClassPathXmlApplicationContext(&quot;applicationContext.xml&quot;); 时，构造方法ClassPathXmlApplicationContext(String configLocation)调用了this(new String[] {configLocation}, true, null);， 该构造方法具体代码如下。 从时序图来看启动上述初始化 好了，ioc容器篇就简单总结到这里，下一篇会整理下Spring MVC的相关内容]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring-ioc</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Spring框架学习笔记整理-概述]]></title>
    <url>%2Fblog%2F2017%2F05%2F04%2F2017-05-04-Spring%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E6%A6%82%E8%BF%B0%2F</url>
    <content type="text"><![CDATA[关于SpringSpring 是一个开源的Java／Java EE全功能栈的应用程序框架，他解决的是业务逻辑层和其他各层的松耦合问题，因此它将面向接口的编程思想贯穿整个系统应用。它的主要优势之一就是其分层架构，分层架构允许您选择使用哪一个组件，同时为 Java EE 应用程序开发提供集成的框架。 特点： Sping架构Spring框架是分模块存在，除了最核心的Spring Core Container(即Spring容器)是必要模块之外，其他模块都是可选，视需要而定。大约有20多个模块。 Spring3与Spring4是有区别的，4.0主要是对Java 8的新函数式语法进行支持，还有加强了对网络各种新技术比如http-streaming, websocket的更好的支持。 一般来说，Spring主要分为7个模块： Spring的主要jar包 Spring的常用注解bean注入与装配的的方式有很多种，可以通过xml，getset方式，构造函数或者注解等。简单易用的方式就是使用Spring的注解了，Spring提供了大量的注解方式，让项目阅读和开发起来更加方便。 常用注解 装配注解比较 第三方框架集成Spring框架的开发不是为了替代现有的优秀第三方框架，而是通过集成的方式把它们都连接起来。下面总结了一些常集成的优秀框架。 最后本篇博客简单总结了下Spring是什么，没涉及到原理的东西。后面还会整理4篇笔记出来。分为ioc篇，mvc篇，aop篇和cache篇。]]></content>
      <categories>
        <category>Spring</category>
      </categories>
      <tags>
        <tag>Spring概述</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis框架学习笔记整理-下篇]]></title>
    <url>%2Fblog%2F2017%2F04%2F21%2F2017-04-21-MyBatis%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E4%B8%8B%E7%AF%87%2F</url>
    <content type="text"><![CDATA[上一篇博客写了MyBatis的基本原理和配置文件的基本使用，这一篇写MyBatis的使用，主要包括与Sping集成、动态sql、还有mapper的xml文件一下复杂配置等。值得注意的是，思维导图”resultMap子元素”中讲解的时候可能讲得不是很清楚，应该需要大量的代码来讲解使用方法，思维导图恰恰不适合这种表现方法。所以需要根据给出的链接去读一些好的博客文章，这样才能更好理解。毕竟是技术性的东西，实践出真理。 MyBatis简介MyBatis 是支持普通 SQL查询，存储过程和高级映射的优秀持久层框架。MyBatis 消除了几乎所有的JDBC代码和参数的手工设置以及结果集的检索。MyBatis 使用简单的 XML或注解用于配置和原始映射，将接口和 Java 的POJOs（Plain Old Java Objects，普通的 Java对象）映射成数据库中的记录。 MyBatis的优缺点 与Spring整合 mapper的xml文件 官方参考文档 属性值可以用于包含的refid属性或者包含的字句里面的属性值1234567891011121314151617&lt;sql id="sometable"&gt; $&#123;prefix&#125;Table&lt;/sql&gt;&lt;sql id="someinclude"&gt; from &lt;include refid="$&#123;include_target&#125;"/&gt;&lt;/sql&gt;&lt;select id="select" resultType="map"&gt; select field1, field2, field3 &lt;include refid="someinclude"&gt; &lt;property name="prefix" value="Some"/&gt; &lt;property name="include_target" value="sometable"/&gt; &lt;/include&gt;&lt;/select&gt; 高级结果映射MyBatis 创建的一个想法:数据库不用永远是你想要的或需要它们是什么样的。而我们 最喜欢的数据库最好是第三范式或 BCNF 模式,但它们有时不是。如果可能有一个单独的 数据库映射,所有应用程序都可以使用它,这是非常好的,但有时也不是。结果映射就是 MyBatis 提供处理这个问题的答案。 比如,我们如何映射下面这个语句? 12345678910111213141516171819202122232425262728293031323334&lt;!-- Very Complex Statement --&gt;&lt;select id="selectBlogDetails" resultMap="detailedBlogResultMap"&gt; select B.id as blog_id, B.title as blog_title, B.author_id as blog_author_id, A.id as author_id, A.username as author_username, A.password as author_password, A.email as author_email, A.bio as author_bio, A.favourite_section as author_favourite_section, P.id as post_id, P.blog_id as post_blog_id, P.author_id as post_author_id, P.created_on as post_created_on, P.section as post_section, P.subject as post_subject, P.draft as draft, P.body as post_body, C.id as comment_id, C.post_id as comment_post_id, C.name as comment_name, C.comment as comment_text, T.id as tag_id, T.name as tag_name from Blog B left outer join Author A on B.author_id = A.id left outer join Post P on B.id = P.blog_id left outer join Comment C on P.id = C.post_id left outer join Post_Tag PT on PT.post_id = P.id left outer join Tag T on PT.tag_id = T.id where B.id = #&#123;id&#125;&lt;/select&gt; 你可能想把它映射到一个智能的对象模型,包含一个作者写的博客,有很多的博文,每 篇博文有零条或多条的评论和标签。 下面是一个完整的复杂结果映射例子 (假设作者, 博客, 博文, 评论和标签都是类型的别名) 我们来看看, 。 但是不用紧张, 我们会一步一步来说明。 当天最初它看起来令人生畏,但实际上非常简单。 1234567891011121314151617181920212223242526272829&lt;!-- Very Complex Result Map --&gt;&lt;resultMap id="detailedBlogResultMap" type="Blog"&gt; &lt;constructor&gt; &lt;idArg column="blog_id" javaType="int"/&gt; &lt;/constructor&gt; &lt;result property="title" column="blog_title"/&gt; &lt;association property="author" javaType="Author"&gt; &lt;id property="id" column="author_id"/&gt; &lt;result property="username" column="author_username"/&gt; &lt;result property="password" column="author_password"/&gt; &lt;result property="email" column="author_email"/&gt; &lt;result property="bio" column="author_bio"/&gt; &lt;result property="favouriteSection" column="author_favourite_section"/&gt; &lt;/association&gt; &lt;collection property="posts" ofType="Post"&gt; &lt;id property="id" column="post_id"/&gt; &lt;result property="subject" column="post_subject"/&gt; &lt;association property="author" javaType="Author"/&gt; &lt;collection property="comments" ofType="Comment"&gt; &lt;id property="id" column="comment_id"/&gt; &lt;/collection&gt; &lt;collection property="tags" ofType="Tag" &gt; &lt;id property="id" column="tag_id"/&gt; &lt;/collection&gt; &lt;discriminator javaType="int" column="draft"&gt; &lt;case value="1" resultType="DraftPost"/&gt; &lt;/discriminator&gt; &lt;/collection&gt;&lt;/resultMap&gt; resultMap子元素概念视图1234567891011121314151617181920212223242526272829&lt;!-- Very Complex Result Map --&gt;&lt;resultMap id="detailedBlogResultMap" type="Blog"&gt; &lt;constructor&gt; &lt;idArg column="blog_id" javaType="int"/&gt; &lt;/constructor&gt; &lt;result property="title" column="blog_title"/&gt; &lt;association property="author" javaType="Author"&gt; &lt;id property="id" column="author_id"/&gt; &lt;result property="username" column="author_username"/&gt; &lt;result property="password" column="author_password"/&gt; &lt;result property="email" column="author_email"/&gt; &lt;result property="bio" column="author_bio"/&gt; &lt;result property="favouriteSection" column="author_favourite_section"/&gt; &lt;/association&gt; &lt;collection property="posts" ofType="Post"&gt; &lt;id property="id" column="post_id"/&gt; &lt;result property="subject" column="post_subject"/&gt; &lt;association property="author" javaType="Author"/&gt; &lt;collection property="comments" ofType="Comment"&gt; &lt;id property="id" column="comment_id"/&gt; &lt;/collection&gt; &lt;collection property="tags" ofType="Tag" &gt; &lt;id property="id" column="tag_id"/&gt; &lt;/collection&gt; &lt;discriminator javaType="int" column="draft"&gt; &lt;case value="1" resultType="DraftPost"/&gt; &lt;/discriminator&gt; &lt;/collection&gt;&lt;/resultMap&gt; resultMap子元素 官方参考文档 关联的嵌套查询我们有两个查询语句:一个来加载博客,另外一个来加载作者,而且博客的结果映射描 述了“selectAuthor”语句应该被用来加载它的 author 属性。 其他所有的属性将会被自动加载,假设它们的列和属性名相匹配。 这种方式很简单, 但是对于大型数据集合和列表将不会表现很好。 问题就是我们熟知的 “N+1 查询问题”。概括地讲,N+1 查询问题可以是这样引起的: 你执行了一个单独的 SQL 语句来获取结果列表(就是“+1”)。对返回的每条记录,你执行了一个查询语句来为每个加载细节(就是“N”)。这个问题会导致成百上千的 SQL 语句被执行。这通常不是期望的。 MyBatis 能延迟加载这样的查询就是一个好处,因此你可以分散这些语句同时运行的消 耗。然而,如果你加载一个列表,之后迅速迭代来访问嵌套的数据,你会调用所有的延迟加 载,这样的行为可能是很糟糕的。 元素集合collection 鉴别器discriminator 例子1234567891011121314151617181920212223&lt;resultMap id="vehicleResult" type="Vehicle"&gt; &lt;id property="id" column="id" /&gt; &lt;result property="vin" column="vin"/&gt; &lt;result property="year" column="year"/&gt; &lt;result property="make" column="make"/&gt; &lt;result property="model" column="model"/&gt; &lt;result property="color" column="color"/&gt; &lt;discriminator javaType="int" column="vehicle_type"&gt; &lt;case value="1" resultType="carResult"&gt; &lt;result property="doorCount" column="door_count" /&gt; &lt;/case&gt; &lt;case value="2" resultType="truckResult"&gt; &lt;result property="boxSize" column="box_size" /&gt; &lt;result property="extendedCab" column="extended_cab" /&gt; &lt;/case&gt; &lt;case value="3" resultType="vanResult"&gt; &lt;result property="powerSlidingDoor" column="power_sliding_door" /&gt; &lt;/case&gt; &lt;case value="4" resultType="suvResult"&gt; &lt;result property="allWheelDrive" column="all_wheel_drive" /&gt; &lt;/case&gt; &lt;/discriminator&gt;&lt;/resultMap&gt; 动态 SQL #{}和${}的区别]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyBatis框架学习笔记整理-上篇]]></title>
    <url>%2Fblog%2F2017%2F04%2F16%2F2017-04-16-MyBatis%E6%A1%86%E6%9E%B6%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86-%E4%B8%8A%E7%AF%87%2F</url>
    <content type="text"><![CDATA[前言与Hibernate相比，我无疑更喜欢MyBatis，就因为我觉得它真的好用，哈哈。它简单上手和掌握；sql语句和代码分开，方便统一管理和优化；当然缺点也有：sql工作量很大，尤其是字段多、关联表多时，更是如此。而且sql依赖于数据库，导致数据库移植性差。 选用一个框架之前最好先了解它的优缺点，对项目最好用，效率最高的才是一个你最好的选择。本次决定使用XMind思维导图来总结，会更为直观。如果图片看不清，可以右键在新标签页打开或者保存到本地。一共分为上下两篇来总结MyBatis的相关知识点。本篇为上篇，主要写一些MyBatis的原理和配置文件的基本实用，后一篇会写MyBatis的使用方法。 MyBaits简介MyBatis 是一款优秀的持久层框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生信息，将接口和 Java 的 POJOs(Plain Old Java Objects,普通的 Java对象)映射成数据库中的记录。 官方参考文档 与Hibernate的对比 两者的缓存机制异同 相同点 Hibernate和MyBatis的二级缓存除了采用系统默认的缓存机制外，都可以通过实现你自己的缓存或为其他第三方缓存方案，创建适配器来完全覆盖缓存行为。 不同点 Hibernate的二级缓存配置在SessionFactory生成的配置文件中进行详细配置，然后再在具体的表-对象映射中配置是那种缓存。 MyBatis的二级缓存配置都是在每个具体的表-对象映射中进行详细配置，这样针对不同的表可以自定义不同的缓存机制。并且MyBatis可以在命名空间中共享相同的缓存配置和实例，通过Cache-ref来实现。 两者比较 因为Hibernate对查询对象有着良好的管理机制，用户无需关心SQL。所以在使用二级缓存时如果出现脏数据，系统会报出错误并提示。 而MyBatis在这一方面，使用二级缓存时需要特别小心。如果不能完全确定数据更新操作的波及范围，避免Cache的盲目使用。否则，脏数据的出现会给系统的正常运行带来很大的隐患。 Hibernate的内置和外置缓存内置缓存中存放的是SessionFactory对象的一些集合属性包含的数据(映射元素据及预定SQL语句等),对于应用程序来说,它是只读的。 外置缓存中存放的是数据库数据的副本,其作用和一级缓存类似.二级缓存除了以内存作为存储介质外,还可以选用硬盘等外部存储设备。二级缓存称为进程级缓存或SessionFactory级缓存，它可以被所有session共享，它的生命周期伴随着SessionFactory的生命周期存在和消亡。 MyBatis的几个关键类 MyBatis的执行浅析 参考文章： MyBatis原理深入解析 MyBatis框架整体设计 MyBatis初始化与执行sql过程 MyBatis源码的几个主要部件 XML 映射配置文件 官方参考文档 自定义类型处理器使用这个的类型处理器将会覆盖已经存在的处理 Java 的 String 类型属性和 VARCHAR 参数及结果的类型处理器。 要注意 MyBatis 不会窥探数据库元信息来决定使用哪种类型，所以你必须在参数和结果映射中指明那是 VARCHAR 类型的字段， 以使其能够绑定到正确的类型处理器上。 这是因为：MyBatis 直到语句被执行才清楚数据类型。 通过类型处理器的泛型，MyBatis 可以得知该类型处理器处理的 Java 类型，不过这种行为可以通过两种方法改变： 在类型处理器的配置元素（typeHandler element）上增加一个 javaType 属性（比如：javaType=”String”）；在类型处理器的类上（TypeHandler class）增加一个 @MappedTypes 注解来指定与其关联的 Java 类型列表。 如果在 javaType 属性中也同时指定，则注解方式将被忽略。可以通过两种方式来指定被关联的 JDBC 类型： 在类型处理器的配置元素上增加一个 jdbcType 属性（比如：jdbcType=”VARCHAR”）； 在类型处理器的类上（TypeHandler class）增加一个 @MappedJdbcTypes 注解来指定与其关联的 JDBC 类型列表。 如果在 jdbcType 属性中也同时指定，则注解方式将被忽略。 当决定在ResultMap中使用某一TypeHandler时，此时java类型是已知的（从结果类型中获得），但是JDBC类型是未知的。 因此Mybatis使用javaType=[TheJavaType], jdbcType=null的组合来选择一个TypeHandler。 这意味着使用@MappedJdbcTypes注解可以限制TypeHandler的范围，同时除非显示的设置，否则TypeHandler在ResultMap中将是无效的。 如果希望在ResultMap中使用TypeHandler，那么设置@MappedJdbcTypes注解的includeNullJdbcType=true即可。 然而从Mybatis 3.4.0开始，如果只有一个注册的TypeHandler来处理Java类型，那么它将是ResultMap使用Java类型时的默认值（即使没有includeNullJdbcType=true）。 最后，可以让 MyBatis 为你查找类型处理器： 1234&lt;!-- mybatis-config.xml --&gt;&lt;typeHandlers&gt; &lt;package name="org.mybatis.example"/&gt;&lt;/typeHandlers&gt; 注意在使用自动检索（autodiscovery）功能的时候，只能通过注解方式来指定 JDBC 的类型。 你能创建一个泛型类型处理器，它可以处理多于一个类。为达到此目的， 需要增加一个接收该类作为参数的构造器，这样在构造一个类型处理器的时候 MyBatis 就会传入一个具体的类。 12345678910// GenericTypeHandler.javapublic class GenericTypeHandler&lt;E extends MyObject&gt; extends BaseTypeHandler&lt;E&gt; &#123; private Class&lt;E&gt; type; public GenericTypeHandler(Class&lt;E&gt; type) &#123; if (type == null) throw new IllegalArgumentException("Type argument cannot be null"); this.type = type; &#125; ... 处理枚举类型若想映射枚举类型 Enum，则需要从 EnumTypeHandler 或者 EnumOrdinalTypeHandler 中选一个来使用。 比如说我们想存储取近似值时用到的舍入模式。默认情况下，MyBatis 会利用 EnumTypeHandler 来把 Enum 值转换成对应的名字。 注意 EnumTypeHandler 在某种意义上来说是比较特别的，其他的处理器只针对某个特定的类，而它不同，它会处理任意继承了 Enum 的类。不过，我们可能不想存储名字，相反我们的 DBA 会坚持使用整形值代码。那也一样轻而易举： 在配置文件中把 EnumOrdinalTypeHandler 加到 typeHandlers 中即可， 这样每个 RoundingMode 将通过他们的序数值来映射成对应的整形。 1234&lt;!-- mybatis-config.xml --&gt;&lt;typeHandlers&gt; &lt;typeHandler handler="org.apache.ibatis.type.EnumOrdinalTypeHandler" javaType="java.math.RoundingMode"/&gt;&lt;/typeHandlers&gt; 但是怎样能将同样的 Enum 既映射成字符串又映射成整形呢？ 自动映射器（auto-mapper）会自动地选用 EnumOrdinalTypeHandler 来处理， 所以如果我们想用普通的 EnumTypeHandler，就非要为那些 SQL 语句显式地设置要用到的类型处理器不可。 databaseIdProviderMyBatis 可以根据不同的数据库厂商执行不同的语句，这种多厂商的支持是基于映射语句中的 databaseId 属性。 MyBatis 会加载不带 databaseId 属性和带有匹配当前数据库 databaseId 属性的所有语句。 如果同时找到带有 databaseId 和不带 databaseId 的相同语句，则后者会被舍弃。 为支持多厂商特性只要像下面这样在 mybatis-config.xml 文件中加入 databaseIdProvider 即可： 1&lt;databaseIdProvider type="DB_VENDOR" /&gt; 这里的 DB_VENDOR 会通过 DatabaseMetaData#getDatabaseProductName() 返回的字符串进行设置。 由于通常情况下这个字符串都非常长而且相同产品的不同版本会返回不同的值，所以最好通过设置属性别名来使其变短，如下： 12345&lt;databaseIdProvider type="DB_VENDOR"&gt; &lt;property name="SQL Server" value="sqlserver"/&gt; &lt;property name="DB2" value="db2"/&gt; &lt;property name="Oracle" value="oracle" /&gt;&lt;/databaseIdProvider&gt; 在有 properties 时，DB_VENDOR databaseIdProvider 的将被设置为第一个能匹配数据库产品名称的属性键对应的值，如果没有匹配的属性将会设置为 “null”。 在这个例子中，如果 getDatabaseProductName() 返回“Oracle (DataDirect)”，databaseId 将被设置为“oracle”。 你可以通过实现接口 org.apache.ibatis.mapping.DatabaseIdProvider 并在 mybatis-config.xml 中注册来构建自己的 DatabaseIdProvider： 1234public interface DatabaseIdProvider &#123; void setProperties(Properties p); String getDatabaseId(DataSource dataSource) throws SQLException;&#125; dataSource数据源（dataSource） dataSource 元素使用标准的 JDBC 数据源接口来配置 JDBC 连接对象的资源。 许多 MyBatis 的应用程序将会按示例中的例子来配置数据源。然而它并不是必须的。要知道为了方便使用延迟加载，数据源才是必须的。有三种内建的数据源类型（也就是 type=”[UNPOOLED|POOLED|JNDI]”）： UNPOOLED– 这个数据源的实现只是每次被请求时打开和关闭连接。虽然一点慢，它对在及时可用连接方面没有性能要求的简单应用程序是一个很好的选择。 不同的数据库在这方面表现也是不一样的，所以对某些数据库来说使用连接池并不重要，这个配置也是理想的。UNPOOLED 类型的数据源仅仅需要配置以下 5 种属性： driver – 这是 JDBC 驱动的 Java 类的完全限定名（并不是JDBC驱动中可能包含的数据源类）。 url – 这是数据库的 JDBC URL 地址。 username – 登录数据库的用户名。 password – 登录数据库的密码。 defaultTransactionIsolationLevel – 默认的连接事务隔离级别。 作为可选项，你也可以传递属性给数据库驱动。要这样做，属性的前缀为“driver.”，例如： 1driver.encoding=UTF8 这将通过DriverManager.getConnection(url,driverProperties)方法传递值为 UTF8 的 encoding 属性给数据库驱动。 POOLED– 这种数据源的实现利用“池”的概念将 JDBC 连接对象组织起来，避免了创建新的连接实例时所必需的初始化和认证时间。 这是一种使得并发 Web 应用快速响应请求的流行处理方式。 除了上述提到 UNPOOLED 下的属性外，会有更多属性用来配置 POOLED 的数据源： poolMaximumActiveConnections – 在任意时间可以存在的活动（也就是正在使用）连接数量，默认值：10poolMaximumIdleConnections – 任意时间可能存在的空闲连接数。poolMaximumCheckoutTime – 在被强制返回之前，池中连接被检出（checked out）时间，默认值：20000 毫秒（即 20 秒）poolTimeToWait – 这是一个底层设置，如果获取连接花费的相当长的时间，它会给连接池打印状态日志并重新尝试获取一个连接（避免在误配置的情况下一直安静的失败），默认值：20000 毫秒（即 20 秒）。poolPingQuery – 发送到数据库的侦测查询，用来检验连接是否处在正常工作秩序中并准备接受请求。默认是“NO PING QUERY SET”，这会导致多数数据库驱动失败时带有一个恰当的错误消息。poolPingEnabled – 是否启用侦测查询。若开启，也必须使用一个可执行的 SQL 语句设置 poolPingQuery 属性（最好是一个非常快的 SQL），默认值：false。poolPingConnectionsNotUsedFor – 配置 poolPingQuery 的使用频度。这可以被设置成匹配具体的数据库连接超时时间，来避免不必要的侦测，默认值：0（即所有连接每一时刻都被侦测 — 当然仅当 poolPingEnabled 为 true 时适用）。JNDI– 这个数据源的实现是为了能在如 EJB 或应用服务器这类容器中使用，容器可以集中或在外部配置数据源，然后放置一个 JNDI 上下文的引用。这种数据源配置只需要两个属性： initial_context – 这个属性用来在 InitialContext 中寻找上下文（即，initialContext.lookup(initial_context)）。这是个可选属性，如果忽略，那么 data_source 属性将会直接从 InitialContext 中寻找。data_source – 这是引用数据源实例位置的上下文的路径。提供了 initial_context 配置时会在其返回的上下文中进行查找，没有提供时则直接在 InitialContext 中查找。和其他数据源配置类似，可以通过添加前缀“env.”直接把属性传递给初始上下文。比如： env.encoding=UTF8这就会在初始上下文（InitialContext）实例化时往它的构造方法传递值为 UTF8 的 encoding 属性。 通过需要实现接口 org.apache.ibatis.datasource.DataSourceFactory ， 也可使用任何第三方数据源，： 1234public interface DataSourceFactory &#123; void setProperties(Properties props); DataSource getDataSource();&#125; org.apache.ibatis.datasource.unpooled.UnpooledDataSourceFactory 可被用作父类来构建新的数据源适配器，比如下面这段插入 C3P0 数据源所必需的代码： 12345678import org.apache.ibatis.datasource.unpooled.UnpooledDataSourceFactory;import com.mchange.v2.c3p0.ComboPooledDataSource; public class C3P0DataSourceFactory extends UnpooledDataSourceFactory &#123; public C3P0DataSourceFactory() &#123; this.dataSource = new ComboPooledDataSource(); &#125;&#125; 为了令其工作，为每个需要 MyBatis 调用的 setter 方法中增加一个属性。下面是一个可以连接至 PostgreSQL 数据库的例子： 123456&lt;dataSource type="org.myproject.C3P0DataSourceFactory"&gt; &lt;property name="driver" value="org.postgresql.Driver"/&gt; &lt;property name="url" value="jdbc:postgresql:mydb"/&gt; &lt;property name="username" value="postgres"/&gt; &lt;property name="password" value="root"/&gt;&lt;/dataSource&gt;]]></content>
      <categories>
        <category>MyBatis</category>
      </categories>
      <tags>
        <tag>MyBatis</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis在Linux上的部署和Jedis简单使用]]></title>
    <url>%2Fblog%2F2017%2F03%2F18%2F2017-03-18-Redis%E5%9C%A8Linux%E4%B8%8A%E7%9A%84%E9%83%A8%E7%BD%B2%E5%92%8CJedis%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[一、redis的安装这里演示的版本是Redis4.0，Linux系统是CentOS6.7，Jdk1.7，Jedis2.8 这是官方文档介绍的安装方式 下载，解压，编译: 1234$ wget http://download.redis.io/releases/redis-4.0.0.tar.gz$ tar xzf redis-4.0.0.tar.gz$ cd redis-4.0.0$ make 二进制文件是编译完成后在src目录下，通过下面的命令启动Redis服务： 1$ src/redis-server 你可以使用内置的客户端命令redis-cli进行使用： 12345$ src/redis-cliredis&gt; set foo barOKredis&gt; get foo"bar" 本人不建议直接使用源码文件中的服务，make编译完成后，可以安装到指定目录： 1make PREFIX=/usr/local/redis install 现在去刚刚tar包解压出来的源码目录中，拷贝一个redis.conf配置文件，放到/usr/local/redis/bin/目录下，以后在这个目录下使用就好了。 启动服务端（暂时不使用自己刚才复制过来的redis.conf配置文件） 1./redis-server 复制一个会话窗口，启动客户端（暂时不设置ip，端口号和密码） 1./redis-cli 测试下： 123127.0.0.1:6379&gt; pingPONG# 客户端启动成功 二、Java程序中jedis操作redis上面的方式只是一种小练习，我们现在通过Java程序用jedis来操作Linux服务器上的redis。 用maven来引入jedis： 1234567&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.8.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; Java代码： 1234567public static void main(String[] args) &#123; // 服务器ip，redis默认端口号 Jedis jedis = new Jedis("192.168.133.128", 6379); jedis.set("key01", "zhangsan"); jedis.set("key02", "lisi"); System.out.println(jedis.get("key01"));&#125; 注意上面的代码是有问题的！ 三、redis配置文件上面的代码运行后，会报错 redis.clients.jedis.exceptions.JedisConnectionException: java.net.ConnectException: Connection refused: connect 连接超被拒绝了，这是因为，redis的访问ip默认是127.0.0.1 你需要在自己拷贝的redis.conf配置文件中修改： 把绑定的主机ip添加进去，之后启动redis服务的时候，需要手动加载配置文件 我的配置文件放在了和server服务的同一个目录里，所以启动服务时输入： 1./redis-server redis.conf 注意啊：如果不输入后面的配置文件目录，那么该配置文件不起作用，会提示说启动默认的配置文件。 之后再次运行Java代码 又报错！！ redis.clients.jedis.exceptions.JedisDataException: DENIED Redis is running in protected mode because protected mode is enabled, no bind address was specified, no authentication password is requested to clients. In this mode connections are only accepted from the loopback interface. If you want to connect from external computers to Redis you may adopt one of the following solutions: 1) Just disable protected mode sending the command &#39;CONFIG SET protected-mode no&#39; from the loopback interface by connecting to Redis from the same host the server is running, however MAKE SURE Redis is not publicly accessible from internet if you do so. Use CONFIG REWRITE to make this change permanent. 2) Alternatively you can just disable the protected mode by editing the Redis configuration file, and setting the protected mode option to &#39;no&#39;, and then restarting the server. 3) If you started the server manually just for testing, restart it with the &#39;--protected-mode no&#39; option. 4) Setup a bind address or an authentication password. NOTE: You only need to do one of the above things in order for the server to start accepting connections from the outside. 这错报的好长。。 好心的博主帮你谷歌翻译了一下。 简单来说呢？就是给你提供了几个解决方案 1）只需禁用保护模式，即可通过从同一主机连接到Redis，从回送接口发送命令“CONFIG SET protected-mode no”正在运行，但是如果您这样做，请勿使用互联网公开访问互联网。使用CONFIG REWRITE使此更改永久。 2）或者，您可以通过编辑Redis配置文件并将protected mode选项设置为“no”来禁用保护模式，然后重新启动服务器。 3）如果您只是为了测试而手动启动服务器，请使用“ –protected-mode no”选项重新启动服务器。 4）设置绑定地址或认证密码。 这是redis4.0版本的新特性，redis3不会报错。 在这里我选择设置redis密码，同样打开redis.conf配置文件，设置密码为123456，保存退出 然后启动服务器 1./redis-server redis.conf 以后如果你要想在Linux里用命令打开redis客户端，就需要输入一些参数才行 1./redis-cli -h 192.168.133.128 -p 6379 -a 123456 很显然，-h是redis服务绑定的主机ip，-p是redis服务的端口号，-a是redis服务的密码，都可以在redis.conf里更改的 然后就好了 这个时候，Java代码中的问题还没解决完，运行代码还会报错的，没有访问权限 redis.clients.jedis.exceptions.JedisDataException: NOAUTH Authentication required. 你还需要在Java代码中增加一条密码设置 123456789public static void main(String[] args) &#123; // 服务器ip，redis默认端口号 Jedis jedis = new Jedis("192.168.133.128", 6379); // redis访问密码 jedis.auth("123456"); jedis.set("key01", "zhangsan"); jedis.set("key02", "lisi"); System.out.println(jedis.get("key01"));&#125; OK，运行正常 四、其他分享一个redis详细学习教程的网址： http://www.runoob.com/redis/redis-intro.html redis.conf 配置项说明如下： 1.Redis默认不是以守护进程的方式运行，可以通过该配置项修改，使用yes启用守护进程，可以后台启动，用ps -ef | grep redis查看redis进程 daemonize no 2.当Redis以守护进程方式运行时，Redis默认会把pid写入/var/run/redis.pid文件，可以通过pidfile指定 pidfile /var/run/redis.pid 3.指定Redis监听端口，默认端口为6379，作者在自己的一篇博文中解释了为什么选用6379作为默认端口，因为6379在手机按键上MERZ对应的号码，而MERZ取自意大利歌女Alessia Merz的名字 port 6379 4.绑定的主机地址 bind 127.0.0.1 5.当客户端闲置多长时间后关闭连接，如果指定为0，表示关闭该功能 timeout 300 6.指定日志记录级别，Redis总共支持四个级别：debug、verbose、notice、warning，默认为verbose loglevel verbose 7.日志记录方式，默认为标准输出，如果配置Redis为守护进程方式运行，而这里又配置为日志记录方式为标准输出，则日志将会发送给/dev/null logfile stdout 8.设置数据库的数量，默认数据库为0，可以使用SELECT \命令在连接上指定数据库id databases 16 9.指定在多长时间内，有多少次更新操作，就将数据同步到数据文件，可以多个条件配合 save \ \ Redis默认配置文件中提供了三个条件： save 900 1 save 300 10 save 60 10000 分别表示900秒（15分钟）内有1个更改，300秒（5分钟）内有10个更改以及60秒内有10000个更改。 10.指定存储至本地数据库时是否压缩数据，默认为yes，Redis采用LZF压缩，如果为了节省CPU时间，可以关闭该选项，但会导致数据库文件变的巨大 rdbcompression yes 11.指定本地数据库文件名，默认值为dump.rdb dbfilename dump.rdb 12.指定本地数据库存放目录 dir ./ 13.设置当本机为slav服务时，设置master服务的IP地址及端口，在Redis启动时，它会自动从master进行数据同步 slaveof \ \ 14.当master服务设置了密码保护时，slav服务连接master的密码 masterauth 15.设置Redis连接密码，如果配置了连接密码，客户端在连接Redis时需要通过AUTH \命令提供密码，默认关闭 requirepass foobared 16.设置同一时间最大客户端连接数，默认无限制，Redis可以同时打开的客户端连接数为Redis进程可以打开的最大文件描述符数，如果设置 maxclients 0，表示不作限制。当客户端连接数到达限制时，Redis会关闭新的连接并向客户端返回max number of clients reached错误信息 maxclients 128 17.指定Redis最大内存限制，Redis在启动时会把数据加载到内存中，达到最大内存后，Redis会先尝试清除已到期或即将到期的Key，当此方法处理 后，仍然到达最大内存设置，将无法再进行写入操作，但仍然可以进行读取操作。Redis新的vm机制，会把Key存放内存，Value会存放在swap区 maxmemory \ 18.指定是否在每次更新操作后进行日志记录，Redis在默认情况下是异步的把数据写入磁盘，如果不开启，可能会在断电时导致一段时间内的数据丢失。因为 redis本身同步数据文件是按上面save条件来同步的，所以有的数据会在一段时间内只存在于内存中。默认为no appendonly no 19.指定更新日志文件名，默认为appendonly.aof appendfilename appendonly.aof 20.指定更新日志条件，共有3个可选值： no：表示等操作系统进行数据缓存同步到磁盘（快）always：表示每次更新操作后手动调用fsync()将数据写到磁盘（慢，安全）everysec：表示每秒同步一次（折衷，默认值） appendfsync everysec 21.指定是否启用虚拟内存机制，默认值为no，简单的介绍一下，VM机制将数据分页存放，由Redis将访问量较少的页即冷数据swap到磁盘上，访问多的页面由磁盘自动换出到内存中（在后面的文章我会仔细分析Redis的VM机制） vm-enabled no 22.虚拟内存文件路径，默认值为/tmp/redis.swap，不可多个Redis实例共享 vm-swap-file /tmp/redis.swap 23.将所有大于vm-max-memory的数据存入虚拟内存,无论vm-max-memory设置多小,所有索引数据都是内存存储的(Redis的索引数据 就是keys),也就是说,当vm-max-memory设置为0的时候,其实是所有value都存在于磁盘。默认值为0 vm-max-memory 0 24.Redis swap文件分成了很多的page，一个对象可以保存在多个page上面，但一个page上不能被多个对象共享，vm-page-size是要根据存储的 数据大小来设定的，作者建议如果存储很多小对象，page大小最好设置为32或者64bytes；如果存储很大大对象，则可以使用更大的page，如果不 确定，就使用默认值 vm-page-size 32 25.设置swap文件中的page数量，由于页表（一种表示页面空闲或使用的bitmap）是在放在内存中的，在磁盘上每8个pages将消耗1byte的内存。 vm-pages 134217728 26.设置访问swap文件的线程数,最好不要超过机器的核数,如果设置为0,那么所有对swap文件的操作都是串行的，可能会造成比较长时间的延迟。默认值为4 vm-max-threads 4 27.设置在向客户端应答时，是否把较小的包合并为一个包发送，默认为开启 glueoutputbuf yes 28.指定在超过一定的数量或者最大的元素超过某一临界值时，采用一种特殊的哈希算法 hash-max-zipmap-entries 64 hash-max-zipmap-value 512 29.指定是否激活重置哈希，默认为开启（后面在介绍Redis的哈希算法时具体介绍） activerehashing yes 30.指定包含其它的配置文件，可以在同一主机上多个Redis实例之间使用同一份配置文件，而同时各个实例又拥有自己的特定配置文件 include /path/to/local.conf]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Redis搭建</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于异常处理的一些建议]]></title>
    <url>%2Fblog%2F2017%2F02%2F02%2F2017-02-02-%E5%85%B3%E4%BA%8E%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86%E7%9A%84%E4%B8%80%E4%BA%9B%E5%BB%BA%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[合理地使用异常处理可以帮你节省数小时（甚至数天）调试时间。一个乘法异常会毁掉你的晚餐乃至周末计划。如果处置不及时，甚至对你的名誉都会造成影响。一个清晰的异常处理策略可以助你节省诊断、重现和问题纠正时间。下面是我工作中总结的6条异常处理建议。 一、使用一个系统全局异常类不必为每种异常类型建立单独的类，一个就够了。确保这个异常类继承RuntimeException，这样可以减少类个数并且移除不必要的异常声明。 我知道你正在想什么：如果类型只有一个，那么怎么能知道异常具体是什么？我将如何追踪具体的属性？请继续往下看。 二、使用枚举错误码我们大多被教授的方法是将异常转为错误信息。这在查看日志文件时很好，（呃）但是这样也有缺点： 错误信息不会被翻译（除非你是Google） 错误信息不会转换为用户友好的文字 错误信息不能用编程的方式检测 将异常消息留给开发者定义也会出现同样的错误有多种不同的描述。 一个更好的办法是使用枚举表示异常类型。为每个错误分类创建一个枚举（付款、认证等），让枚举实现ErrorCode接口并作为异常的一个属性。 当抛出异常时，只要传入合适的枚举就可以了。 1throw new SystemException(PaymentCode.CREDIT_CARD_EXPIRED); 现在如果需要测试异常只要比较异常代码和枚举就可以了。 12345catch (SystemException e) &#123; if (e.getErrorCode() == PaymentCode.CREDIT_CARD_EXPIRED) &#123; ... &#125;&#125; 通过将错误码作为查找资源的key就可以方便地提供友好的国际化文本。 12345678910111213141516public class SystemExceptionExample &#123; public static void main(String[] args) &#123; System.out.println(getUserText(ValidationCode.VALUE_TOO_SHORT)); &#125; public static String getUserText(ErrorCode errorCode) &#123; if (errorCode == null) &#123; return null; &#125; String key = errorCode.getClass().getSimpleName() + "__" + errorCode; ResourceBundle bundle = ResourceBundle.getBundle("cn.dijia478.exception.example.exceptions"); return bundle.getString(key); &#125;&#125; 三、为枚举添加错误值在很多时候可以为异常添加错误值，比如HTTP返回值。这种情况下，可以在ErrorCode接口添加一个getNumber方法并在每个枚举中实现这个方法。 12345678910111213141516171819202122public enum PaymentCode implements ErrorCode &#123; SERVICE_TIMEOUT(101), CREDIT_CARD_EXPIRED(102), AMOUNT_TOO_HIGH(103), INSUFFICIENT_FUNDS(104); private final int number; private PaymentCode(int number) &#123; this.number = number; &#125; @Override public int getNumber() &#123; return number; &#125; &#125; 添加错误码可以是全局数值也可以每个枚举自己负责。你可以直接使用枚举里的ordinal()方法或者从文件或数据库加载。 四、为异常添加动态属性好的异常处理还应该记录相关数据而不仅仅是堆栈信息，这样可以在诊断错误和重现错误时节省大量时间。用户不会在你的应用停止工作时告诉你他们到底做了什么。 最简单的办法是给异常添加一个java.util.Map字段。新字段的职责就是通过名字保存相关数据。通过添加setter方法可以遵循流式接口。 可以像下面示例这样添加相关数据并抛出异常： 1234throw new SystemException(ValidationCode.VALUE_TOO_SHORT) .set("field", field) .set("value", value) .set("min-length", MIN_LENGTH); 五、避免不必要的嵌套冗长的堆栈信息不会有任何帮助，更糟糕的是会浪费你的时间和资源。重新抛出异常时调用静态函数而不是异常构造函数。封装的静态函数决定什么时候嵌套异常什么时候只要返回原来的实例。 123456789101112131415public static SystemException wrap(Throwable exception, ErrorCode errorCode) &#123; if (exception instanceof SystemException) &#123; SystemException se = (SystemException) exception; if (errorCode != null &amp;&amp; errorCode != se.getErrorCode()) &#123; return new SystemException(exception.getMessage(), exception, errorCode); &#125; return se; &#125; else &#123; return new SystemException(exception.getMessage(), exception, errorCode); &#125;&#125;public static SystemException wrap(Throwable exception) &#123; return wrap(exception, null);&#125; 重新抛出异常的新代码如下所示。 123catch (IOException e) &#123; throw SystemException.wrap(e).set("fileName", fileName);&#125; 六、使用带Web支持的集中式logger再额外附赠一个建议。可能你情况很难向产品记录日志，这个麻烦可能来自多个中间商（很多开发者不能直接访问产品环境）。 在多服务器环境下情况可能会更糟。找到正确的服务器或者确定问题影响到了哪个服务器是一件非常令人头痛的事情。 我的建议是： 将你的日志记录到一个地方，推荐记录到数据库中。 通过Web浏览器访问数据库。 有很多方法和备选产品可以达成这一目标，log collector、远程logger、JMX agent、系统监视软件等。好处就是： 几秒钟之内定位错误 为每个异常增加一个URL，可以记录或者发送email 让你的小伙伴可以在没有你的情况下定位错误原因 避免测试人员为同一个bug添加多个记录。他们可以在bug记录里增加一条异常URL 省钱 让你的周末和名誉不受影响]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>异常</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx学习笔记]]></title>
    <url>%2Fblog%2F2017%2F01%2F10%2F2017-01-10-Nginx%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[前言Nginx是一款轻量级的Web服务器、反向代理服务器，由于它的内存占用少，启动极快，高并发能力强，在互联网项目中广泛应用。 上图基本上说明了当下流行的技术架构，其中Nginx有点入口网关的味道。 反向代理服务器经常听人说到一些术语，如反向代理，那么什么是反向代理，什么又是正向代理呢？ 由于防火墙的原因，我们并不能直接访问谷歌，那么我们可以借助VPN来实现，这就是一个简单的正向代理的例子。这里你能够发现，正向代理“代理”的是客户端，而且客户端是知道目标的，而目标是不知道客户端是通过VPN访问的。 当我们在外网访问百度的时候，其实会进行一个转发，代理到内网去，这就是所谓的反向代理，即反向代理“代理”的是服务器端，而且这一个过程对于客户端而言是透明的。 Nginx的Master-Worker模式 启动Nginx后，其实就是在80端口启动了Socket服务进行监听，如图所示，Nginx涉及Master进程和Worker进程。 Master进程的作用是？读取并验证配置文件nginx.conf；管理worker进程； Worker进程的作用是？每一个Worker进程都维护一个线程（避免线程切换），处理连接和请求；注意Worker进程的个数由配置文件决定，一般和CPU个数相关（有利于进程切换），配置几个就有几个Worker进程。 Nginx如何做到热部署？所谓热部署，就是配置文件nginx.conf修改后，不需要stop Nginx，不需要中断请求，就能让配置文件生效！（nginx -s reload 重新加载/nginx -t检查配置/nginx -s stop强行停止）通过上文我们已经知道worker进程负责处理具体的请求，那么如果想达到热部署的效果，可以想象： 方案一：修改配置文件nginx.conf后，主进程master负责推送给woker进程更新配置信息，woker进程收到信息后，更新进程内部的线程信息。（有点valatile的味道） 方案二：修改配置文件nginx.conf后，重新生成新的worker进程，当然会以新的配置进行处理请求，而且新的请求必须都交给新的worker进程，至于老的worker进程，等把那些以前的请求处理完毕后，kill掉即可。 Nginx采用的就是方案二来达到热部署的！ Nginx如何做到高并发下的高效处理？上文已经提及Nginx的worker进程个数与CPU绑定、worker进程内部包含一个线程高效回环处理请求，这的确有助于效率，但这是不够的。作为专业的程序员，我们可以开一下脑洞：BIO/NIO/AIO、异步/同步、阻塞/非阻塞… 要同时处理那么多的请求，要知道，有的请求需要发生IO，可能需要很长时间，如果等着它，就会拖慢worker的处理速度。 Nginx采用了Linux的epoll模型，epoll模型基于事件驱动机制，它可以监控多个事件是否准备完毕，如果OK，那么放入epoll队列中，这个过程是异步的。worker只需要从epoll队列循环处理即可 Nginx挂了怎么办？Nginx既然作为入口网关，很重要，如果出现单点问题，显然是不可接受的。 答案是：Keepalived+Nginx实现高可用。 Keepalived是一个高可用解决方案，主要是用来防止服务器单点发生故障，可以通过和Nginx配合来实现Web服务的高可用。（其实，Keepalived不仅仅可以和Nginx配合，还可以和很多其他服务配合） Keepalived+Nginx实现高可用的思路：第一：请求不要直接打到Nginx上，应该先通过Keepalived（这就是所谓虚拟IP，VIP）第二：Keepalived应该能监控Nginx的生命状态（提供一个用户自定义的脚本，定期检查Nginx进程状态，进行权重变化,，从而实现Nginx故障切换） 我们的主战场：nginx.conf很多时候，在开发、测试环境下，我们都得自己去配置Nginx，就是去配置nginx.conf。 nginx.conf是典型的分段配置文件，下面我们来分析下。 虚拟主机 其实这是把Nginx作为web server来处理静态资源。 第一：location可以进行正则匹配，应该注意正则的几种形式以及优先级。从高到低分别为：确切的名字、最长的以*起始的通配符名字、最长的以*结束的通配符名字、第一个匹配的正则表达式名字 第二：Nginx能够提高速度的其中一个特性就是：动静分离，就是把静态资源放到Nginx上，由Nginx管理，动态请求转发给后端。 第三：我们可以在Nginx下把静态资源、日志文件归属到不同域名下（也即是目录），这样方便管理维护。 第四：Nginx可以进行IP访问控制，有些电商平台，就可以在Nginx这一层，做一下处理，内置一个黑名单模块，那么就不必等请求通过Nginx达到后端在进行拦截，而是直接在Nginx这一层就处理掉。 反向代理【proxy_pass】所谓反向代理，很简单，其实就是在location这一段配置中的root替换成proxy_pass即可。root说明是静态资源，可以由Nginx进行返回；而proxy_pass说明是动态请求，需要进行转发，比如代理到Tomcat上。 反向代理，上面已经说了，过程是透明的，比如说request -&gt; Nginx -&gt; Tomcat，那么对于Tomcat而言，请求的IP地址就是Nginx的地址，而非真实的request地址，这一点需要注意。不过好在Nginx不仅仅可以反向代理请求，还可以由用户自定义设置HTTP HEADER。 负载均衡【upstream】上面的反向代理中，我们通过proxy_pass来指定Tomcat的地址，很显然我们只能指定一台Tomcat地址，那么我们如果想指定多台来达到负载均衡呢？ 第一，通过upstream来定义一组Tomcat，并指定负载策略（IPHASH、加权论调、最少连接），健康检查策略（Nginx可以监控这一组Tomcat的状态）等。第二，将proxy_pass替换成upstream指定的值即可。 负载均衡可能带来的问题？负载均衡所带来的明显的问题是，一个请求，可以到A server，也可以到B server，这完全不受我们的控制，当然这也不是什么问题，只是我们得注意的是：用户状态的保存问题，如Session会话信息，不能在保存到服务器上。 缓存缓存，是Nginx提供的，可以加快访问速度的机制，说白了，在配置上就是一个开启，同时指定目录，让缓存可以存储到磁盘上。具体配置，大家可以参考Nginx官方文档，这里就不在展开了。]]></content>
      <categories>
        <category>Nginx</category>
      </categories>
      <tags>
        <tag>Nginx</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java研发知识点总结]]></title>
    <url>%2Fblog%2F2016%2F12%2F24%2F2016-12-24-Java%E7%A0%94%E5%8F%91%E7%9F%A5%E8%AF%86%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[一、Java基础（语言、集合框架、OOP、设计模式等）1. HashMap和Hashtable的区别 Hashtable是基于陈旧的Dictionary的Map接口的实现，而HashMap是基于哈希表的Map接口的实现 从方法上看，HashMap去掉了Hashtable的contains方法 HashTable是同步的(线程安全)，而HashMap线程不安全，效率上HashMap更快 HashMap允许空键值，而Hashtable不允许 HashMap的iterator迭代器执行快速失败机制，也就是说在迭代过程中修改集合结构，除非调用迭代器自身的remove方法，否则以其他任何方式的修改都将抛出并发修改异常。而Hashtable返回的Enumeration不是快速失败的。 注：Fast-fail机制:在使用迭代器的过程中有其它线程修改了集合对象结构或元素数量,都将抛出ConcurrentModifiedException，但是抛出这个异常是不保证的，我们不能编写依赖于此异常的程序。 2. java的线程安全Vector、Stack、HashTable、ConcurrentHashMap、Properties 3. java集合框架(常用)123456789101112Collection - List - ArrayListCollection - List - LinkedListCollection - List - VectorCollection - List - Vector - StackCollection - Set - HashSetCollection - Set - TreeSetCollection - List - LinkedHashSetMap - HashMapMap - TreeMapMap - HashTableMap - LinkedHashMapMap - ConcurrentHashMap 3.1 List集合和Set集合 List中元素存取是有序的、可重复的；Set集合中元素是无序的，不可重复的。CopyOnWriteArrayList:COW的策略，即写时复制的策略。适用于读多写少的并发场景Set集合元素存取无序，且元素不可重复。HashSet不保证迭代顺序，线程不安全；LinkedHashSet是Set接口的哈希表和链接列表的实现，保证迭代顺序，线程不安全。TreeSet：可以对Set集合中的元素排序，元素以二叉树形式存放，线程不安全。 3.2 ArrayList、LinkedList、Vector的区别 首先它们均是List接口的实现。 ArrayList、LinkedList的区别：1.随机存取：ArrayList是基于可变大小的数组实现，LinkedList是链接列表的实现。这也就决定了对于随机访问的get和set的操作，ArrayList要优于LinkedList，因为LinkedList要移动指针。2.插入和删除：LinkedList要好一些，因为ArrayList要移动数据，更新索引。3.内存消耗：LinkedList需要更多的内存，因为需要维护指向后继结点的指针。 Vector从JDK 1.0起就存在，在1.2时改为实现List接口，功能与ArrayList类似，但是Vector具备线程安全。 3.3 Map集合 Hashtable:基于Dictionary类，线程安全，速度快。底层是哈希表数据结构。是同步的。不允许null作为键，null作为值。Properties:Hashtable的子类。用于配置文件的定义和操作，使用频率非常高，同时键和值都是字符串。HashMap：线程不安全，底层是数组加链表实现的哈希表。允许null作为键，null作为值。HashMap去掉了contains方法。注意：HashMap不保证元素的迭代顺序。如果需要元素存取有序，请使用LinkedHashMapTreeMap：可以用来对Map集合中的键进行排序。ConcurrentHashMap:是JUC包下的一个并发集合。 3.4 为什么使用ConcurrentHashMap而不是HashMap或Hashtable？ HashMap的缺点：主要是多线程同时put时，如果同时触发了rehash操作，会导致HashMap中的链表中出现循环节点，进而使得后面get的时候，会死循环，CPU达到100%，所以在并发情况下不能使用HashMap。让HashMap同步：Map m = Collections.synchronizeMap(hashMap);而Hashtable虽然是同步的，使用synchronized来保证线程安全，但在线程竞争激烈的情况下HashTable的效率非常低下。因为当一个线程访问HashTable的同步方法时，其他线程访问HashTable的同步方法时，可能会进入阻塞或轮询状态。如线程1使用put进行添加元素，线程2不但不能使用put方法添加元素，并且也不能使用get方法来获取元素，所以竞争越激烈效率越低。 ConcurrentHashMap的原理： HashTable容器在竞争激烈的并发环境下表现出效率低下的原因在于所有访问HashTable的线程都必须竞争同一把锁，那假如容器里有多把锁，每一把锁用于锁容器其中一部分数据，那么当多线程访问容器里不同数据段的数据时，线程间就不会存在锁竞争，从而可以有效的提高并发访问效率，这就是ConcurrentHashMap所使用的锁分段技术，首先将数据分成一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一个段数据的时候，其他段的数据也能被其他线程访问。 ConcurrentHashMap的结构： ConcurrentHashMap是由Segment数组结构和HashEntry数组结构组成。Segment是一种可重入互斥锁ReentrantLock，在ConcurrentHashMap里扮演锁的角色，HashEntry则用于存储键值对数据。一个ConcurrentHashMap里包含一个Segment数组，Segment的结构和HashMap类似，是一种数组和链表结构， 一个Segment里包含一个HashEntry数组，每个HashEntry是一个链表结构的元素，当对某个HashEntry数组的数据进行修改时，必须首先获得它对应的Segment锁。 ConcurrentHashMap的构造、get、put操作： 构造函数：传入参数分别为 1、初始容量，默认16 2、装载因子 装载因子用于rehash的判定，就是当ConcurrentHashMap中的元素大于装载因子乘以最大容量时进行扩容，默认0.75 3、并发级别 这个值用来确定Segment的个数，Segment的个数是大于等于concurrencyLevel的第一个2的n次方的数。比如，如果concurrencyLevel为12，13，14，15，16这些数，则Segment的数目为16(2的4次方)。默认值为static final int DEFAULT_CONCURRENCY_LEVEL = 16;。理想情况下ConcurrentHashMap的真正的并发访问量能够达到concurrencyLevel，因为有concurrencyLevel个Segment，假如有concurrencyLevel个线程需要访问Map，并且需要访问的数据都恰好分别落在不同的Segment中，则这些线程能够无竞争地自由访问（因为他们不需要竞争同一把锁），达到同时访问的效果。这也是为什么这个参数起名为“并发级别”的原因。默认16. 初始化的一些动作： 初始化segments数组（根据并发级别得到数组大小ssize），默认16初始化segmentShift和segmentMask（这两个全局变量在定位segment时的哈希算法里需要使用），默认情况下segmentShift为28，segmentMask为15初始化每个Segment，这一步会确定Segment里HashEntry数组的长度. put操作： 1、判断value是否为null，如果为null，直接抛出异常。2、key通过一次hash运算得到一个hash值。将得到hash值向右按位移动segmentShift位，然后再与segmentMask做&amp;运算得到segment的索引j。即segmentFor方法3、使用Unsafe的方式从Segment数组中获取该索引对应的Segment对象。向这个Segment对象中put值，这个put操作也基本是一样的步骤（通过&amp;运算获取HashEntry的索引，然后set）。 get操作： 1、和put操作一样，先通过key进行hash确定应该去哪个Segment中取数据。2、使用Unsafe获取对应的Segment，然后再进行一次&amp;运算得到HashEntry链表的位置，然后从链表头开始遍历整个链表（因为Hash可能会有碰撞，所以用一个链表保存），如果找到对应的key，则返回对应的value值，如果链表遍历完都没有找到对应的key，则说明Map中不包含该key，返回null。 定位Segment的hash算法：(hash &gt;&gt;&gt; segmentShift) &amp; segmentMask定位HashEntry所使用的hash算法：int index = hash &amp; (tab.length - 1); 注： 1.tab为HashEntry数组2.ConcurrentHashMap既不允许null key也不允许null value 3.5 Collection 和 Collections的区别 Collection是集合类的上级接口，子接口主要有Set 和List、QueueCollections是针对集合类的一个辅助类，提供了操作集合的工具方法：一系列静态方法实现对各种集合的搜索、排序、线程安全化等操作。 3.6 Map、Set、List、Queue、Stack的特点与用法 Set集合类似于一个罐子，”丢进”Set集合里的多个对象之间没有明显的顺序。 List集合代表元素有序、可重复的集合，集合中每个元素都有其对应的顺序索引。 Stack是Vector提供的一个子类，用于模拟”栈”这种数据结构(LIFO后进先出) Queue用于模拟”队列”这种数据结构(先进先出 FIFO)。 Map用于保存具有”映射关系”的数据，因此Map集合里保存着两组值。 3.7 HashMap的工作原理 HashMap维护了一个Entry数组，Entry内部类有key,value，hash和next四个字段，其中next也是一个Entry类型。可以将Entry数组理解为一个个的散列桶。每一个桶实际上是一个单链表。当执行put操作时，会根据key的hashcode定位到相应的桶。遍历单链表检查该key是否已经存在，如果存在，覆盖该value，反之，新建一个新的Entry，并放在单链表的头部。当通过传递key调用get方法时，它再次使用key.hashCode()来找到相应的散列桶，然后使用key.equals()方法找出单链表中正确的Entry，然后返回它的值。 3.8 Map的实现类的介绍 HashMap基于散列表来的实现，即使用hashCode()进行快速查询元素的位置，显著提高性能。插入和查询“键值对”的开销是固定的。可以通过设置容量和装载因子，以调整容器的性能。 LinkedHashMap, 类似于HashMap,但是迭代遍历它时，保证迭代的顺序是其插入的次序，因为它使用链表维护内部次序。此外可以在构造器中设定LinkedHashMap，使之采用LRU算法。使没有被访问过的元素或较少访问的元素出现在前面，访问过的或访问多的出现在后面。这对于需要定期清理元素以节省空间的程序员来说，此功能使得程序员很容易得以实现。 TreeMap, 是基于红黑树的实现。同时TreeMap实现了SortedMap接口，该接口可以确保键处于排序状态。所以查看“键”和“键值对”时，所有得到的结果都是经过排序的，次序由自然排序或提供的Comparator决定。SortedMap接口拥有其他额外的功能，如：返回当前Map使用的Comparator比较强，firstKey()，lastKey(),headMap(toKey),tailMap(fromKey)以及可以返回一个子树的subMap()方法等。 WeakHashMap，表示弱键映射，WeakHashMap 的工作与正常的 HashMap 类似，但是使用弱引用作为 key，意思就是当 key 对象没有任何引用时，key/value 将会被回收。 ConcurrentHashMap， 在HashMap基础上分段锁机制实现的线程安全的HashMap。 IdentityHashMap 使用==代替equals() 对“键”进行比较的散列映射。专为解决特殊问题而设计。 HashTable：基于Dictionary类的Map接口的实现，它是线程安全的。 3.9 LinkedList 和 PriorityQueue 的区别 它们均是Queue接口的实现。拥有FIFO的特点，它们的区别在于排序行为。LinkedList 支持双向列表操作，PriorityQueue 按优先级组织的队列，元素的出队次序由元素的自然排序或者由Comparator比较器指定。 3.10 线程安全的集合类。 Vector、Hashtable、Properties和Stack、ConcurrentHashMap 3.11 BlockingQueue Java.util.concurrent.BlockingQueue是一个队列，在进行获取元素时，它会等待队列变为非空；当在添加一个元素时，它会等待队列中的可用空间。BlockingQueue接口是Java集合框架的一部分，主要用于实现生产者-消费者模式。我们不需要担心等待生产者有可用的空间，或消费者有可用的对象，因为它都在BlockingQueue的实现类中被处理了。Java提供了集中BlockingQueue的实现，比如ArrayBlockingQueue、LinkedBlockingQueue、PriorityBlockingQueue,、SynchronousQueue等。 **3.12 如何对一组对象进行排序 如果需要对一个对象数组进行排序，我们可以使用Arrays.sort()方法。如果我们需要排序一个对象列表，我们可以使用Collections.sort()方法。排序时是默认根据元素的自然排序（使用Comparable）或使用Comparator外部比较器。Collections内部使用数组排序方法，所有它们两者都有相同的性能，只是Collections需要花时间将列表转换为数组。 4. ArrayList 无参构造 容量为10 ArrayList(Collections&lt;?extends E&gt; c)构造包含指定collection的元素的列表 ArrayList(int initialCapacity) 指定初始容量 5. final关键字final修饰的变量是常量，必须进行初始化，可以显示初始化，也可以通过构造进行初始化，如果不初始化编译会报错。 6. 接口与抽象类6.1 一个子类只能继承一个抽象类,但能实现多个接口6.2 抽象类可以有构造方法,接口没有构造方法6.3 抽象类可以有普通成员变量,接口没有普通成员变量6.4 抽象类和接口都可有静态成员变量,抽象类中静态成员变量访问类型任意，接口只能public static final(默认)6.5 抽象类可以没有抽象方法,抽象类可以有普通方法,接口中都是抽象方法6.6 抽象类可以有静态方法，接口不能有静态方法6.7 抽象类中的方法可以是public、protected;接口方法只有public abstract 7. 抽象类和最终类抽象类可以没有抽象方法, 最终类可以没有最终方法 最终类不能被继承, 最终方法不能被重写(可以重载) 8.异常相关的关键字 throw、throws、try…catch、finally throws 用在方法签名上, 以便抛出的异常可以被调用者处理 throw 方法内部通过throw抛出异常 try 用于检测包住的语句块, 若有异常, catch子句捕获并执行catch块 9. 关于finally finally不管有没有异常都要处理 当try和catch中有return时，finally仍然会执行，finally比return先执行 不管有木有异常抛出, finally在return返回前执行 finally是在return后面的表达式运算后执行的（此时并没有返回运算后的值，而是先把要返回的值保存起来，管finally中的代码怎么样，返回的值都不会改变，仍然是之前保存的值），所以函数返回值是在finally执行前确定的 注意：finally中最好不要包含return，否则程序会提前退出，返回值不是try或catch中保存的返回值 finally不执行的几种情况：程序提前终止如调用了System.exit, 病毒，断电 10. 受检查异常和运行时异常 10.1 粉红色的是受检查的异常(checked exceptions),其必须被try…catch语句块所捕获, 或者在方法签名里通过throws子句声明。受检查的异常必须在编译时被捕捉处理,命名为Checked Exception是因为Java编译器要进行检查, Java虚拟机也要进行检查, 以确保这个规则得到遵守。 常见的checked exception：ClassNotFoundException IOException FileNotFoundException EOFException 10.2 绿色的异常是运行时异常(runtime exceptions), 需要程序员自己分析代码决定是否捕获和处理,比如空指针,被0除… 常见的runtime exception：NullPointerException ArithmeticException ClassCastException IllegalArgumentException IllegalStateException IndexOutOfBoundsException NoSuchElementException 10.3 而声明为Error的，则属于严重错误，如系统崩溃、虚拟机错误、动态链接失败等，这些错误无法恢复或者不可能捕捉，将导致应用程序中断，Error不需要捕获。 11. this &amp; super11.1 super出现在父类的子类中。有三种存在方式 super.xxx(xxx为变量名或对象名)意思是获取父类中xxx的变量或引用 super.xxx(); (xxx为方法名)意思是直接访问并调用父类中的方法 super() 调用父类构造 注：super只能指代其直接父类 11.2 this() &amp; super()在构造方法中的区别 调用super()必须写在子类构造方法的第一行, 否则编译不通过 super从子类调用父类构造, this在同一类中调用其他构造 均需要放在第一行 尽管可以用this调用一个构造器, 却不能调用2个 this和super不能出现在同一个构造器中, 否则编译不通过 this()、super()都指的对象,不可以在static环境中使用 本质this指向本对象的指针。super是一个关键字 12. 修饰符一览12345修饰符 类内部 同一个包 子类 任何地方private yesdefault yes yesprotected yes yes yespublic yes yes yes yes 13. 构造内部类和静态内部类对象12345678910111213public class Enclosingone &#123; public class Insideone &#123;&#125; public static class Insideone&#123;&#125;&#125;public class Test &#123; public static void main(String[] args) &#123; // 构造内部类对象需要外部类的引用 Enclosingone.Insideone obj1 = new Enclosingone().new Insideone(); // 构造静态内部类的对象 Enclosingone.Insideone obj2 = new Enclosingone.Insideone(); &#125;&#125; 静态内部类不需要有指向外部类的引用。但非静态内部类需要持有对外部类的引用。非静态内部类能够访问外部类的静态和非静态成员。静态内部类不能访问外部类的非静态成员，只能访问外部类的静态成员。 14. 序列化声明为static和transient类型的数据不能被序列化， 反序列化需要一个无参构造函数 15.正则表达式次数符号12345* 0或多次+ 1或多次？0或1次&#123;n&#125; 恰n次&#123;n,m&#125; 从n到m次 其他符号 符号 等价形式1234567\d [0-9]\D [^0-9] \w [a-zA-Z_0-9]\W [^a-zA-Z_0-9]\s [\t\n\r\f]\S [^\t\n\r\f]. 任何字符 边界匹配器 行开头 ^行结尾 $单词边界 \b 贪婪模式:最大长度匹配 非贪婪模式:匹配到结果就好,最短匹配 环视 1234567891011121314字符 描述 匹配对象. 单个任意字符 [...] 字符组 列出的任意字符[^...] 未列出的任意字符^ caret 行的起始位置$ dollar 行的结束位置\&lt; 单词的起始位置\&gt; 单词的结束位置\b 单词边界\B 非单词边界(?=Expression) 顺序肯定环视 成功,如果右边能够匹配(?!Expression) 顺序否定环视 成功,如果右边不能够匹配(?&lt;=Expression) 逆序肯定环视 成功,如果左边能够匹配(?&lt;!Expression) 逆序否定环视 成功,如果左边不能够匹配 举例:北京市(海淀区)(朝阳区)(西城区) Regex: .*(?=\() 模式和匹配器的典型调用次序 把正则表达式编译到模式中Pattern p = Pattern.compile(“a*b”); 创建给定输入与此模式的匹配器Matcher m = p.matcher(“aaab”); 尝试将整个区域与此模式匹配boolean b = m.matches(); 16. 面向对象的五大基本原则(solid) S单一职责SRP:Single-Responsibility Principle一个类,最好只做一件事,只有一个引起它的变化。单一职责原则可以看做是低耦合,高内聚在面向对象原则的引申,将职责定义为引起变化的原因,以提高内聚性减少引起变化的原因。 O开放封闭原则OCP:Open-Closed Principle软件实体应该是可扩展的,而不是可修改的。对扩展开放,对修改封闭 L里氏替换原则LSP:Liskov-Substitution Principle子类必须能够替换其基类。这一思想表现为对继承机制的约束规范,只有子类能够替换其基类时,才能够保证系统在运行期内识别子类,这是保证继承复用的基础。 I接口隔离原则ISP:Interface-Segregation Principle使用多个小的接口,而不是一个大的总接口 D依赖倒置原则DIP:Dependency-Inversion Principle依赖于抽象。具体而言就是高层模块不依赖于底层模块,二者共同依赖于抽象。抽象不依赖于具体,具体依赖于抽象。 17. 面向对象设计其他原则 封装变化 少用继承 多用组合 针对接口编程 不针对实现编程 为交互对象之间的松耦合设计而努力 类应该对扩展开发 对修改封闭（开闭OCP原则） 依赖抽象，不要依赖于具体类（依赖倒置DIP原则） 密友原则：只和朋友交谈（最少知识原则，迪米特法则） 说明：一个对象应当对其他对象有尽可能少的了解，将方法调用保持在界限内，只调用属于以下范围的方法：该对象本身（本地方法）对象的组件 被当作方法参数传进来的对象 此方法创建或实例化的任何对象 别找我（调用我） 我会找你（调用你）（好莱坞原则） 一个类只有一个引起它变化的原因（单一职责SRP原则） 18. null可以被强制转型为任意类型的对象19.代码执行次序 多个静态成员变量, 静态代码块按顺序执行 单个类中: 静态代码 -&gt; main方法 -&gt; 构造代码块 -&gt; 构造方法 构造块在每一次创建对象时执行 静态代码只执行一次 涉及父类和子类的初始化过程a.初始化父类中的静态成员变量和静态代码块b.初始化子类中的静态成员变量和静态代码块c.初始化父类的普通成员变量和构造代码块(按次序)，再执行父类的构造方法(注意父类构造方法中的子类方法覆盖)d.初始化子类的普通成员变量和构造代码块(按次序)，再执行子类的构造方法 20. 数组复制方法 for逐一复制 System.arraycopy() -&gt; 效率最高native方法 Arrays.copyOf() -&gt; 本质调用arraycopy clone方法 -&gt; 返回Object[],需要强制类型转换 21. 多态 Java通过方法重写和方法重载实现多态 方法重写是指子类重写了父类的同名方法 方法重载是指在同一个类中，方法的名字相同，但是参数列表不同 22. Java文件.java文件可以包含多个类，唯一的限制就是：一个文件中只能有一个public类， 并且此public类必须与文件名相同。而且这些类和写在多个文件中没有区别。 23. Java移位运算符java中有三种移位运算符 &lt;&lt; :左移运算符,x &lt;&lt; 1,相当于x乘以2(不溢出的情况下),低位补0 &gt;&gt; :带符号右移,x &gt;&gt; 1,相当于x除以2,正数高位补0,负数高位补1 &gt;&gt;&gt; :无符号右移,忽略符号位,空位都以0补齐 24. 形参&amp;实参 形式参数可被视为local variable.形参和局部变量一样都不能离开方法。只有在方法中使用，不会在方法外可见。 形式参数只能用final修饰符，其它任何修饰符都会引起编译器错误。但是用这个修饰符也有一定的限制，就是在方法中不能对参数做任何修改。不过一般情况下，一个方法的形参不用final修饰。只有在特殊情况下，那就是：方法内部类。一个方法内的内部类如果使用了这个方法的参数或者局部变量的话，这个参数或局部变量应该是final。 形参的值在调用时根据调用者更改，实参则用自身的值更改形参的值（指针、引用皆在此列），也就是说真正被传递的是实参。 25. IO流一览 26. 局部变量为什么要初始化局部变量是指类方法中的变量，必须初始化。局部变量运行时被分配在栈中，量大，生命周期短，如果虚拟机给每个局部变量都初始化一下，是一笔很大的开销，但变量不初始化为默认值就使用是不安全的。出于速度和安全性两个方面的综合考虑，解决方案就是虚拟机不初始化，但要求编写者一定要在使用前给变量赋值。 27. Java语言的鲁棒性Java在编译和运行程序时，都要对可能出现的问题进行检查，以消除错误的产生。它提供自动垃圾收集来进行内存管理，防止程序员在管理内存时容易产生的错误。通过集成的面向对象的例外处理机制，在编译时，Java揭示出可能出现但未被处理的异常，帮助程序员正确地进行选择以防止系统的崩溃。另外，Java在编译时还可捕获类型声明中的许多常见错误，防止动态运行时不匹配问题的出现。 28. Java语言特性 Java致力于检查程序在编译和运行时的错误 Java虚拟机实现了跨平台接口 类型检查帮助检查出许多开发早期出现的错误 Java自己操纵内存减少了内存出错的可能性 Java还实现了真数组，避免了覆盖数据的可能 29. 包装类的equals()方法不处理数据转型，必须类型和值都一样才相等。30. 子类可以继承父类的静态方法！但是不能覆盖。因为静态方法是在编译时确定了，不能多态，也就是不能运行时绑定。31. Java语法糖 Java7的switch用字符串 - hashcode方法 switch用于enum枚举 伪泛型 - List原始类型 自动装箱拆箱 - Integer.valueOf和Integer.intValue foreach遍历 - Iterator迭代器实现 条件编译 enum枚举类、内部类 可变参数 - 数组 断言语言 try语句中定义和关闭资源 32. Java 中应该使用什么数据类型来代表价格？如果不是特别关心内存和性能的话，使用BigDecimal，否则使用预定义精度的 double 类型。 33. 怎么将 byte 转换为 String？可以使用 String 接收 byte[] 参数的构造器来进行转换，需要注意的点是要使用的正确的编码，否则会使用平台默认编码，这个编码可能跟原来的编码相同，也可能不同。 34. Java 中怎样将 bytes 转换为 long 类型？String接收bytes的构造器转成String，再Long.parseLong 35. 我们能将 int 强制转换为 byte 类型的变量吗？如果该值大于 byte 类型的范围，将会出现什么现象？是的，我们可以做强制转换，但是 Java 中 int 是 32 位的，而 byte 是 8 位的，所以，如果强制转化是，int 类型的高 24 位将会被丢弃，byte 类型的范围是从 -128 到 127。 36. 存在两个类，B 继承 A，C 继承 B，我们能将 B 转换为 C 么？如 C = (C) B；可以，向下转型。但是不建议使用，容易出现类型转型异常. 37. 哪个类包含 clone 方法？是 Cloneable 还是 Object？java.lang.Cloneable 是一个标示性接口，不包含任何方法，clone 方法在 object 类中定义。并且需要知道 clone() 方法是一个本地方法，这意味着它是由 c 或 c++ 或 其他本地语言实现的。 38. Java 中 ++ 操作符是线程安全的吗？不是线程安全的操作。它涉及到多个指令，如读取变量值，增加，然后存储回内存，这个过程可能会出现多个线程交差。还会存在竞态条件（读取-修改-写入）。 39. a = a + b 与 a += b 的区别+= 隐式的将加操作的结果类型强制转换为持有结果的类型。如果两这个整型相加，如 byte、short 或者 int，首先会将它们提升到 int 类型，然后在执行加法操作。 1234byte a = 127;byte b = 127;b = a + b; // error : cannot convert from int to byteb += a; // ok （因为 a+b 操作会将 a、b 提升为 int 类型，所以将 int 类型赋值给 byte 就会编译出错） 40. 我能在不进行强制转换的情况下将一个 double 值赋值给 long 类型的变量吗？不行，你不能在没有强制类型转换的前提下将一个 double 值赋值给 long 类型的变量，因为 double 类型的范围比 long 类型更广，所以必须要进行强制转换。 41. 3*0.1 == 0.3 将会返回什么？true 还是 false？false，因为有些浮点数不能完全精确的表示出来。 42. int 和 Integer 哪个会占用更多的内存？Integer 对象会占用更多的内存。Integer 是一个对象，需要存储对象的元数据。但是 int 是一个原始类型的数据，所以占用的空间更少。 43. 为什么 Java 中的 String 是不可变的（Immutable）？Java 中的 String 不可变是因为 Java 的设计者认为字符串使用非常频繁，将字符串设置为不可变可以允许多个客户端之间共享相同的字符串。而且是为了安全和效率的考虑。 44. 我们能在 Switch 中使用 String 吗？从 Java 7 开始，我们可以在 switch case 中使用字符串，但这仅仅是一个语法糖。内部实现在 switch 中使用字符串的 hash code。 45. Java 中的构造器链是什么？当你从一个构造器中调用另一个构造器，就是Java 中的构造器链。这种情况只在重载了类的构造器的时候才会出现。 46. 枚举类JDK1.5出现 每个枚举值都需要调用一次构造函数 47. 什么是不可变对象（immutable object）？不可变对象指对象一旦被创建，状态就不能再改变。任何修改都会创建一个新的对象，如 String、Integer及其它包装类。 48. Java 中怎么创建一个不可变对象？如何在Java中写出Immutable的类？ 要写出这样的类，需要遵循以下几个原则： 1）immutable对象的状态在创建之后就不能发生改变，任何对它的改变都应该产生一个新的对象。 2）Immutable类的所有的属性都应该是final的。 3）对象必须被正确的创建，比如：对象引用在对象创建过程中不能泄露(leak)。 4）对象应该是final的，以此来限制子类继承父类，以避免子类改变了父类的immutable特性。 5）如果类中包含mutable类对象，那么返回给客户端的时候，返回该对象的一个拷贝，而不是该对象本身（该条可以归为第一条中的一个特例） 49. 我们能创建一个包含可变对象的不可变对象吗？是的，我们是可以创建一个包含可变对象的不可变对象的，你只需要谨慎一点，不要共享可变对象的引用就可以了，如果需要变化时，就返回原对象的一个拷贝。最常见的例子就是对象中包含一个日期对象的引用。 50. List和SetList 是一个有序集合，允许元素重复。它的某些实现可以提供基于下标值的常量访问时间，但是这不是 List 接口保证的。Set 是一个无序集合。 51. poll() 方法和 remove() 方法的区别？poll() 和 remove() 都是从队列中取出一个元素，但是 poll() 在获取元素失败的时候会返回空，但是 remove() 失败的时候会抛出异常。 52. Java 中 LinkedHashMap 和 PriorityQueue 的区别是什么？PriorityQueue 保证最高或者最低优先级的的元素总是在队列头部，但是 LinkedHashMap 维持的顺序是元素插入的顺序。当遍历一个 PriorityQueue 时，没有任何顺序保证，但是 LinkedHashMap 课保证遍历顺序是元素插入的顺序。 53. ArrayList 与 LinkedList 的区别？最明显的区别是 ArrrayList 底层的数据结构是数组，支持随机访问，而 LinkedList 的底层数据结构书链表，不支持随机访问。使用下标访问一个元素，ArrayList 的时间复杂度是 O(1)，而 LinkedList 是 O(n)。 54. 用哪两种方式来实现集合的排序？你可以使用有序集合，如 TreeSet 或 TreeMap，你也可以使用有顺序的的集合，如 list，然后通过 Collections.sort() 来排序。 55. Java 中怎么打印数组？你可以使用 Arrays.toString() 和 Arrays.deepToString() 方法来打印数组。由于数组没有实现 toString() 方法，所以如果将数组传递给 System.out.println() 方法，将无法打印出数组的内容，但是 Arrays.toString() 可以打印每个元素。 56. Java 中的 LinkedList 是单向链表还是双向链表？是双向链表，你可以检查 JDK 的源码。在 Eclipse，你可以使用快捷键 Ctrl + T，直接在编辑器中打开该类。 57. Java 中的 TreeMap 是采用什么树实现的？Java 中的 TreeMap 是使用红黑树实现的。 58. Java 中的 HashSet，内部是如何工作的？HashSet 的内部采用 HashMap来实现。由于 Map 需要 key 和 value，所以所有 key 的都有一个默认 value。类似于 HashMap，HashSet 不允许重复的 key，只允许有一个null key，意思就是 HashSet 中只允许存储一个 null 对象。 59. 写一段代码在遍历 ArrayList 时移除一个元素？该问题的关键在于面试者使用的是 ArrayList 的 remove() 还是 Iterator 的 remove()方法。这有一段示例代码，是使用正确的方式来实现在遍历的过程中移除元素，而不会出现 ConcurrentModificationException 异常的示例代码。 60. 我们能自己写一个容器类，然后使用 for-each 循环吗？可以，你可以写一个自己的容器类。如果你想使用 Java 中增强的循环来遍历，你只需要实现 Iterable 接口。如果你实现 Collection 接口，默认就具有该属性。 61. ArrayList 和 HashMap 的默认大小是多数？在 Java 7 中，ArrayList 的默认大小是 10 个元素，HashMap 的默认大小是16个元素（必须是2的幂）。这就是 Java 7 中 ArrayList 和 HashMap 类的代码片段： 12345// from ArrayList.java JDK 1.7private static final int DEFAULT_CAPACITY = 10; //from HashMap.java JDK 7static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16 62. 有没有可能两个不相等的对象有有相同的 hashcode？有可能，两个不相等的对象可能会有相同的 hashcode 值，这就是为什么在 hashmap 中会有冲突。相等 hashcode 值的规定只是说如果两个对象相等，必须有相同的hashcode 值，但是没有关于不相等对象的任何规定。 63. 两个相同的对象会有不同的的 hash code 吗？不能，根据 hash code 的规定，这是不可能的。 64. 我们可以在 hashcode() 中使用随机数字吗？不行，因为对象的 hashcode 值必须是相同的。 65. Java 中，Comparator 与 Comparable 有什么不同？Comparable 接口用于定义对象的自然顺序，而 comparator 通常用于定义用户定制的顺序。Comparable 总是只有一个，但是可以有多个 comparator 来定义对象的顺序。 66. 为什么在重写 equals 方法的时候需要重写 hashCode 方法？因为有强制的规范指定需要同时重写 hashcode 与 equal 是方法，许多容器类，如 HashMap、HashSet 都依赖于 hashcode 与 equals 的规定。 67. “a==b”和”a.equals(b)”有什么区别？如果 a 和 b 都是对象，则 a==b 是比较两个对象的引用，只有当 a 和 b 指向的是堆中的同一个对象才会返回 true，而 a.equals(b) 是进行逻辑比较，所以通常需要重写该方法来提供逻辑一致性的比较。例如，String 类重写 equals() 方法，所以可以用于两个不同对象，但是包含的字母相同的比较。 68. a.hashCode() 有什么用？与 a.equals(b) 有什么关系？简介：hashCode() 方法是相应对象整型的 hash 值。它常用于基于 hash 的集合类，如 Hashtable、HashMap、LinkedHashMap等等。它与 equals() 方法关系特别紧密。根据 Java 规范，两个使用 equal() 方法来判断相等的对象，必须具有相同的 hash code。 1、hashcode的作用 List和Set，如何保证Set不重复呢？通过迭代使用equals方法来判断，数据量小还可以接受，数据量大怎么解决？引入hashcode，实际上hashcode扮演的角色就是寻址，大大减少查询匹配次数。 2、hashcode重要吗 对于数组、List集合就是一个累赘。而对于hashmap, hashset, hashtable就异常重要了。 3、equals方法遵循的原则 对称性 若x.equals(y)true，则y.equals(x)true 自反性 x.equals(x)必须true 传递性 若x.equals(y)true,y.equals(z)true,则x.equals(z)必为true 一致性 只要x,y内容不变，无论调用多少次结果不变 其他 x.equals(null) 永远false，x.equals(和x数据类型不同)始终false 69. final、finalize 和 finally 的不同之处？final 是一个修饰符，可以修饰变量、方法和类。如果 final 修饰变量，意味着该变量的值在初始化后不能被改变。Java 技术允许使用 finalize() 方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。这个方法是由垃圾收集器在确定这个对象没有被引用时对这个对象调用的，但是什么时候调用 finalize 没有保证。finally 是一个关键字，与 try 和 catch 一起用于异常的处理。finally 块一定会被执行，无论在 try 块中是否有发生异常。 70. Java 中的编译期常量是什么？使用它又什么风险？变量也就是我们所说的编译期常量，这里的 public 可选的。实际上这些变量在编译时会被替换掉，因为编译器知道这些变量的值，并且知道这些变量在运行时不能改变。这种方式存在的一个问题是你使用了一个内部的或第三方库中的公有编译时常量，但是这个值后面被其他人改变了，但是你的客户端仍然在使用老的值，甚至你已经部署了一个新的jar。为了避免这种情况，当你在更新依赖 JAR 文件时，确保重新编译你的程序。 71. 说出几点 Java 中使用 Collections 的最佳实践这是我在使用 Java 中 Collectionc 类的一些最佳实践：a）使用正确的集合类，例如，如果不需要同步列表，使用 ArrayList 而不是 Vector。b）优先使用并发集合，而不是对集合进行同步。并发集合提供更好的可扩展性。c）使用接口代表和访问集合，如使用List存储 ArrayList，使用 Map 存储 HashMap 等等。d）使用迭代器来循环集合。e）使用集合的时候使用泛型。 72. 静态内部类与顶级类有什么区别？一个公共的顶级类的源文件名称与类名相同，而嵌套静态类没有这个要求。一个嵌套类位于顶级类内部，需要使用顶级类的名称来引用嵌套静态类，如 HashMap.Entry 是一个嵌套静态类，HashMap 是一个顶级类，Entry是一个嵌套静态类。 73. Java 中，Serializable 与 Externalizable 的区别？Serializable 接口是一个序列化 Java 类的接口，以便于它们可以在网络上传输或者可以将它们的状态保存在磁盘上，是 JVM 内嵌的默认序列化方式，成本高、脆弱而且不安全。Externalizable 允许你控制整个序列化过程，指定特定的二进制格式，增加安全机制。 74. 说出 JDK 1.7 中的三个新特性？虽然 JDK 1.7 不像 JDK 5 和 8 一样的大版本，但是，还是有很多新的特性，如 try-with-resource 语句，这样你在使用流或者资源的时候，就不需要手动关闭，Java 会自动关闭。Fork-Join 池某种程度上实现 Java 版的 Map-reduce。允许 Switch 中有 String 变量和文本。菱形操作符(&lt;&gt;)用于泛型推断，不再需要在变量声明的右边申明泛型，因此可以写出可读写更强、更简洁的代码。另一个值得一提的特性是改善异常处理，如允许在同一个 catch 块中捕获多个异常。 75. 说出 5 个 JDK 1.8 引入的新特性？Java 8 在 Java 历史上是一个开创新的版本，下面 JDK 8 中 5 个主要的特性：Lambda 表达式，允许像对象一样传递匿名函数Stream API，充分利用现代多核 CPU，可以写出很简洁的代码Date 与 Time API，最终，有一个稳定、简单的日期和时间库可供你使用扩展方法，现在，接口中可以有静态、默认方法。重复注解，现在你可以将相同的注解在同一类型上使用多次。 下述包含 Java 面试过程中关于 SOLID 的设计原则，OOP 基础，如类，对象，接口，继承，多态，封装，抽象以及更高级的一些概念，如组合、聚合及关联。也包含了 GOF 设计模式的问题。 76. 接口是什么？为什么要使用接口而不是直接使用具体类？接口用于定义 API。它定义了类必须得遵循的规则。同时，它提供了一种抽象，因为客户端只使用接口，这样可以有多重实现，如 List 接口，你可以使用可随机访问的 ArrayList，也可以使用方便插入和删除的 LinkedList。接口中不允许普通方法，以此来保证抽象，但是 Java 8 中你可以在接口声明静态方法和默认普通方法。 77. Java 中，抽象类与接口之间有什么不同？Java 中，抽象类和接口有很多不同之处，但是最重要的一个是 Java 中限制一个类只能继承一个类，但是可以实现多个接口。抽象类可以很好的定义一个家族类的默认行为，而接口能更好的定义类型，有助于后面实现多态机制参见第六条。 78. 除了单例模式，你在生产环境中还用过什么设计模式?这需要根据你的经验来回答。一般情况下，你可以说依赖注入，工厂模式，装饰模式或者观察者模式，随意选择你使用过的一种即可。不过你要准备回答接下的基于你选择的模式的问题。 79. 你能解释一下里氏替换原则吗?严格定义：如果对每一个类型为S的对象o1，都有类型为T的对象o2，使得以T定义的所有程序P在所有的对象用o1替换o2时，程序P的行为没有变化，那么类型S是类型T的子类型。 通俗表述：所有引用基类（父类）的地方必须能透明地使用其子类的对象。也就是说子类可以扩展父类的功能，但不能改变父类原有的功能。它包含以下4层含义： 子类可以实现父类的抽象方法，但不能覆盖父类的非抽象方法。 子类中可以增加自己特有的方法。 当子类的方法重载父类的方法时，方法的前置条件（即方法的形参）要比父类方法的输入参数更宽松。 当子类的方法实现父类的抽象方法时，方法的后置条件（即方法的返回值）要比父类更严格。 80.什么情况下会违反迪米特法则？为什么会有这个问题？迪米特法则建议“只和朋友说话，不要陌生人说话”，以此来减少类之间的耦合。 81. 适配器模式是什么？什么时候使用？适配器模式提供对接口的转换。如果你的客户端使用某些接口，但是你有另外一些接口，你就可以写一个适配去来连接这些接口。 82. 构造器注入和 setter 依赖注入，那种方式更好？**每种方式都有它的缺点和优点。构造器注入保证所有的注入都被初始化，但是 setter 注入提供更好的灵活性来设置可选依赖。如果使用 XML 来描述依赖，Setter 注入的可读写会更强。经验法则是强制依赖使用构造器注入，可选依赖使用 setter 注入。 83. 依赖注入和工厂模式之间有什么不同？虽然两种模式都是将对象的创建从应用的逻辑中分离，但是依赖注入比工程模式更清晰。通过依赖注入，你的类就是 POJO，它只知道依赖而不关心它们怎么获取。使用工厂模式，你的类需要通过工厂来获取依赖。因此，使用 DI 会比使用工厂模式更容易测试。 84. 适配器模式和装饰器模式有什么区别？虽然适配器模式和装饰器模式的结构类似，但是每种模式的出现意图不同。适配器模式被用于桥接两个接口，而装饰模式的目的是在不修改类的情况下给类增加新的功能。 85. 适配器模式和代理模式之前有什么不同？这个问题与前面的类似，适配器模式和代理模式的区别在于他们的意图不同。由于适配器模式和代理模式都是封装真正执行动作的类，因此结构是一致的，但是适配器模式用于接口之间的转换，而代理模式则是增加一个额外的中间层，以便支持分配、控制或智能访问。 86. 什么是模板方法模式？模板方法提供算法的框架，你可以自己去配置或定义步骤。例如，你可以将排序算法看做是一个模板。它定义了排序的步骤，但是具体的比较，可以使用 Comparable 或者其语言中类似东西，具体策略由你去配置。列出算法概要的方法就是众所周知的模板方法。 87. 什么时候使用访问者模式？访问者模式用于解决在类的继承层次上增加操作，但是不直接与之关联。这种模式采用双派发的形式来增加中间层。 88. 什么时候使用组合模式？组合模式使用树结构来展示部分与整体继承关系。它允许客户端采用统一的形式来对待单个对象和对象容器。当你想要展示对象这种部分与整体的继承关系时采用组合模式。 89. 继承和组合之间有什么不同？虽然两种都可以实现代码复用，但是组合比继承共灵活，因为组合允许你在运行时选择不同的实现。用组合实现的代码也比继承测试起来更加简单。 90. 描述 Java 中的重载和重写？重载和重写都允许你用相同的名称来实现不同的功能，但是重载是编译时活动，而重写是运行时活动。你可以在同一个类中重载方法，但是只能在子类中重写方法。重写必须要有继承。 91. OOP 中的 组合、聚合和关联有什么区别？如果两个对象彼此有关系，就说他们是彼此相关联的。组合和聚合是面向对象中的两种形式的关联。组合是一种比聚合更强力的关联。组合中，一个对象是另一个的拥有者，而聚合则是指一个对象使用另一个对象。如果对象 A 是由对象 B 组合的，则 A 不存在的话，B一定不存在，但是如果 A 对象聚合了一个对象 B，则即使 A 不存在了，B 也可以单独存在。 92. 给我一个符合开闭原则的设计模式的例子？开闭原则要求你的代码对扩展开放，对修改关闭。这个意思就是说，如果你想增加一个新的功能，你可以很容易的在不改变已测试过的代码的前提下增加新的代码。有好几个设计模式是基于开闭原则的，如策略模式，如果你需要一个新的策略，只需要实现接口，增加配置，不需要改变核心逻辑。一个正在工作的例子是 Collections.sort() 方法，这就是基于策略模式，遵循开闭原则的，你不需为新的对象修改 sort() 方法，你需要做的仅仅是实现你自己的 Comparator 接口。 93. 什么时候使用享元模式（蝇量模式）？享元模式通过共享对象来避免创建太多的对象。为了使用享元模式，你需要确保你的对象是不可变的，这样你才能安全的共享。JDK 中 String 池、Integer 池以及 Long 池都是很好的使用了享元模式的例子。 94. Java 中如何格式化一个日期？如格式化为 ddMMyyyy 的形式？Java 中，可以使用 SimpleDateFormat 类或者 joda-time 库来格式日期。DateFormat 类允许你使用多种流行的格式来格式化日期。 95. Java 中，怎么在格式化的日期中显示时区？pattern中加z yyyy-MM-dd HH:mm:ss.SSS Z 96. Java 中 java.util.Date 与 java.sql.Date 有什么区别？java.sql.Date是针对SQL语句使用的，它只包含日期而没有时间部分,它们都有getTime方法返回毫秒数，自然就可以直接构建。java.util.Date 是 java.sql.Date 的父类，前者是常用的表示时间的类，我们通常格式化或者得到当前时间都是用他，后者之后在读写数据库的时候用他，因为PreparedStament的setDate()的第2参数和ResultSet的getDate()方法的第2个参数都是java.sql.Date。 97. Java 中，如何计算两个日期之间的差距？12345678public static int dateDiff(Date d1, Date d2) throws Exception &#123;long n1 = d1.getTime();long n2 = d2.getTime();long diff = Math.abs(n1 - n2);diff /= 3600 * 1000 * 24;return diff;&#125; 98. Java 中，如何将字符串 YYYYMMDD 转换为日期？SimpleDateFormat的parse方法 99. 说出几条 Java 中方法重载的最佳实践？下面有几条可以遵循的方法重载的最佳实践来避免造成自动装箱的混乱。a）不要重载这样的方法：一个方法接收 int 参数，而另个方法接收 Integer 参数。b）不要重载参数数量一致，而只是参数顺序不同的方法。c）如果重载的方法参数个数多于 5 个，采用可变参数。 100. 说出 5 条 IO 的最佳实践IO 对 Java 应用的性能非常重要。理想情况下，你应该在你应用的关键路径上避免 IO 操作。下面是一些你应该遵循的 Java IO 最佳实践： a）使用有缓冲区的 IO 类，而不要单独读取字节或字符b）使用 NIO 和 NIO2c）在 finally 块中关闭流，或者使用 try-with-resource（Java7） 语句d）使用内存映射文件获取更快的 IO 101. Object有哪些公用方法？clone equals hashcode wait notify notifyall finalize toString getClass除了clone和finalize其他均为公共方法。 11个方法，wait被重载了两次 102. equals与==的区别区别1. ==是一个运算符 equals是Object类的方法 区别2. 比较时的区别 a. 用于基本类型的变量比较时：==用于比较值是否相等，equals不能直接用于基本数据类型的比较，需要转换为其对应的包装类型。b. 用于引用类型的比较时。==和equals都是比较栈内存中的地址是否相等 。相等为true 否则为false。但是通常会重写equals方法去实现对象内容的比较。 103. String、StringBuffer与StringBuilder的区别第一点：可变和适用范围。String对象是不可变的，而StringBuffer和StringBuilder是可变字符序列。每次对String的操作相当于生成一个新的String对象，而对StringBuffer和StringBuilder的操作是对对象本身的操作，而不会生成新的对象，所以对于频繁改变内容的字符串避免使用String，因为频繁的生成对象将会对系统性能产生影响。 第二点：线程安全。String由于有final修饰，是immutable的，安全性是简单而纯粹的。StringBuilder和StringBuffer的区别在于StringBuilder不保证同步，也就是说如果需要线程安全需要使用StringBuffer，不需要同步的StringBuilder效率更高。 104. switch能否用String做参数Java1.7开始支持，但实际这是一颗Java语法糖。除此之外，byte，short，long，枚举，boolean均可用于switch，只有浮点型不可以。 105. 封装、继承、多态封装： 1.概念：就是把对象的属性和操作（或服务）结合为一个独立的整体，并尽可能隐藏对象的内部实现细节。 2.好处： (1)隐藏内部实现细节。 继承： 1.概念：继承是从已有的类中派生出新的类，新的类能吸收已有类的数据属性和行为，并能扩展新的能力 2.好处：提高代码的复用，缩短开发周期。 多态： 1.概念：多态（Polymorphism）按字面的意思就是“多种状态，即同一个实体同时具有多种形式。一般表现形式是程序在运行的过程中，同一种类型在不同的条件下表现不同的结果。多态也称为动态绑定，一般是在运行时刻才能确定方法的具体执行对象。 2.好处：1）将接口和实现分开，改善代码的组织结构和可读性，还能创建可拓展的程序。2）消除类型之间的耦合关系。允许将多个类型视为同一个类型。3）一个多态方法的调用允许有多种表现形式 106. Comparable和Comparator接口区别Comparator位于包java.util下，而Comparable位于包java.lang下 如果我们需要使用Arrays或Collections的排序方法对对象进行排序时，我们需要在自定义类中实现Comparable接口并重写compareTo方法，compareTo方法接收一个参数，如果this对象比传递的参数小，相等或大时分别返回负整数、0、正整数。Comparable被用来提供对象的自然排序。String、Integer实现了该接口。 Comparator比较器的compare方法接收2个参数，根据参数的比较大小分别返回负整数、0和正整数。Comparator 是一个外部的比较器，当这个对象自然排序不能满足你的要求时，你可以写一个比较器来完成两个对象之间大小的比较。用 Comparator 是策略模式（strategy design pattern），就是不改变对象自身，而用一个策略对象（strategy object）来改变它的行为。 107. 与Java集合框架相关的有哪些最好的实践（1）根据需要选择正确的集合类型。比如，如果指定了大小，我们会选用Array而非ArrayList。如果我们想根据插入顺序遍历一个Map，我们需要使用TreeMap。如果我们不想重复，我们应该使用Set。 （2）一些集合类允许指定初始容量，所以如果我们能够估计到存储元素的数量，我们可以使用它，就避免了重新哈希或大小调整。 （3）基于接口编程，而非基于实现编程，它允许我们后来轻易地改变实现。 （4）总是使用类型安全的泛型，避免在运行时出现ClassCastException。 （5）使用JDK提供的不可变类作为Map的key，可以避免自己实现hashCode()和equals()。 108. IO和NIO简述1、简述 在以前的Java IO中，都是阻塞式IO，NIO引入了非阻塞式IO。第一种方式：我从硬盘读取数据，然后程序一直等，数据读完后，继续操作。这种方式是最简单的，叫阻塞IO。第二种方式：我从硬盘读取数据，然后程序继续向下执行，等数据读取完后，通知当前程序（对硬件来说叫中断，对程序来说叫回调），然后此程序可以立即处理数据，也可以执行完当前操作在读取数据。 2.流与块的比较 原来的 I/O 以流的方式处理数据，而 NIO 以块的方式处理数据。面向流 的 I/O 系统一次一个字节地处理数据。一个输入流产生一个字节的数据，一个输出流消费一个字节的数据。这样做是相对简单的。不利的一面是，面向流的 I/O 通常相当慢。一个 面向块 的 I/O 系统以块的形式处理数据。每一个操作都在一步中产生或者消费一个数据块。按块处理数据比按(流式的)字节处理数据要快得多。但是面向块的 I/O 缺少一些面向流的 I/O 所具有的优雅性和简单性。 3.通道与流 Channel是一个对象，可以通过它读取和写入数据。通道与流功能类似，不同之处在于通道是双向的。而流只是在一个方向上移动(一个流必须是 InputStream 或者 OutputStream 的子类)， 而通道可以用于读、写或者同时用于读写。 4.缓冲区Buffer 在 NIO 库中，所有数据都是用缓冲区处理的。在 NIO 库中，所有数据都是用缓冲区处理的。 Position: 表示下一次访问的缓冲区位置Limit: 表示当前缓冲区存放的数据容量。Capacity:表示缓冲区最大容量 flip()方法:读写模式切换 clear方法:它将 limit 设置为与 capacity 相同。它设置 position 为 0。 二、Java高级（JavaEE、框架、服务器、工具等）1. Servlet1.1 Servlet继承实现结构 1234Servlet(接口) --&gt; init|service|destroy方法GenericServlet(抽象类) --&gt; 与协议无关的ServletHttpServlet(抽象类) --&gt; 实现了http协议自定义Servlet --&gt; 重写doGet/doPost 1.2 编写Servlet的步骤 继承HttpServlet 重写doGet/doPost方法 在web.xml中注册servlet 1.3 Servlet生命周期 init:仅执行一次,负责装载servlet时初始化servlet对象 service:核心方法,一般get/post两种方式 destroy:停止并卸载servlet,释放资源 1.4 过程 客户端request请求 -&gt; 服务器检查Servlet实例是否存在 -&gt; 若存在调用相应service方法 客户端request请求 -&gt; 服务器检查Servlet实例是否存在 -&gt; 若不存在装载Servlet类并创建实例 -&gt; 调用init初始化 -&gt; 调用service 加载和实例化、初始化、处理请求、服务结束 1.5 doPost方法要抛出的异常:ServletExcception、IOException 1.6 Servlet容器装载Servlet web.xml中配置load-on-startup启动时装载 客户首次向Servlet发送请求 Servlet类文件被更新后, 重新装载Servlet 1.7 HttpServlet容器响应web客户请求流程 Web客户向servlet容器发出http请求 servlet容器解析Web客户的http请求 servlet容器创建一个HttpRequest对象, 封装http请求信息 servlet容器创建一个HttpResponse对象 servlet容器调用HttpServlet的service方法, 把HttpRequest和HttpResponse对象作为service方法的参数传给HttpServlet对象 HttpServlet调用httprequest的有关方法, 获取http请求信息 httpservlet调用httpresponse的有关方法, 生成响应数据 Servlet容器把HttpServlet的响应结果传给web客户 1.8 HttpServletRequest完成的一些功能 request.getCookie() request.getHeader(String s) request.getContextPath() request.getSession() 12HttpSession session = request.getSession(boolean create)// 返回当前请求的会话 1.9 HttpServletResponse完成一些的功能 设http响应头 设置Cookie 输出返回数据 1.10 Servlet与JSP九大内置对象的关系 JSP对象 怎样获得1234567891. out -&gt; response.getWriter2. request -&gt; Service方法中的req参数3. response -&gt; Service方法中的resp参数4. session -&gt; request.getSession5. application -&gt; getServletContext6. exception -&gt; Throwable7. page -&gt; this8. pageContext -&gt; PageContext9. Config -&gt; getServletConfig exception是JSP九大内置对象之一，其实例代表其他页面的异常和错误。只有当页面是错误处理页面时，即isErroePage为 true时，该对象才可以使用。 2. JSPJSP的前身就是Servlet 3. Tomcat3.1 Tomcat容器的等级 Tomcat - Container - Engine - Host - Servlet - 多个Context(一个Context对应一个web工程)-Wrapper 4. struts struts可进行文件上传 struts基于MVC模式 struts让流程结构更清晰 struts有许多action类, 会增加类文件数目 5. Hibernate的7大鼓励措施 尽量使用many-to-one, 避免使用单项one-to-many 灵活使用单项one-to-many 不用一对一, 使用多对一代替一对一 配置对象缓存, 不使用集合对象 一对多使用bag, 多对一使用set 继承使用显示多态 消除大表, 使用二级缓存 6. Hibernate延迟加载 Hibernate2延迟加载实现：a)实体对象 b)集合（Collection） Hibernate3 提供了属性的延迟加载功能当Hibernate在查询数据的时候，数据并没有存在与内存中，当程序真正对数据的操作时，对象才存在与内存中，就实现了延迟加载，他节省了服务器的内存开销，从而提高了服务器的性能。 hibernate使用Java反射机制，而不是字节码增强程序来实现透明性。 hibernate的性能非常好，因为它是个轻量级框架。映射的灵活性很出色。它支持各种关系数据库，从一对一到多对多的各种复杂关系。 7. Java 中，DOM 和 SAX 解析器有什么不同？DOM 解析器将整个 XML 文档加载到内存来创建一棵 DOM 模型树，这样可以更快的查找节点和修改 XML 结构，而 SAX 解析器是一个基于事件的解析器，不会将整个 XML 文档加载到内存。由于这个原因，DOM 比 SAX 更快，也要求更多的内存，但不适合于解析大的 XML 文件。 8. Java 中，Maven 和 ANT 有什么区别？虽然两者都是构建工具，都用于创建 Java 应用，但是 Maven 做的事情更多，在基于“约定优于配置”的概念下，提供标准的Java 项目结构，同时能为应用自动管理依赖（应用中所依赖的 JAR 文件）。 9. 解析XML不同方式对比DOM、SAX、JDOM、DOM4J [x] DOM DOM树驻留内存 可以进行修改和写入,耗费内存。 步骤：创建DocumentBuilderFactory对象 -&gt; 创建DocumentBuilder对象 -&gt; Document document = db.parse(“xml”) [x] SAX 事件驱动模式 获取一个SAXParserFactory工厂的实例 -&gt; 根据该实例获取SAXParser -&gt; 创建Handler对象 -&gt; 调用SAXParser的parse方法解析 用于读取节点数据 不易编码 事件有顺序 很难同时访问xml的多处数据 [x] JDOM 创建一个SAXBuilder的对象 -&gt; 创建一个输入流，加载xml文件 -&gt;通过saxBuilder的build方法将输入流加载至saxBuilder并接收Document对象 使用具体类而不使用接口 [x] DOM4J 通过SAXReader的read方法加载xml文件并获取document对象 使用接口和抽象类，灵活性好，功能强大 10. Nginx相关11. XML与JSON对比和区别XML 1231）应用广泛，可扩展性强，被广泛应用各种场合2）读取、解析没有JSON快3）可读性强，可描述复杂结构 JSON 1234561）结构简单，都是键值对2）读取、解析速度快，很多语言支持3）传输数据量小，传输速率大大提高4）描述复杂结构能力较弱JavaScript、PHP等原生支持，简化了读取解析。成为当前互联网时代普遍应用的数据结构。 三、多线程和并发0. Java 中的 volatile 变量是什么Java 语言提供了一种稍弱的同步机制,即volatile变量。但是volatile并不容易完全被正确、完整的理解。一般来说，volatile具备2条语义，或者说2个特性。第一是保证volatile修饰的变量对所有线程的可见性，这里的可见性是指当一条线程修改了该变量，新值对于其它线程来说是立即可以得知的。而普通变量做不到这一点。 第二条语义是禁止指令重排序优化，这条语义在JDK1.5才被修复。 关于第一点：根据JMM，所有的变量存储在主内存，而每个线程还有自己的工作内存，线程的工作内存保存该线程使用到的变量的主内存副本拷贝，线程对变量的操作在工作内存中进行，不能直接读写主内存的变量。在volatile可见性这一点上，普通变量做不到的原因正因如此。比如，线程A修改了一个普通变量的值，然后向主内存进行回写，线程B在线程A回写完成后再从主内存读取，新变量才能对线程B可见。其实，按照虚拟机规范，volatile变量依然有工作内存的拷贝，要借助主内存来实现可见性。但由于volatile的特殊规则保证了新值能立即同步回主内存，以及每次使用从主内存刷新，以此保证了多线程操作volatile变量的可见性。 关于第二点：先说指令重排序，指令重排序是指CPU采用了允许将多条指令不按规定顺序分开发送给相应的处理单元处理，但并不是说任意重排，CPU需要正确处理指令依赖情况确保最终的正确结果，指令重排序是机器级的优化操作。那么为什么volatile要禁止指令重排序呢，又是如何去做的。举例，DCL（双重检查加锁）的单例模式。volatile修饰后，代码中将会插入许多内存屏障指令保证处理器不发生乱序执行。同时由于Happens-before规则的保证，在刚才的例子中写操作会发生在后续的读操作之前。 除了以上2点，volatile还保证对于64位long和double的读取是原子性的。因为在JMM中允许虚拟机对未被volatile修饰的64位的long和double读写操作分为2次32位的操作来执行，这也就是所谓的long和double的非原子性协定。 基于以上几点，我们知道volatile虽然有这些语义和特性在并发的情况下仍然不能保证线程安全。大部分情况下仍然需要加锁。 除非是以下2种情况，1.运算结果不依赖变量的当前值，或者能够确保只有单一线程修改变量的值；2.变量不需要与其他的状态变量共同参与不变约束。 1. volatile简述Java 语言提供了一种稍弱的同步机制,即volatile变量.用来确保将变量的更新操作通知到其他线程,保证了新值能立即同步到主内存,以及每次使用前立即从主内存刷新。 当把变量声明为volatile类型后,编译器与运行时都会注意到这个变量是共享的。volatile修饰变量,每次被线程访问时强迫其从主内存重读该值,修改后再写回。保证读取的可见性,对其他线程立即可见。volatile的另一个语义是禁止指令重排序优化。但是volatile并不保证原子性,也就不能保证线程安全。 2. Java 中能创建 volatile 数组吗？能，Java 中可以创建 volatile 类型数组，不过只是一个指向数组的引用，而不是整个数组。我的意思是，如果改变引用指向的数组，将会受到 volatile 的保护，但是如果多个线程同时改变数组的元素，volatile 就不能起到之前的保护作用了。 3. volatile 能使得一个非原子操作变成原子操作吗？一个典型的例子是在类中有一个 long 类型的成员变量。如果你知道该成员变量会被多个线程访问，如计数器、价格等，你最好是将其设置为 volatile。为什么？因为 Java 中读取 long 类型变量不是原子的，需要分成两步，如果一个线程正在修改该 long 变量的值，另一个线程可能只能看到该值的一半（前 32 位）。但是对一个 volatile 型的 long 或 double 变量的读写是原子。 4. volatile 禁止指令重排序的底层原理指令重排序，是指CPU允许多条指令不按程序规定的顺序分开发送给相应电路单元处理。但并不是说任意重排，CPU需要能正确处理指令依赖情况以正确的执行结果。volatile禁止指令重排序是通过内存屏障实现的，指令重排序不能把后面的指令重排序到内存屏障之前。由内存屏障保证一致性。注：该条语义在JDK1.5才得以修复，这点也是JDK1.5之前无法通过双重检查加锁来实现单例模式的原因。 5. volatile 类型变量提供什么保证？volatile 变量提供有序性和可见性保证，例如，JVM 或者 JIT为了获得更好的性能会对语句重排序，但是 volatile 类型变量即使在没有同步块的情况下赋值也不会与其他语句重排序。 volatile 提供 happens-before 的保证，确保一个线程的修改能对其他线程是可见的。某些情况下，volatile 还能提供原子性，如读 64 位数据类型，像 long 和 double 都不是原子的，但 volatile 类型的 double 和 long 就是原子的。 volatile的使用场景： 运算结果不依赖变量的当前值，或者能够确保只有单一的线程修改该值 变量不需要与其他状态变量共同参与不变约束 6. volatile的性能volatile变量的读操作性能消耗和普通变量差不多，但是写操作可能相对慢一些，因为它需要在本地代码中插入许多内存屏障指令以确保处理器不发生乱序执行。大多数情况下，volatile总开销比锁低，但我们要注意volatile的语义能否满足使用场景。 7. 10 个线程和 2 个线程的同步代码，哪个更容易写？从写代码的角度来说，两者的复杂度是相同的，因为同步代码与线程数量是相互独立的。但是同步策略的选择依赖于线程的数量，因为越多的线程意味着更大的竞争，所以你需要利用同步技术，如锁分离，这要求更复杂的代码和专业知识。 8. 你是如何调用 wait（）方法的？使用 if 块还是循环？为什么？wait() 方法应该在循环调用，因为当线程获取到 CPU 开始执行的时候，其他条件可能还没有满足，所以在处理前，循环检测条件是否满足会更好。下面是一段标准的使用 wait 和 notify 方法的代码： 123456// The standard idiom for using the wait methodsynchronized (obj) &#123;while (condition does not hold)obj.wait(); // (Releases lock, and reacquires on wakeup)... // Perform action appropriate to condition&#125; 参见 Effective Java 第 69 条，获取更多关于为什么应该在循环中来调用 wait 方法的内容。 9. 什么是多线程环境下的伪共享（false sharing）？伪共享是多线程系统（每个处理器有自己的局部缓存）中一个众所周知的性能问题。伪共享发生在不同处理器的上的线程对变量的修改依赖于相同的缓存行，如下图所示： 伪共享问题很难被发现，因为线程可能访问完全不同的全局变量，内存中却碰巧在很相近的位置上。如其他诸多的并发问题，避免伪共享的最基本方式是仔细审查代码，根据缓存行来调整你的数据结构。 10. 线程的run方法和start方法 run方法 只是thread类的一个普通方法,若直接调用程序中依然只有主线程这一个线程,还要顺序执行,依然要等待run方法体执行完毕才可执行下面的代码。 start方法 用start方法来启动线程,是真正实现了多线程。调用thread类的start方法来启动一个线程,此时线程处于就绪状态,一旦得到cpu时间片,就开始执行run方法。 11. ReadWriteLock(读写锁)写写互斥 读写互斥 读读并发, 在读多写少的情况下可以提高效率 12. resume(继续挂起的线程)和suspend(挂起线程)一起用13. wait与notify、notifyall一起用14. sleep与wait的异同点 sleep是Thread类的静态方法, wait来自object类 sleep方法短暂停顿不释放锁, wait方法条件等待要释放锁，因为只有这样，其他等待的线程才能在满足条件时获取到该锁。 wait, notify, notifyall必须在同步代码块中使用, sleep可以在任何地方使用 都可以抛出InterruptedException 15. 让一个线程停止执行异常 - 停止执行休眠 - 停止执行阻塞 - 停止执行 16. ThreadLocal简介16.1 ThreadLocal解决了变量并发访问的冲突问题 当使用ThreadLocal维护变量时,ThreadLocal为每个使用该变量的线程提供独立的变量副本,每个线程都可以独立地改变自己的副本,而不会影响其它线程所对应的副本,是线程隔离的。线程隔离的秘密在于ThreadLocalMap类(ThreadLocal的静态内部类) 16.2 与synchronized同步机制的比较 首先,它们都是为了解决多线程中相同变量访问冲突问题。不过,在同步机制中,要通过对象的锁机制保证同一时间只有一个线程访问该变量。该变量是线程共享的, 使用同步机制要求程序缜密地分析什么时候对该变量读写, 什么时候需要锁定某个对象, 什么时候释放对象锁等复杂的问题,程序设计编写难度较大, 是一种“以时间换空间”的方式。而ThreadLocal采用了以“以空间换时间”的方式。 17. 线程局部变量原理当使用ThreadLocal维护变量时,ThreadLocal为每个使用该变量的线程提供独立的变量副本,每个线程都可以独立地改变自己的副本,而不会影响其它线程所对应的副本,是线程隔离的。线程隔离的秘密在于ThreadLocalMap类(ThreadLocal的静态内部类) 线程局部变量是局限于线程内部的变量，属于线程自身所有，不在多个线程间共享。Java 提供 ThreadLocal 类来支持线程局部变量，是一种实现线程安全的方式。但是在管理环境下（如 web 服务器）使用线程局部变量的时候要特别小心，在这种情况下，工作线程的生命周期比任何应用变量的生命周期都要长。任何线程局部变量一旦在工作完成后没有释放，Java 应用就存在内存泄露的风险。 ThreadLocal的方法：void set(T value)、T get()以及T initialValue()。 ThreadLocal是如何为每个线程创建变量的副本的： 首先，在每个线程Thread内部有一个ThreadLocal.ThreadLocalMap类型的成员变量threadLocals，这个threadLocals就是用来存储实际的变量副本的，键值为当前ThreadLocal变量，value为变量副本（即T类型的变量）。初始时，在Thread里面，threadLocals为空，当通过ThreadLocal变量调用get()方法或者set()方法，就会对Thread类中的threadLocals进行初始化，并且以当前ThreadLocal变量为键值，以ThreadLocal要保存的副本变量为value，存到threadLocals。然后在当前线程里面，如果要使用副本变量，就可以通过get方法在threadLocals里面查找。 总结： 实际的通过ThreadLocal创建的副本是存储在每个线程自己的threadLocals中的 为何threadLocals的类型ThreadLocalMap的键值为ThreadLocal对象，因为每个线程中可有多个threadLocal变量，就像上面代码中的longLocal和stringLocal； 在进行get之前，必须先set，否则会报空指针异常；如果想在get之前不需要调用set就能正常访问的话，必须重写initialValue()方法 18. JDK提供的用于并发编程的同步器 Semaphore Java并发库的Semaphore可以很轻松完成信号量控制，Semaphore可以控制某个资源可被同时访问的个数，通过 acquire() 获取一个许可，如果没有就等待，而 release() 释放一个许可。 CyclicBarrier 主要的方法就是一个：await()。await()方法每被调用一次，计数便会减少1，并阻塞住当前线程。当计数减至0时，阻塞解除，所有在此CyclicBarrier上面阻塞的线程开始运行。 CountDownLatch 直译过来就是倒计数(CountDown)门闩(Latch)。倒计数不用说，门闩的意思顾名思义就是阻止前进。在这里就是指 CountDownLatch.await() 方法在倒计数为0之前会阻塞当前线程。 19. 什么是 Busy spin？我们为什么要使用它？Busy spin 是一种在不释放 CPU 的基础上等待事件的技术。它经常用于避免丢失 CPU 缓存中的数据（如果线程先暂停，之后在其他CPU上运行就会丢失）。所以，如果你的工作要求低延迟，并且你的线程目前没有任何顺序，这样你就可以通过循环检测队列中的新消息来代替调用 sleep() 或 wait() 方法。它唯一的好处就是你只需等待很短的时间，如几微秒或几纳秒。LMAX 分布式框架是一个高性能线程间通信的库，该库有一个 BusySpinWaitStrategy 类就是基于这个概念实现的，使用 busy spin 循环 EventProcessors 等待屏障。 20. Java 中怎么获取一份线程 dump 文件？在 Linux 下，你可以通过命令 kill -3 PID （Java 进程的进程 ID）来获取 Java 应用的 dump 文件。在 Windows 下，你可以按下 Ctrl + Break 来获取。这样 JVM 就会将线程的 dump 文件打印到标准输出或错误文件中，它可能打印在控制台或者日志文件中，具体位置依赖应用的配置。 21. Swing 是线程安全的？不是，Swing 不是线程安全的。你不能通过任何线程来更新 Swing 组件，如 JTable、JList 或 JPanel，事实上，它们只能通过 GUI 或 AWT 线程来更新。这就是为什么 Swing 提供 invokeAndWait() 和 invokeLater() 方法来获取其他线程的 GUI 更新请求。这些方法将更新请求放入 AWT 的线程队列中，可以一直等待，也可以通过异步更新直接返回结果。 22. 用 wait-notify 写一段代码来解决生产者-消费者问题？记住在同步块中调用 wait() 和 notify()方法，如果阻塞，通过循环来测试等待条件。 23. 用 Java 写一个线程安全的单例模式（Singleton）？当我们说线程安全时，意思是即使初始化是在多线程环境中，仍然能保证单个实例。Java 中，使用枚举作为单例类是最简单的方式来创建线程安全单例模式的方式。 24. Java 中，编写多线程程序的时候你会遵循哪些最佳实践？这是我在写Java 并发程序的时候遵循的一些最佳实践： a）给线程命名，这样可以帮助调试。 b）最小化同步的范围，而不是将整个方法同步，只对关键部分做同步。 c）如果可以，更偏向于使用 volatile 而不是 synchronized。 d）使用更高层次的并发工具，而不是使用 wait() 和 notify() 来实现线程间通信，如 BlockingQueue，CountDownLatch 及 Semeaphore。 e）优先使用并发集合，而不是对集合进行同步。并发集合提供更好的可扩展性。 25. 说出至少 5 点在 Java 中使用线程的最佳实践。这个问题与之前的问题类似，你可以使用上面的答案。对线程来说，你应该： a）对线程命名 b）将线程和任务分离，使用线程池执行器来执行 Runnable 或 Callable。 c）使用线程池 26. 在多线程环境下，SimpleDateFormat 是线程安全的吗？不是，非常不幸，DateFormat 的所有实现，包括 SimpleDateFormat 都不是线程安全的，因此你不应该在多线程序中使用，除非是在对外线程安全的环境中使用，如将 SimpleDateFormat 限制在 ThreadLocal 中。如果你不这么做，在解析或者格式化日期的时候，可能会获取到一个不正确的结果。因此，从日期、时间处理的所有实践来说，我强力推荐 joda-time 库。 27. Happens-Before规则 程序次序规则 按控制流顺序先后发生 管程锁定规则 一个unlock操作先行发生于后面对同一个锁的lock操作 volatile变量规则 对一个volatile变量的写操作先行发生于后面对这个变量的读操作 线程启动规则 start方法先行发生于线程的每一个动作 线程中断规则 对线程的interrupt方法调用先行发生于被中断线程的代码检测到中断时间的发生 线程终止规则 线程内的所有操作都先行发生于对此线程的终止检测 对象终结规则 一个对象的初始化完成先行发生于它的finalize方法的开始 传递性 如果A先行发生于操作B，B先行发生于操作C，则A先行发生于操作C 28. 什么是线程线程是操作系统能够进行运算调度的最小单位，它被包含在进程之中，是进程中的实际运作单位。程序员可以通过它进行多处理器编程，可以使用多线程对运算密集型任务提速。比如，如果一个线程完成一个任务要100 毫秒，那么用十个线程完成改任务只需 10 毫秒。Java在语言层面对多线程提供了很好的支持。 29. 线程和进程有什么区别从概念上： 进程：一个程序对一个数据集的动态执行过程，是分配资源的基本单位。线程：存在于进程内，是进程内的基本调度单位。共享进程的资源。 从执行过程中来看： 进程：拥有独立的内存单元，而多个线程共享内存，从而提高了应用程序的运行效率。线程：每一个独立的线程，都有一个程序运行的入口、顺序执行序列、和程序的出口。但是线程不能够独立的执行，必须依存在应用程序中，由应用程序提供多个线程执行控制。 从逻辑角度来看：（重要区别） 多线程的意义在于一个应用程序中，有多个执行部分可以同时执行。但是，操作系统并没有将多个线程看做多个独立的应用，来实现进程的调度和管理及资源分配。 简言之，一个程序至少有一个进程,一个进程至少有一个线程。进程是资源分配的基本单位，线程共享进程的资源。 30. 用 Runnable 还是 ThreadJava 不支持类的多重继承，但允许你调用多个接口。所以如果你要继承其他类，当然是实现Runnable接口好了。 31. Java 中 Runnable 和 Callable 有什么不同Runnable和 Callable 都代表那些要在不同的线程中执行的任务。Runnable 从 JDK1.0 开始就有了，Callable 是在 JDK1.5 增加的。它们的主要区别是 Callable 的 call () 方法可以返回值和抛出异常，而 Runnable 的 run ()方法没有这些功能。 32. Java 中 CyclicBarrier 和 CountDownLatch 有什么不同它们都是JUC下的类，CyclicBarrier 和 CountDownLatch 都可以用来让一组线程等待其它线程。区别在于CountdownLatch计数无法被重置。如果需要重置计数，请考虑使用 CyclicBarrier。 33. Java 内存模型是什么Java 内存模型规定和指引Java 程序在不同的内存架构、CPU 和操作系统间有确定性地行为。它在多线程的情况下尤其重要。Java内存模型对一个线程所做的变动能被其它线程可见提供了保证，它们之间是先行发生关系。这个关系定义了一些规则让程序员在并发编程时思路更清晰。 线程内的代码能够按先后顺序执行，这被称为程序次序规则。 对于同一个锁，一个解锁操作一定要发生在时间上后发生的另一个锁定操作之前，也叫做管程锁定规则。 前一个对volatile的写操作在后一个volatile的读操作之前，也叫volatile变量规则。 一个线程内的任何操作必需在这个线程的 start ()调用之后，也叫作线程启动规则。 一个线程的所有操作都会在线程终止之前，线程终止规则。 一个对象的终结操作必需在这个对象构造完成之后，也叫对象终结规则。 a先行于b，b先行于c，传递性 34. 什么是线程安全？Vector 是一个线程安全类吗如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。一个线程安全的计数器类的同一个实例对象在被多个线程使用的情况下也不会出现计算失误。很显然你可以将集合类分成两组，线程安全和非线程安全的。Vector 是用同步方法来实现线程安全的，而和它相似的 ArrayList 不是线程安全的。 35. Java 中什么是竞态条件？ 举个例子说明。竞态条件会导致程序在并发情况下出现一些 bugs。多线程对一些资源的竞争的时候就会产生竞态条件，如果首先要执行的程序竞争失败排到后面执行了，那么整个程序就会出现一些不确定的 bugs。这种 bugs 很难发现而且会重复出现，因为线程间的随机竞争。几类竞态条件check-and-act、读取-修改-写入、put-if-absent。 36. Java 中如何停止一个线程当 run () 或者 call () 方法执行完的时候线程会自动结束，如果要手动结束一个线程，你可以用 volatile 布尔变量来退出 run ()方法的循环或者是取消任务来中断线程。其他情形：异常 - 停止执行 休眠 - 停止执行 阻塞 - 停止执行 37. 一个线程运行时发生异常会怎样简单的说，如果异常没有被捕获该线程将会停止执行。Thread.UncaughtExceptionHandler 是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候 JVM 会使用 Thread.getUncaughtExceptionHandler ()来查询线程的 UncaughtExceptionHandler 并将线程和异常作为参数传递给 handler 的 uncaughtException ()方法进行处理。 38. 如何在两个线程间共享数据？通过共享对象来实现这个目的，或者是使用像阻塞队列这样并发的数据结构 39. Java 中 notify 和 notifyAll 有什么区别notify ()方法不能唤醒某个具体的线程，所以只有一个线程在等待的时候它才有用武之地。而 notifyAll ()唤醒所有线程并允许他们争夺锁确保了至少有一个线程能继续运行。 40. 为什么 wait, notify 和 notifyAll 这些方法不在 thread 类里面一个很明显的原因是 JAVA 提供的锁是对象级的而不是线程级的。如果线程需要等待某些锁那么调用对象中的 wait ()方法就有意义了。如果 wait ()方法定义在 Thread 类中，线程正在等待的是哪个锁就不明显了。简单的说，由于 wait，notify 和 notifyAll 都是锁级别的操作，所以把他们定义在 Object 类中因为锁属于对象。 41. 什么是 FutureTask？在 Java 并发程序中 FutureTask 表示一个可以取消的异步运算。它有启动和取消运算、查询运算是否完成和取回运算结果等方法。只有当运算完成的时候结果才能取回，如果运算尚未完成 get 方法将会阻塞。一个 FutureTask 对象可以对调用了 Callable 和 Runnable 的对象进行包装，由于 FutureTask 也是调用了 Runnable 接口所以它可以提交给 Executor 来执行。 42. Java 中 interrupted 和 isInterruptedd 方法的区别interrupted是静态方法，isInterruptedd是一个普通方法 如果当前线程被中断（没有抛出中断异常，否则中断状态就会被清除），你调用interrupted方法，第一次会返回true。然后，当前线程的中断状态被方法内部清除了。第二次调用时就会返回false。如果你刚开始一直调用isInterrupted，则会一直返回true，除非中间线程的中断状态被其他操作清除了。也就是说isInterrupted 只是简单的查询中断状态，不会对状态进行修改。 43. 为什么 wait 和 notify 方法要在同步块中调用如果不这么做，代码会抛出 IllegalMonitorStateException异常。还有一个原因是为了避免 wait 和 notify 之间产生竞态条件。 44. 为什么你应该在循环中检查等待条件？处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。因此，当一个等待线程醒来时，不能认为它原来的等待状态仍然是有效的，在 notify 方法调用之后和等待线程醒来之前这段时间它可能会改变。这就是在循环中使用 wait 方法效果更好的原因。 45. Java 中的同步集合与并发集合有什么区别同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在 Java1.5 之前程序员们只有同步集合来用且在多线程并发的时候会导致争用，阻碍了系统的扩展性。Java1.5加入了并发集合像 ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分区等现代技术提高了可扩展性。它们大部分位于JUC包下。 46. 什么是线程池？ 为什么要使用它？创建线程要花费昂贵的资源和时间，如果任务来了才创建线程那么响应时间会变长，而且一个进程能创建的线程数有限。为了避免这些问题，在程序启动的时候就创建若干线程来响应处理，它们被称为线程池，里面的线程叫工作线程。从 JDK1.5 开始，Java API 提供了 Executor 框架让你可以创建不同的线程池。比如单线程池，每次处理一个任务；数目固定的线程池或者是缓存线程池（一个适合很多生存期短的任务的程序的可扩展线程池）。 47. 如何写代码来解决生产者消费者问题？在现实中你解决的许多线程问题都属于生产者消费者模型，就是一个线程生产任务供其它线程进行消费，你必须知道怎么进行线程间通信来解决这个问题。比较低级的办法是用 wait 和 notify 来解决这个问题，比较赞的办法是用 Semaphore 或者 BlockingQueue 来实现生产者消费者模型。 48.如何避免死锁？死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起无法完成任务，死锁的发生必须满足以下四个条件： 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁。 49. Java 中活锁和死锁有什么区别？活锁和死锁类似，不同之处在于处于活锁的线程或进程的状态是不断改变的，活锁可以认为是一种特殊的饥饿。一个现实的活锁例子是两个人在狭小的走廊碰到，两个人都试着避让对方好让彼此通过，但是因为避让的方向都一样导致最后谁都不能通过走廊。简单的说就是，活锁和死锁的主要区别是前者进程的状态可以改变但是却不能继续执行。 50. 怎么检测一个线程是否拥有锁在 java.lang.Thread 中有一个方法叫 holdsLock，当且仅当当前线程拥有某个具体对象的锁时它返回true。 51. 你如何在 Java 中获取线程堆栈在 Linux 下，你可以通过命令 kill -3 PID （Java 进程的进程 ID）来获取 Java 应用的 dump 文件。在 Windows 下，你可以按下 Ctrl + Break 来获取。这样 JVM 就会将线程的 dump 文件打印到标准输出或错误文件中，它可能打印在控制台或者日志文件中，具体位置依赖应用的配置。 52.Java 中 synchronized 和 ReentrantLock 有什么不同Java 在过去很长一段时间只能通过 synchronized 关键字来实现互斥，它有一些缺点。比如你不能扩展锁之外的方法或者块边界，尝试获取锁时不能中途取消等。Java 5 通过 Lock 接口提供了更复杂的控制来解决这些问题。 ReentrantLock 类实现了 Lock，它拥有与 synchronized 相同的并发性和内存语义且它还具有可扩展性。 53.有三个线程 T1，T2，T3，怎么确保它们按顺序执行可以用线程类的 join ()方法。具体操作是在T3的run方法中调用t2.join()，让t2执行完再执行t3；T2的run方法中调用t1.join()，让t1执行完再执行t2。这样就按T1，T2，T3的顺序执行了 54.Thread 类中的 yield 方法有什么作用Yield 方法可以暂停当前正在执行的线程对象，让其它有相同优先级的线程执行。它是一个静态方法而且只保证当前线程放弃 CPU 占用而不能保证使其它线程一定能占用 CPU，执行 yield的线程有可能在进入到暂停状态后马上又被执行。 55.Java 中 ConcurrentHashMap 的并发度是什么ConcurrentHashMap 把实际 map 划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是 ConcurrentHashMap 类构造函数的一个可选参数，默认值为 16，这样在多线程情况下就能避免争用。 56.Java 中 Semaphore是什么JUC下的一种新的同步类，它是一个计数信号。从概念上讲，Semaphore信号量维护了一个许可集合。如有必要，在许可可用前会阻塞每一个 acquire，然后再获取该许可。每个 release添加一个许可，从而可能释放一个正在阻塞的获取者。但是，不使用实际的许可对象，Semaphore 只对可用许可的号码进行计数，并采取相应的行动。信号量常常用于多线程的代码中，比如数据库连接池。 57.如果你提交任务时，线程池队列已满。会发会生什么？这个问题问得很狡猾，许多程序员会认为该任务会阻塞直到线程池队列有空位。事实上如果一个任务不能被调度执行那么 ThreadPoolExecutor’s submit ()方法将会抛出一个 RejectedExecutionException 异常。 58.Java 线程池中 submit () 和 execute ()方法有什么区别两个方法都可以向线程池提交任务，execute ()方法的返回类型是 void，它定义在 Executor 接口中， 而 submit ()方法可以返回持有计算结果的 Future 对象，它定义在 ExecutorService 接口中，它扩展了 Executor 接口，其它线程池类像 ThreadPoolExecutor 和 ScheduledThreadPoolExecutor 都有这些方法。 59.什么是阻塞式方法？阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket 的 accept ()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。 60.Swing 是线程安全的吗？你可以很肯定的给出回答，Swing 不是线程安全的。你不能通过任何线程来更新 Swing 组件，如 JTable、JList 或 JPanel，事实上，它们只能通过 GUI 或 AWT 线程来更新。这就是为什么 Swing 提供 invokeAndWait() 和 invokeLater() 方法来获取其他线程的 GUI 更新请求。这些方法将更新请求放入 AWT 的线程队列中，可以一直等待，也可以通过异步更新直接返回结果。 61.Java 中 invokeAndWait 和 invokeLater 有什么区别这两个方法是 Swing API 提供给 Java 开发者用来从当前线程而不是事件派发线程更新 GUI 组件用的。InvokeAndWait ()同步更新 GUI 组件，比如一个进度条，一旦进度更新了，进度条也要做出相应改变。如果进度被多个线程跟踪，那么就调用 invokeAndWait ()方法请求事件派发线程对组件进行相应更新。而 invokeLater ()方法是异步调用更新组件的。 62.Swing API 中那些方法是线程安全的？虽然Swing不是线程安全的但是有一些方法是可以被多线程安全调用的。如repaint ()， revalidate ()。 JTextComponent 的 setText ()方法和 JTextArea 的 insert () 和 append () 方法也是线程安全的。 63.如何在 Java 中创建 Immutable 对象Immutable 对象可以在没有同步的情况下共享，降低了对该对象进行并发访问时的同步化开销。可是 Java 没有@Immutable 这个注解符，要创建不可变类，要实现下面几个步骤：通过构造方法初始化所有成员、对变量不要提供 setter 方法、将所有的成员声明为私有的，这样就不允许直接访问这些成员、在 getter 方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝。 64.Java 中的 ReadWriteLock 是什么？一般而言，读写锁是用来提升并发程序性能的锁分离技术的成果。Java 中的 ReadWriteLock 是 Java 5 中新增的一个接口，一个 ReadWriteLock 维护一对关联的锁，一个用于只读操作一个用于写。在没有写线程的情况下一个读锁可能会同时被多个读线程持有。写锁是独占的，你可以使用 JDK 中的 ReentrantReadWriteLock 来实现这个规则，它最多支持 65535 个写锁和 65535 个读锁。 65.多线程中的忙循环是什么?忙循环就是程序员用循环让一个线程等待，不像传统方法 wait ()， sleep () 或 yield () 它们都放弃了 CPU 控制，而忙循环不会放弃 CPU，它就是在运行一个空循环。这么做的目的是为了保留 CPU 缓存，在多核系统中，一个等待线程醒来的时候可能会在另一个内核运行，这样会重建缓存。为了避免重建缓存和减少等待重建的时间就可以使用它了。 66.volatile 变量和 atomic 变量有什么不同volatile 变量和 atomic 变量看起来很像，但功能却不一样。volatile 变量可以确保先行关系，即写操作会发生在后续的读操作之前， 但它并不能保证原子性。例如用 volatile 修饰 count 变量那么 count++ 操作并不是原子性的。而 AtomicInteger 类提供的 atomic 方法可以让这种操作具有原子性如 getAndIncrement ()方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。 67.如果同步块内的线程抛出异常会发生什么？无论你的同步块是正常还是异常退出的，里面的线程都会释放锁，所以对比锁接口我更喜欢同步块，因为它不用我花费精力去释放锁，该功能可以在 finally block 里释放锁实现。 68.如何在 Java 中创建线程安全的 Singleton5种，急加载，同步方法，双检锁，静态内部类，枚举 69.如何强制启动一个线程？这个问题就像是如何强制进行 Java 垃圾回收，目前还没有觉得方法，虽然你可以使用 System.gc ()来进行垃圾回收，但是不保证能成功。在 Java 里面没有办法强制启动一个线程，它是被线程调度器控制着且 Java 没有公布相关的 API。 70.Java 中的 fork join 框架是什么？fork join 框架是 JDK7 中出现的一款高效的工具，Java 开发人员可以通过它充分利用现代服务器上的多处理器。它是专门为了那些可以递归划分成许多子模块设计的，目的是将所有可用的处理能力用来提升程序的性能。fork join 框架一个巨大的优势是它使用了工作窃取算法，可以完成更多任务的工作线程可以从其它线程中窃取任务来执行。 71.Java 多线程中调用 wait () 和 sleep ()方法有什么不同？Java 程序中 wait 和 sleep 都会造成某种形式的暂停，它们可以满足不同的需要。wait ()方法意味着条件等待，如果等待条件为真且其它线程被唤醒时它会释放锁，而 sleep ()方法仅仅释放 CPU 资源或者让当前线程短暂停顿，但不会释放锁。 72.可重入锁可重入锁：如果当前线程已经获得了某个监视器对象所持有的锁，那么该线程在该方法中调用另外一个同步方法也同样持有该锁。 12345678public synchrnozied void test() &#123; xxxxxx; test2();&#125;public synchronized void test2() &#123; yyyyy;&#125; 在上面代码段中，执行 test 方法需要获得当前对象作为监视器的对象锁，但方法中又调用了 test2 的同步方法。 如果锁是具有可重入性的话，那么该线程在调用 test2 时并不需要再次获得当前对象的锁，可以直接进入 test2 方法进行操作。 如果锁是不具有可重入性的话，那么该线程在调用test2前会等待当前对象锁的释放，实际上该对象锁已被当前线程所持有，不可能再次获得。 如果锁是不具有可重入性特点的话，那么线程在调用同步方法、含有锁的方法时就会产生死锁。 73. 同步方法和同步代码块同步方法默认用this或者当前类class对象作为锁；同步代码块可以选择以什么来加锁，比同步方法要更细颗粒度，我们可以选择只同步会发生同步问题的部分代码而不是整个方法。 四、Java虚拟机0. 对哪些区域回收Java运行时数据区域：程序计数器、JVM栈、本地方法栈、方法区和堆。 由于程序计数器、JVM栈、本地方法栈3个区域随线程而生随线程而灭，对这几个区域内存的回收和分配具有确定性。而方法区和堆则不一样，程序需要在运行时才知道创建哪些对象，对这部分内存的分配是动态的，GC关注的也就是这部分内存。 1. 主动GC调用system.gc() Runtime.getRuntime.gc() 2. 垃圾回收释放那些不在持有任何引用的对象的内存 3. 怎样判断是否需要收集 引用计数法：对象没有任何引用与之关联(无法解决循环引用) ext：Python使用引用计数法 可达性分析法：通过一组称为GC Root的对象为起点,从这些节点向下搜索，如果某对象不能从这些根对象的一个(至少一个)所到达,则判定该对象应当回收。 ext：可作为GCRoot的对象：虚拟机栈中引用的对象。方法区中类静态属性引用的对象，方法区中类常量引用的对象，本地方法栈中JNI引用的对象 4.对象的自我救赎即使在可达性算法中判定为不可达时，也并非一定被回收。对象存在自我救赎的可能。要真正宣告对象的死亡，需要经历2次标记的过程。如果对象经过可达性分析法发现不可达时，对象将被第一次标记被进行筛选，筛选的条件是此对象是否有必要执行finalize方法。如果对象没有重写finalize方法或finalize方法已经被JVM调用过，则判定为不需要执行。 如果对象被判定为需要执行finalize方法，该对象将被放置在一个叫做F-Queue的队列中，JVM会建立一个低优先级的线程执行finalize方法，如果对象想要完成自我救赎需要在finalize方法中与引用链上的对象关联，比如把自己也就是this赋值给某个类变量。当GC第二次对F-Queue中对象标记时，该对象将被移出“即将回收”的集合，完成自我救赎。简言之，finalize方法是对象逃脱死亡命运的最后机会，并且任何对象的finalize方法只会被JVM调用一次。 5.垃圾回收算法Mark-Sweep法：标记清除法 容易产生内存碎片，导致分配较大对象时没有足够的连续内存空间而提前出发GC。这里涉及到另一个问题，即对象创建时的内存分配，对象创建内存分配主要有2种方法，分别是指针碰撞法和空闲列表法。指针碰撞法：使用的内存在一侧，空闲的在另一侧，中间使用一个指针作为分界点指示器，对象内存分配时只要指针向空闲的移动对象大小的距离即可。空闲列表法：使用的和空闲的内存相互交错无法进行指针碰撞，JVM必须维护一个列表记录哪些内存块可用，分配时从列表中找出一个足够的分配给对象，并更新列表记录。所以，当采用Mark-Sweep算法的垃圾回收器时，内存分配通常采用空闲列表法。 Copy法：将内存分为2块，每次使用其中的一块，当一块满了，将存活的对象复制到另一块，把使用过的那一块一次性清除。显然，Copy法解决了内存碎片的问题，但算法的代价是内存缩小为原来的一半。现代的垃圾收集器对新生代采用的正是Copy算法。但通常不执行1:1的策略，HotSpot虚拟机默认Eden区Survivor区8:1。每次使用Eden和其中一块Survivor区。也就是说新生代可用内存为新生代内存空间的90%。 Mark-Compact法：标记整理法。它的第一阶段与Mark-Sweep法一样，但不直接清除，而是将存活对象向一端移动，然后清除端边界以外的内存，这样也不存在内存碎片。 分代收集算法：将堆内存划分为新生代，老年代，根据新生代老年代的特点选取不同的收集算法。因为新生代对象大多朝生夕死，而老年代对象存活率高，没有额外空间进行分配担保，通常对新生代执行复制算法，老年代执行Mark-Sweep算法或Mark-Compact算法。 6.垃圾收集器通常来说，新生代老年代使用不同的垃圾收集器。新生代的垃圾收集器有Serial（单线程）、ParNew（Serial的多线程版本）、ParallelScavenge（吞吐量优先的垃圾收集器），老年代有SerialOld（单线程老年代）、ParallelOld（与ParallelScavenge搭配的多线程执行标记整理算法的老年代收集器）、CMS（标记清除算法，容易产生内存碎片，可以开启内存整理的参数），以及当前最先进的垃圾收集器G1，G1通常面向服务器端的垃圾收集器，在我自己的Java应用程序中通过-XX:+PrintGCDetails，发现自己的垃圾收集器是使用了ParallelScavenge + ParallelOld的组合。 7. 不同垃圾回收算法对比 标记清除法(Mark-Sweeping):易产生内存碎片 复制回收法(Copying)：为了解决Mark-Sweep法而提出,内存空间减至一半 标记压缩法(Mark-Compact):为了解决Copying法的缺陷,标记后移动到一端再清除 分代回收法(GenerationalCollection):新生代对象存活周期短,需要大量回收对象,需要复制的少,执行copy算法;老年代对象存活周期相对长,回收少量对象,执行mark-compact算法.新生代划分：较大的eden区 和 2个survivor区 8. 内存分配 新生代的三部分 |Eden Space|From Space|To Space|，对象主要分配在新生代的Eden区 大对象直接进入老年代大对象比如大数组直接进入老年代，可通过虚拟机参数-XX：PretenureSizeThreshold参数设置 长期存活的对象进入老年代ext：虚拟机为每个对象定义一个年龄计数器，如果对象在Eden区出生并经过一次MinorGC仍然存活，将其移入Survivor的To区，GC完成标记互换后，相当于存活的对象进入From区，对象年龄加1，当增加到默认15岁时，晋升老年代。可通过-XX：MaxTenuringThreshold设置 GC的过程：GC开始前，对象只存在于Eden区和From区，To区逻辑上始终为空。对象分配在Eden区，Eden区空间不足，发起MinorGC，将Eden区所有存活的对象复制到To区，From区存活的对象根据年龄判断去向，若到达年龄阈值移入老年代，否则也移入To区，GC完成后Eden区和From区被清空，From区和To区标记互换。对象每在Survivor区躲过一次MinorGC年龄加一。MinorGC将重复这样的过程，直到To区被填满，To区满了以后，将把所有对象移入老年代。 动态对象年龄判定 suvivor区相同年龄对象总和大于suvivor区空间的一半,年龄大于等于该值的对象直接进入老年代 空间分配担保 在MinorGC开始前，虚拟机检查老年代最大可用连续空间是否大于新生代所有对象总空间，如果成立，MinorGC可以确保是安全的。否则，虚拟机会查看HandlePromotionFailure设置值是否允许担保失败，如果允许，继续查看老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于则尝试MinorGC，尽管这次MinorGC是有风险的。如果小于，或者HandlerPromotionFailure设置不允许，则要改为FullGC。 新生代的回收称为MinorGC,对老年代的回收成为MajorGC又名FullGC 9. 关于GC的虚拟机参数GC相关 -XX:NewSize和-XX:MaxNewSize 新生代大小-XX:SurvivorRatio Eden和其中一个survivor的比值-XX：PretenureSizeThreshold 大对象进入老年代的阈值-XX:MaxTenuringThreshold 晋升老年代的对象年龄 收集器设置-XX:+UseSerialGC:设置串行收集器-XX:+UseParallelGC:设置并行收集器-XX:+UseParalledlOldGC:设置并行年老代收集器-XX:+UseConcMarkSweepGC:设置并发收集器 堆大小设置 -Xmx:最大堆大小-Xms:初始堆大小(最小内存值)-Xmn:年轻代大小-XXSurvivorRatio:3 意思是Eden:Survivor=3:2-Xss栈容量 垃圾回收统计信息 -XX:+PrintGC 输出GC日志-XX:+PrintGCDetails 输出GC的详细日志 10. 方法区的回收方法区通常会与永久代划等号，实际上二者并不等价，只不过是HotSpot虚拟机设计者用永久代实现方法区，并将GC分代扩展至方法区。永久代垃圾回收通常包括两部分内容：废弃常量和无用的类。常量的回收与堆区对象的回收类似，当没有其他地方引用该字面量时，如果有必要，将被清理出常量池。 判定无用的类的3个条件： 1.该类的所有实例都已经被回收，也就是说堆中不存在该类的任何实例 2.加载该类的ClassLoader已经被回收 3.该类对应的java.lang.Class对象没有在任何地方被引用，无法在任何地方通过反射访问该类的方法。 当然，这也仅仅是判定，不代表立即卸载该类。 11. JVM工具命令行 jps(jvm processor status)虚拟机进程状况工具 jstat(jvm statistics monitoring)统计信息监视 jinfo(configuration info for java)配置信息工具 jmap(memory map for java)Java内存映射工具 jhat(JVM Heap Analysis Tool)虚拟机堆转储快照分析工具 jstack(Stack Trace for Java)Java堆栈跟踪工具 HSDIS：JIT生成代码反汇编 可视化 JConsole(Java Monitoring and Management Console):Java监视与管理控制台 VisualVM(All-in-one Java Troubleshooting Tool):多合一故障处理工具 12. JVM内存结构 堆:新生代和年老代 方法区(非堆):持久代, 代码缓存, 线程共享 JVM栈:中间结果,局部变量,线程隔离 本地栈:本地方法(非Java代码) 程序计数器 ：线程私有，每个线程都有自己独立的程序计数器，用来指示下一条指令的地址 注：持久代Java8消失, 取代的称为元空间(本地堆内存的一部分) 13. JVM的方法区与堆一样，是线程共享的区域。方法区中存储：被虚拟机加载的类信息，常量，静态变量，JIT编译后的代码等数据。参见我是一个Java Class。 14. Java类加载器一个jvm中默认的classloader有Bootstrap ClassLoader、Extension ClassLoader、App ClassLoader，分别各司其职： Bootstrap ClassLoader(引导类加载器) 负责加载java基础类，主要是 %JRE_HOME/lib/目录下的rt.jar、resources.jar、charsets.jar等 Extension ClassLoader(扩展类加载器) 负责加载java扩展类，主要是 %JRE_HOME/lib/ext目录下的jar等 App ClassLoader(系统类加载器) 负责加载当前java应用的classpath中的所有类。classloader 加载类用的是全盘负责委托机制。 所谓全盘负责，即是当一个classloader加载一个Class的时候，这个Class所依赖的和引用的所有 Class也由这个classloader负责载入，除非是显式的使用另外一个classloader载入。所以，当我们自定义的classloader加载成功了com.company.MyClass以后，MyClass里所有依赖的class都由这个classLoader来加载完成。 15. 64 位 JVM 中，int 的长度是多大？Java 中，int 类型变量的长度是一个固定值，与平台无关，都是 32 位。意思就是说，在 32 位 和 64 位 的Java 虚拟机中，int 类型的长度是相同的。 16. Serial 与 Parallel GC之间的不同之处？Serial 与 Parallel 在GC执行的时候都会引起 stop-the-world。它们之间主要不同 serial 收集器是默认的复制收集器，执行 GC 的时候只有一个线程，而 parallel 收集器使用多个 GC 线程来执行。 17.Java 中 WeakReference 与 SoftReference的区别？Java中一共有四种类型的引用。StrongReference、 SoftReference、 WeakReference 以及 PhantomReference。 StrongReference：Java 的默认引用实现, 它会尽可能长时间的存活于 JVM 内，当没有任何对象指向它时将会被GC回收 SoftReference：尽可能长时间保留引用，直到JVM内存不足，适合某些缓存应用 WeakReference：顾名思义, 是一个弱引用, 当所引用的对象在 JVM 内不再有强引用时, 下一次将被GC回收 PhantomReference：它是最弱的一种引用关系，也无法通过PhantomReference取得对象的实例。仅用来当该对象被回收时收到一个通知 虽然 WeakReference 与 SoftReference 都有利于提高 GC 和 内存的效率，但是 WeakReference ，一旦失去最后一个强引用，就会被 GC 回收，而 SoftReference 会尽可能长的保留引用直到 JVM 内存不足时才会被回收(虚拟机保证), 这一特性使得 SoftReference 非常适合缓存应用。 18. WeakHashMap 是怎么工作的？WeakHashMap 的工作与正常的 HashMap 类似，但是使用弱引用作为 key，意思就是当 key 对象没有任何引用时，key/value 将会被回收。 19. JVM 选项 -XX:+UseCompressedOops 有什么作用？为什么要使用？当你将你的应用从 32 位的 JVM 迁移到 64 位的 JVM 时，由于对象的指针从 32 位增加到了 64 位，因此堆内存会突然增加，差不多要翻倍。这也会对 CPU 缓存（容量比内存小很多）的数据产生不利的影响。因为，迁移到 64 位的 JVM 主要动机在于可以指定最大堆大小，通过压缩 OOP 可以节省一定的内存。通过 -XX:+UseCompressedOops 选项，JVM 会使用 32 位的 OOP，而不是 64 位的 OOP。 20. 怎样通过 Java 程序来判断 JVM 是 32 位 还是 64 位？你可以检查某些系统属性如 sun.arch.data.model 或 os.arch 来获取该信息。 21. 32 位 JVM 和 64 位 JVM 的最大堆内存分别是多数？理论上说上 32 位的 JVM 堆内存可以到达 2^32，即 4GB，但实际上会比这个小很多。不同操作系统之间不同，如 Windows 系统大约 1.5 GB，Solaris 大约 3GB。64 位 JVM允许指定最大的堆内存，理论上可以达到 2^64，这是一个非常大的数字，实际上你可以指定堆内存大小到 100GB。甚至有的 JVM，如 Azul，堆内存到 1000G 都是可能的。 22. JRE、JDK、JVM 及 JIT 之间有什么不同？JRE 代表 Java 运行时（Java run-time），是运行 Java 应用所必须的。JDK 代表 Java 开发工具（Java development kit），是 Java 程序的开发工具，如 Java 编译器，它也包含 JRE。JVM 代表 Java 虚拟机（Java virtual machine），它的责任是运行 Java 应用。JIT 代表即时编译（Just In Time compilation），当代码执行的次数超过一定的阈值时，会将 Java 字节码转换为本地代码，如，主要的热点代码会被准换为本地代码，这样有利大幅度提高 Java 应用的性能。 23. 解释 Java 堆空间及 GC？当通过 Java 命令启动 Java 进程的时候，会为它分配内存。内存的一部分用于创建堆空间，当程序中创建对象的时候，就从对空间中分配内存。GC 是 JVM 内部的一个后台进程，回收无效对象的内存用于将来的分配。 24. 你能保证 GC 执行吗？不能，虽然你可以调用 System.gc() 或者 Runtime.getRuntime().gc()，但是没有办法保证 GC 的执行。 25. 怎么获取 Java 程序使用的内存？堆使用的百分比？可以通过 java.lang.Runtime 类中与内存相关方法来获取剩余的内存，总内存及最大堆内存。通过这些方法你也可以获取到堆使用的百分比及堆内存的剩余空间。Runtime.freeMemory() 方法返回剩余空间的字节数，Runtime.totalMemory() 方法总内存的字节数，Runtime.maxMemory() 返回最大内存的字节数。 26. Java 中堆和栈有什么区别？JVM 中堆和栈属于不同的内存区域，使用目的也不同。栈常用于保存方法帧和局部变量，而对象总是在堆上分配。栈通常都比堆小，也不会在多个线程之间共享，而堆被整个 JVM 的所有线程共享。 27. JVM调优使用工具Jconsol、VisualVM、JProfiler等 堆信息查看 可查看堆空间大小分配（年轻代、年老代、持久代分配）提供即时的垃圾回收功能垃圾监控（长时间监控回收情况） 查看堆内类、对象信息查看：数量、类型等 对象引用情况查看 有了堆信息查看方面的功能，我们一般可以顺利解决以下问题： 年老代年轻代大小划分是否合理 内存泄漏 垃圾回收算法设置是否合理 线程监控 线程信息监控：系统线程数量。线程状态监控：各个线程都处在什么样的状态下 Dump线程详细信息：查看线程内部运行情况死锁检查 热点分析 CPU热点：检查系统哪些方法占用的大量CPU时间内存热点：检查哪些对象在系统中数量最大（一定时间内存活对象和销毁对象一起统计） 快照系统两个不同运行时刻，对象（或类、线程等）的不同举例说，我要检查系统进行垃圾回收以后，是否还有该收回的对象被遗漏下来的了。那么，我可以在进行垃圾回收前后，分别进行一次堆情况的快照，然后对比两次快照的对象情况。 内存泄漏检查 年老代堆空间被占满持久代被占满堆栈溢出线程堆栈满系统内存被占满 28. Java中有内存泄漏吗？内存泄露的定义: 当某些对象不再被应用程序所使用,但是由于仍然被引用而导致垃圾收集器不能释放。 内存泄漏的原因：对象的生命周期不同。比如说对象A引用了对象B. A的生命周期比B的要长得多，当对象B在应用程序中不会再被使用以后, 对象 A 仍然持有着B的引用. (根据虚拟机规范)在这种情况下GC不能将B从内存中释放。这种情况很可能会引起内存问题，倘若A还持有着其他对象的引用,那么这些被引用的(无用)对象也不会被回收,并占用着内存空间。甚至有可能B也持有一大堆其他对象的引用。这些对象由于被B所引用,也不会被垃圾收集器所回收，所有这些无用的对象将消耗大量宝贵的内存空间。并可能导致内存泄漏。 怎样防止：1、当心集合类, 比如HashMap, ArrayList等,因为这是最容易发生内存泄露的地方.当集合对象被声明为static时,他们的生命周期一般和整个应用程序一样长。 29. OOM解决办法内存溢出的空间：Permanent Generation和Heap Space，也就是永久代和堆区 1、永久代的OOM 解决办法有2种：a.通过虚拟机参数-XX：PermSize和-XX：MaxPermSize调整永久代大小b.清理程序中的重复的Jar文件，减少类的重复加载 2、堆区的溢出 发生这种问题的原因是java虚拟机创建的对象太多，在进行垃圾回收之间，虚拟机分配的到堆内存空间已经用满了，与Heap Space的size有关。解决这类问题有两种思路： 检查程序，看是否存在死循环或不必要地重复创建大量对象，定位原因，修改程序和算法。 通过虚拟机参数-Xms和-Xmx设置初始堆和最大堆的大小 30. DirectMemory直接内存直接内存并不是Java虚拟机规范定义的内存区域的一部分，但是这部分内存也被频繁使用，而且也可能导致OOM异常的出现。 JDK1.4引入了NIO，这是一种基于通道和缓冲区的非阻塞IO模式，它可以使用Native函数库分配直接堆外内存，然后通过一个存储在Java堆中的DirectByteBuffer对象作为这块内存的引用进行操作，使得在某些场合显著提高性能，因为它避免了在Java堆和本地堆之间来回复制数据。 31. Java 中堆和栈有什么不同每个线程都有自己的栈内存，用于存储本地变量，方法参数和栈调用，一个线程中存储的变量对其它线程是不可见的。而堆是所有线程共享的一片公用内存区域。对象都在堆里创建，为了提升效率线程会从堆中弄一个缓存到自己的栈，如果多个线程使用该变量就可能引发问题，这时 volatile 变量就可以发挥作用了，它要求线程从主存中读取变量的值。 32. 双亲委派模型中的方法findLoadedClass(),LoadClass(),findBootstrapClassOrNull(),findClass(),resolveClass() 33. IO模型一般来说 I/O 模型可以分为：同步阻塞，同步非阻塞，异步阻塞，异步非阻塞 四种IO模型 同步阻塞 IO ：在此种方式下，用户进程在发起一个 IO 操作以后，必须等待 IO 操作的完成，只有当真正完成了 IO 操作以后，用户进程才能运行。 JAVA传统的 IO 模型属于此种方式！ 同步非阻塞 IO：在此种方式下，用户进程发起一个 IO 操作以后可返回做其它事情，但是用户进程需要时不时的询问 IO 操作是否就绪，这就要求用户进程不停的去询问，从而引入不必要的 CPU 资源浪费。其中目前 JAVA 的 NIO 就属于同步非阻塞 IO 。 异步阻塞 IO：此种方式下是指应用发起一个 IO 操作以后，不等待内核 IO 操作的完成，等内核完成 IO 操作以后会通知应用程序，这其实就是同步和异步最关键的区别，同步必须等待或者主动的去询问 IO 是否完成，那么为什么说是阻塞的呢？因为此时是通过 select 系统调用来完成的，而 select 函数本身的实现方式是阻塞的，而采用 select 函数有个好处就是它可以同时监听多个文件句柄，从而提高系统的并发性！ 异步非阻塞 IO：在此种模式下，用户进程只需要发起一个 IO 操作然后立即返回，等 IO 操作真正的完成以后，应用程序会得到 IO 操作完成的通知，此时用户进程只需要对数据进行处理就好了，不需要进行实际的 IO 读写操作，因为 真正的 IO读取或者写入操作已经由 内核完成了。目前 Java7的AIO正是此种类型。 BIO即同步阻塞IO，适用于连接数目较小且固定的架构，这种方式对服务器资源要求比较高，并发局限于应用中，JDK1.4之前的唯一选择，但程序直观、简单、易理解。 NIO即同步非阻塞IO，适用于连接数目多且连接比较短的架构，比如聊天服务器，并发局限于应用中，编程比较复杂，JDK1.4开始支持。 AIO即异步非阻塞IO，适用于连接数目多且连接比较长的架构，如相册服务器，充分调用OS参与并发操作，编程比较复杂，JDK1.7开始支持 34. 类加载器按照层次，从顶层到底层，分别加载哪些类？启动类加载器：负责将存放在JAVA_HOME/lib下的，或者被－Xbootclasspath参数所指定的路径中的，并且是虚拟机识别的类库加载到虚拟机内存中。启动类加载器无法被Java程序直接引用。 扩展类加载器：这个加载器负责加载JAVA_HOME/lib/ext目录中的，或者被java.ext.dirs系统变量所指定的路径中的所有类库，开发者可以直接使用扩展类加载器 应用程序类加载器：这个加载器是ClassLoader中getSystemClassLoader()方法的返回值，所以一般也称它为系统类加载器。它负责加载用户类路径（Classpath）上所指定的类库，可直接使用这个加载器，如果应用程序没有自定义自己的类加载器，一般情况下这个就是程序中默认的类加载器 实现自己的加载器 只需要继承ClassLoader，并覆盖findClass方法。在调用loadClass方法时，会先根据委派模型在父加载器中加载，如果加载失败，则会调用自己的findClass方法来完成加载 五、数据库（Sql、MySQL、Redis等）1. Statement1.1 基本内容 Statement是最基本的用法, 不传参, 采用字符串拼接，存在注入漏洞 PreparedStatement传入参数化的sql语句, 同时检查合法性, 效率高可以重用, 防止sql注入 CallableStatement接口扩展PreparedStatement，用来调用存储过程 BatchedStatement用于批量操作数据库，BatchedStatement不是标准的Statement类 12public interface CallableStatement extends PreparedStatement public interface PreparedStatement extends Statement 1.2 Statement与PrepareStatement的区别 创建时的区别 12Statement statement = conn.createStatement();PreparedStatement preStatement = conn.prepareStatement(sql); 执行的时候 12ResultSet rSet = statement.executeQuery(sql);ResultSet pSet = preStatement.executeQuery(); 由上可以看出，PreparedStatement有预编译的过程，已经绑定sql，之后无论执行多少遍，都不会再去进行编译，而 statement 不同，如果执行多遍，则相应的就要编译多少遍sql，所以从这点看，preStatement 的效率会比 Statement要高一些 安全性 PreparedStatement是预编译的，所以可以有效的防止SQL注入等问题 代码的可读性和可维护性 PreparedStatement更胜一筹 2. 游标3. 列出 5 个应该遵循的 JDBC 最佳实践有很多的最佳实践，你可以根据你的喜好来例举。下面是一些更通用的原则： a）使用批量的操作来插入和更新数据b）使用 PreparedStatement 来避免 SQL 异常，并提高性能c）使用数据库连接池d）通过列名来获取结果集，不要使用列的下标来获取 4. 数据库索引的实现数据库系统还维护着满足特定查找算法的数据结构，这些数据结构以某种方式引用（指向）数据，这样就可以在这些数据结构上实现高级查找算法。这种数据结构，就是索引。 B树： 一棵m阶B树(balanced tree of order m)是一棵平衡的m路搜索树。它或者是空树，或者是满足下列性质的树： 1、根结点至少有两个子女；2、每个非根节点所包含的关键字个数 j 满足：┌m/2┐ - 1 &lt;= j &lt;= m - 1；3、除根结点以外的所有结点（不包括叶子结点）的度数正好是关键字总数加1，故内部子树个数 k 满足：┌m/2┐ &lt;= k &lt;= m ；4、所有的叶子结点都位于同一层。 由于B-Tree的特性，在B-Tree中按key检索数据的算法非常直观：首先从根节点进行二分查找，如果找到则返回对应节点的data，否则对相应区间的指针指向的节点递归进行查找，直到找到节点或找到null指针，前者查找成功，后者查找失败。 一个度为d的B-Tree，设其索引N个key，则其树高h的上限为logd((N+1)/2)，检索一个key，其查找节点个数的渐进复杂度为O(logdN)。从这点可以看出，B-Tree是一个非常有效率的索引数据结构。 B+树： B-Tree有许多变种，其中最常见的是B+Tree，例如MySQL就普遍使用B+Tree实现其索引结构。 B+树是B树的变形，它把所有的data都放在叶子结点中，只将关键字和子女指针保存于内结点，内结点完全是索引的功能。 与B-Tree相比，B+Tree有以下不同点： 1、每个节点的指针上限为2d而不是2d+1。 2、内节点不存储data，只存储key；叶子节点存储data不存储指针。 一般在数据库系统或文件系统中使用的B+Tree结构都在经典B+Tree的基础上进行了优化，增加了顺序访问指针。 在B+Tree的每个叶子节点增加一个指向相邻叶子节点的指针 例如图4中如果要查询key为从18到49的所有数据记录，当找到18后，只需顺着节点和指针顺序遍历就可以一次性访问到所有数据节点，极大提到了区间查询效率。 为什么B树（B+树）？ 一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。 这涉及到磁盘存取原理、局部性原理和磁盘预读。 先从B-Tree分析，根据B-Tree的定义，### 可知检索一次最多需要访问h个节点。数据库系统的设计者巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。**为了达到这个目的，在实际实现B-Tree还需要使用如下技巧： 每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。 B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O(h)=O(logdN)。一般实际应用中，出度d是非常大的数字，通常超过100，因此h非常小（通常不超过3）。 综上所述，用B-Tree作为索引结构效率是非常高的。 而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O(h)，效率明显比B-Tree差很多。 至于B+Tree为什么更适合外存索引，原因和内节点出度d有关。 由于B+Tree内节点去掉了data域，因此可以拥有更大的出度，拥有更好的性能。 六、算法与数据结构1. 二叉搜索树(Binary Search Tree又名：二叉查找树,二叉排序树)它或者是一棵空树,或者是具有下列性质的二叉树： 若它的左子树不空,则左子树上所有结点的值均小于它的根结点的值；若它的右子树不空,则右子树上所有结点的值均大于它的根结点的值；它的左、右子树也分别为二叉搜索树。 2. RBT红黑树红黑树是一棵二叉搜索树，它在每个结点上增加一个存储位来表示结点的颜色，可以是RED或BLACK。通过对任何一条从根到叶子的简单路径上各个结点的颜色进行约束，红黑树没有一条路径会比其他路径长出2倍，所以红黑树是近似平衡的，使得红黑树的查找、插入、删除等操作的时间复杂度最坏为O(log n)，但需要注意到在红黑树上执行插入或删除后将不在满足红黑树性质，恢复红黑树的属性需要少量(O(logn))的颜色变更(实际是非常快速的)和不超过三次树旋转(对于插入操作是两次)。虽然插入和删除很复杂，但操作时间仍可以保持为 O(log n) 次。具体如何保证？引出红黑树的5个性质。 红黑树的5个性质：满足以下五个性质的二叉搜索树 每个结点或是红色的或是黑色的 根结点是黑色的 每个叶结点是黑色的 如果一个结点是红色的,则它的两个子结点是黑色的 对于每个结点,从该结点到其后代叶结点的简单路径上,均包含相同数目的黑色结点 插入操作： 由于性质的约束，插入的结点都是红色的。插入时性质1、3始终保持。破坏性质2当且仅当当前插入结点为根节点。变一下颜色即可。如果是破坏性质4或5，则需要旋转和变色来继续满足红黑树的性质。下面说一说插入的几种情况，约定当前插入结点为N，其父结点为P，叔叔为U，祖父为G 情形1：树空，直接插入违反性质1，将红色改黑。 情形2：N的父结点为黑，不必修改，直接插入 从情形3开始的情形假定N结点的父结点P为红色，所以存在G，并且G为黑色。且N存在一个叔叔结点U，尽管U可能为叶结点。 情形3：P为红，U为红（G结点一定存在且为黑）这里不论P是G的左孩子还是右孩子；不论N是P的左孩子还是右孩子。 首先把P、U改黑，G改红，并以G作为一个新插入的红结点重新进行各种情况的检查，若一路检索至根节点还未结束，则将根结点变黑。 情形4：P为红，U为黑或不存在（G结点一定存在且为黑），且P为G的左孩子，N为P的左孩子（或者P为G的右孩子，N为P的右孩子，保证同向的）。P、G右旋并将P、G变相反色。因为P取代之前黑G的位置，所以P变黑可以理解，而G变红是为了不违反性质5。 情形5：P为红，U为黑或不存在，且P为G的左孩子，N为P的右孩子（或P为G的右孩子，N为P的左孩子，保证是反向的），对N，P进行一次左旋转换为情形4 删除操作比插入复杂一些，但最多不超过三次旋转可以让红黑树恢复平衡。 其他 黑高从某个结点x出发(不含x)到达一个叶结点的任意一条简单路径上的黑色结点个数称为该结点的黑高。红黑树的黑高为其根结点的黑高。 一个具有n个内部结点的红黑树的高度h&lt;=2lg(n+1) 结点的属性(五元组):color key left right p 动态集合操作最坏时间复杂度为O(lgn) 3. 排序算法 稳定排序:插入排序、冒泡排序、归并排序、基数排序 插入排序[稳定]适用于小数组,数组已排好序或接近于排好序速度将会非常快复杂度：O(n^2) - O(n) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 归并排序[稳定]采用分治法复杂度：O(nlogn) - O(nlgn) - O(nlgn) - O(n)[平均 - 最好 - 最坏 - 空间复杂度] 冒泡排序[稳定]复杂度：O(n^2) - O(n) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 基数排序 分配+收集[稳定]复杂度： O(d(n+r)) r为基数d为位数 空间复杂度O(n+r) 树排序[不稳定]应用：TreeSet的add方法、TreeMap的put方法不支持相同元素,没有稳定性问题复杂度：平均最差O(nlogn) 堆排序(就地排序)[不稳定]复杂度：O(nlogn) - O(nlgn) - O(nlgn) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 快速排序[不稳定]复杂度：O(nlgn) - O(nlgn) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度]栈空间0(lgn) - O(n) 选择排序[不稳定]复杂度：O(n^2) - O(n^2) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 希尔排序[不稳定]复杂度 小于O(n^2) 平均 O(nlgn) 最差O(n^s)[1&lt;s&lt;2] 空间O(1) 4. 查找与散列4.1 散列函数设计 直接定址法:f(key) = a*key+b 简单、均匀,不易产生冲突。但需事先知道关键字的分布情况,适合查找表较小且连续的情况,故现实中并不常用 除留余数法:f(key) = key mod p (p&lt;=m) p取小于表长的最大质数 m为表长 DJBX33A算法(time33哈希算法hash = hash*33+(unsigned int)str[i]; 平方取中法 折叠法 更多…. 4.2 冲突处理 闭散列(开放地址方法):要求装填因子a较小，闭散列方法把所有记录直接存储在散列表中 线性探测:易产生堆积现象(基地址不同堆积在一起) 二次探测:f(key) = (f(key)+di) % m di=1^2,-1^2,2^2,-2^2…可以消除基本聚集 随机探测:f(key) = (f(key)+di),di采用随机函数得到,可以消除基本聚集 双散列:避免二次聚集 开散列(链地址法):原地处理 同义词记录存储在一个单链表中,散列表中子存储单链表的头指针。 优点:无堆积 事先无需确定表长 删除结点易于实现 装载因子a&gt;=1,缺点:需要额外空间 5. 跳表为什么选择跳表？ 目前经常使用的平衡数据结构有：B树，红黑树，AVL树，Splay Tree, Treep等。想象一下，给你一张草稿纸，一只笔，一个编辑器，你能立即实现一颗红黑树，或者AVL树出来吗？ 很难吧，这需要时间，要考虑很多细节，要参考一堆算法与数据结构之类的树，还要参考网上的代码，相当麻烦。用跳表吧，跳表是一种随机化的数据结构，目前开源软件 Redis 和 LevelDB 都有用到它，它的效率和红黑树以及 AVL 树不相上下，但跳表的原理相当简单，只要你能熟练操作链表，就能去实现一个 SkipList。 跳跃表是一种随机化数据结构，基于并联的链表，其效率可比拟于二叉查找树(对于大多数操作需要O(log n)平均时间)，并且对并发算法友好。 Skip list(跳表）是一种可以代替平衡树的数据结构，默认是按照Key值升序的。Skip list让已排序的数据分布在多层链表中，以0-1随机数决定一个数据的向上攀升与否，是一种“空间来换取时间”的一个算法，在每个节点中增加了指向下一层的指针，在插入、删除、查找时可以忽略一些不可能涉及到的结点，从而提高了效率。 在Java的API中已经有了实现：分别是ConcurrentSkipListMap(在功能上对应HashTable、HashMap、TreeMap) ；ConcurrentSkipListSet(在功能上对应HashSet) Skip list的性质(1) 由很多层结构组成，level是通过一定的概率随机产生的(2) 每一层都是一个有序的链表，默认是升序(3) 最底层(Level 1)的链表包含所有元素(4) 如果一个元素出现在Level i 的链表中，则它在Level i 之下的链表也都会出现(5) 每个节点包含两个指针，一个指向同一链表中的下一个元素，一个指向下面一层的元素时间复杂度O(lgn) 最坏O(2lgn) 6. AVL树1.LL型 在某一节点的左孩子的左子树上插入一个新的节点，使得该节点不再平衡。举例 A B Ar Bl Br 在Bl下插入N，执行一次右旋即可，即把B变为父结点，原来的根节点A变为B的左孩子，B的右子树变为A的左子树。 2.RR型 与LL型是对称的，执行一次左旋即可。 3.LR型 指在AVL树某一结点左孩子的右子树上插入一个结点，使得该节点不在平衡。这时需要两次旋转，先左旋再右旋。 4.RL型 与LR对称，执行一次右旋，再执行一次左旋。 删除 1、被删的节点是叶子节点 将该节点直接从树中删除，并利用递归的特点和高度的变化，反向推算其父节点和祖先节点是否失衡。 2、被删的节点只有左子树或只有右子树 将左子树（右子树）替代原有节点的位置，并利用递归的特点和高度的变化，反向推算父节点和祖先节点是否失衡。 3、被删的节点既有左子树又有右子树 找到被删节点的左子树的最右端的节点，将该结点的的值赋给待删除结点，再用该结点的左孩子替换它本来的位置，然后释放该结点，并利用递归特点，反向推断父节点和祖父节点是否失衡。 7. 一致性Hash第一：简单介绍一致性哈希算法是分布式系统中常用的算法。比如，一个分布式的存储系统，要将对象存储到具体的节点上，如果采用普通的hash方法，将数据映射到具体的节点上，如key%N，N是机器节点数。 1、考虑到比如一个服务器down掉，服务器结点N变为N-1，映射公式必须变为key%(N-1) 2、访问量加重，需要添加服务器结点，N变为N+1，映射公式变为hash(object)%(N+1) 当出现1,2的情况意味着我们的映射都将无效，对服务器来说将是一场灾难，尤其是对缓存服务器来说，因为缓存服务器映射的失效，洪水般的访问都将冲向后台服务器。 第二点：hash算法的单调性 Hash 算法的一个衡量指标是单调性，单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。 consistent hash 也是一种hash 算法，简单的说，在移除 / 添加一个结点时，它能够尽可能小的改变已存在的映射关系，尽可能的满足单调性的要求。 第三点：将对象和服务器结点分别映射到环型空间 通常的一致性哈希做法是将 value 映射到一个 32 位的 key 值，也即是 0~2^32-1 次方的数值空间；我们可以将这个空间想象成一个首（ 0 ）尾（ 2^32-1 ）相接的圆环。 我们可以通过hash函数将我们的key映射到环型空间中，同时根据相同的哈希算法把服务器也映射到环型空间中，顺便提一下服务器或者某个计算节点的 hash 计算，一般的方法可以使用机器的 IP 地址或者机器名作为 hash 输入。 第四点：将对象映射到服务器 在这个环形空间中，如果沿着顺时针方向从对象的 key 值出发，直到遇见一个 服务器结点，那么就将该对象存储在这个服务器结点上，因为对象和服务器的hash 值是固定的，因此这个 cache 必然是唯一和确定的。 这时候考察某个服务器down机或者需要添加服务器结点，也就是移除和添加的操作，我们只需要几个对象的映射。 第五点：虚拟结点 Hash 算法的另一个指标是平衡性 (Balance)。平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。 对于上述的做法，可能导致某些对象都映射到某个服务器，使得分布不平衡。为此可以采用“虚拟结点”的做法。 “虚拟结点”（ virtual node ）是实际节点在 hash 空间的复制品，一实际结点对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以 hash 值排列。引入“虚拟结点”会让我们的映射分布更为平衡一些。 引入“虚拟结点”前：Hash(“192.168.1.1”); 引入“虚拟结点”后：Hash(“192.168.1.1#1”);Hash(“192.168.1.1#2”); 8. 如何判断链表是否有环方法1：快慢指针法 2.设两个工作指针p、q，p总是向前走，但q每次都从头开始走，对于每个节点，看p走的步数是否和q一样。比如p从A走到D，用了4步，而q则用了14步。因而步数不等，出现矛盾，存在环。 9. 熟悉哪些算法？ [哈希算法] 一致性哈希 time33哈希 FNV1_32_HASH [排序算法] 快速排序 [搜索算法] DFS BFS [最小生成树算法] Kruskal Prim [最短路径算法] Dijkstra Floyed 七、计算机网络1.停止等待协议停止等待协议是最基本的数据链路层协议，它的工作原理是这样的。在发送端，每发送完一帧就停止发送，等待接收端的确认，如果收到确认就发送下一帧。在接收端，每收到一个无差错的帧，就把这个帧交付上层并向发送端发送确认。若该帧有差错，就丢弃，其他什么也不做。 其他细节：停止等待协议为了可靠交付，需要对帧进行编号，由于每次只发送一帧，所以停止等待协议使用1个比特编号，编号0和1停止等待协议会出现死锁现象（A等待B的确认），解决办法，启动超时计时器，超时计时器有一个重传时间。重传时间一般选择略大于“正常情况下从发完数据帧到收到确认帧所需的平均时间”。 2.滑动窗口协议再说滑动窗口之前，先说下连续ARQ，连续ARQ又称Go-back-N ARQ，意思是当出现差错必须重传时，要向回走N个帧，然后再开始重传，也就意味着只要有一帧出现差错，即使已经正确的帧也需要重传，白白浪费时间，增大开销。为此，应该对发送出去但未被确认的帧的数目加以限制，这就是滑动窗口协议。滑动窗口指收发两端分别维护一个发送窗口和接收窗口，发送窗口有一个窗口值Wt，窗口值Wt代表在没有收到对方确认的情况下最多可以发送的帧的数目。当发送的帧的序号被接收窗口正确收下后，接收端向前滑动并向发送端发去确认，发送端收到确认后，发送窗口向前滑动。收发两端按规律向前推进。 连续ARQ和选择重传ARQ均是窗口大于1的滑动窗口协议，而停止等待协议相当于收发两端窗口等于1。 滑动窗口指接收和发送两端的窗口按规律不断向前推进，是一种流量控制的策略。 3.Http1.0和Http1.1的区别 HTTP/1.0协议使用非持久连接,即在非持久连接下,一个tcp连接只传输一个Web对象。 HTTP/1.1默认使用持久连接(然而,HTTP/1.1协议的客户机和服务器可以配置成使用非持久连接)。在持久连接下,不必为每个Web对象的传送建立一个新的连接,一个连接中可以传输多个对象。 4.Post和Get的区别1.安全性上说：get的方式是把数据在地址栏中明文的形式发送，URL中可见，POST方式对用户是透明的，安全性更高。2.数据量说：Get传送的数据量较小，一般不能大于2KB，POST传送的数据量更大。3.适用范围说：查询用Get，数据添加、修改和删除建议Post 5. TCP/IP体系各层功能及协议TCP/IP体系共有四个层次，分别为网络接口层Host-to-Network Layer, 网际层 Internet Layer， 传输层Transport Layer，应用层Application Layer。 网络接口层 -&gt; 接收和发送数据报主要负责将数据发送到网络传输介质上以及从网络上接收TCP/IP数据报，相当于OSI参考模型的物理层和数据链路层。在实际中，先后流行的以太网、令牌环网、ATM、帧中继等都可视为其底层协议。它将发送的信息组装成帧并通过物理层向选定网络发送，或者从网络上接收物理帧，将去除控制信息后的IP数据报交给网络层。 网际层 -&gt; 数据报封装和路由寻址网际层主要功能是寻址和对数据报的封装以及路由选择功能。这些功能大部分通过IP协议完成，并通过地址解析协议ARP、逆地址解析协议RARP、因特网控制报文协议ICMP、因特网组管理协议IGMP从旁协助。所以IP协议是网络层的核心。 网际协议IP：IP协议是一个无连接的协议，主要负责将数据报从源结点转发到目的结点。也就是说IP协议通过对数据报中源地址和目的地址进行分析，然后进行路由选择，最后再转发到目的地。需要注意的是：IP协议只负责对数据进行转发，并不对数据进行检查，也就是说，它不负责数据的可靠性，这样设计的主要目的是提高IP协议传送和转发数据的效率。 ARP：该协议负责将IP地址解析转换为计算机的物理地址。 虽然我们使用IP地址进行通信，但IP地址只是主机在抽象的网络层中的地址。最终要传到数据链路层封装成MAC帧才能发送到实际的网络。因此不管使用什么协议最终需要的还是硬件地址。 每个主机拥有一个ARP高速缓存（存放所在局域网内主机和路由器的IP地址到硬件地址的映射表） 举例：A发送B (1)A在自己的ARP高速缓存中查到B的MAC地址，写入MAC帧发往此B (2)没查到，A向本局域网广播ARP请求分组，内容包括自己的地址映射和B的IP地址 (3)B发送ARP响应分组，内容为自己的IP地址到物理地址的映射，同时将A的映射写入自己的ARP高速缓存（单播的方式） 注：ARP Cache映射项目具有一个生存时间。 RARP：将计算机物理地址转换为IP地址 ICMP：该协议主要负责发送和传递包含控制信息的数据报，这些控制信息包括了哪台计算机出现了什么错误，网络路由出现了什么错误等内容。 传输层 -&gt; 应用进程间端到端的通信传输层主要负责应用进程间“端到端”的通信，即从某个应用进程传输到另一个应用进程，它与OSI参考模型的传输层功能类似。传输层在某个时刻可能要同时为多个不同的应用进程服务，因此传输层在每个分组中必须增加用于识别应用进程的标识，即端口。TCP/IP体系的传输层主要包含两个主要协议，即传输控制协议TCP和用户数据报协议UDP。TCP协议是一种可靠的、面向连接的协议，保证收发两端有可靠的字节流传输，进行了流量控制，协调双方的发送和接收速度，达到正确传输的目的。UDP是一种不可靠的、无连接的协议，其特点是协议简单、额外开销小、效率较高，不能保证可靠传输。传输层提供应用进程间的逻辑通信。它使应用进程看见的就好像是在两个运输层实体间一条端到端的逻辑通信信道。当运输层采用TCP时，尽管下面的网络是不可靠的，但这种逻辑通信信道相当于一条全双工的可靠信道。可以做到报文的无差错、按序、无丢失、无重复。 注：单单面向连接只是可靠的必要条件，不充分。还需要其他措施，如确认重传，按序接收，无丢失无重复。 熟知端口： 12345678920 FTP数据连接 21 FTP控制连接 22 SSH 23 TELNET 25 SMTP 53 DNS 69 TFTP80 HTTP161 SNMP UDP重要 UDP的优点： 1.发送之前无需建立连接，减小了开销和发送数据的时延2.UDP不使用连接，不使用可靠交付，因此主机不需要维护复杂的参数表、连接状态表3.UDP用户数据报只有8个字节的首部开销，而TCP要20字节。4.由于没有拥塞控制，因此网络出现拥塞不会使源主机的发送速率降低（IP电话等实时应用要求源主机以恒定的速率发送数据是有利的） Table，使用TCP和UDP的应用 应用 应用层协议 运输层协议 名字转换 DNS UDP 文件传送 TFTP UDP 路由选择协议 RIP UDP IP地址配置 BOOTTP,DHCP UDP 网络管理 SNMP UDP 远程文件服务器 NFS UDP IP电话 专用协议 UDP 流式多媒体通信 专用协议 UDP 电子邮件 SMTP TCP 远程终端接入 TELNET TCP 万维网 HTTP TCP 文件传送 FTP TCP 注：TFTP：Trivial File Transfer Protocol UDP的过程（以TFTP举例）： 服务器进程运行着，等待TFTP客户进程的服务请求。客户端TFTP进程启动时，向操作系统申请一个临时端口号，然后操作系统为该进程创建2个队列，入队列和出队列。只要进程在执行，2个队列一直存在。 客户进程将报文发送到出队列中。UDP按报文在队列的先后顺序发送。在传送到IP层之前给报文加上UDP首部，其中目的端口后为69，然后发给IP层。出队列若溢出，则操作系统通知应用层TFTP客户进程暂停发送。 客户端收到来自IP层的报文时，UDP检查报文中目的端口号是否正确，若正确，放入入队列队尾，客户进程按先后顺序一一取走。若不正确，UDP丢弃该报文，并请ICMP发送”端口不可达“差错报文给服务器端。入队列可能会溢出，若溢出，UDP丢弃该报文，不通知对方。 服务器端类似。 UDP首部：源端口 - 目的端口 - 长度 - 检验和，每个字段22字节。 注：IP数据报检验和只检验IP数据报的首部，而UDP的检验和将首部和数据部分一起都检验。 TCP重要 细节： TCP报文段是面向字节的数据流。 TCP首部：20字节固定首部 确认比特ACK，ACK=1 确认号字段才有效；同步比特SYN：SYN=1 ACK=0表示一个连接请求报文段；终止比特FIN，FIN=1时要求释放连接。 窗口：将TCP收发两端记为A和B，A根据TCP缓存空间的大小确定自己的接收窗口大小。并在A发送给B的窗口字段写入该值。作为B的发送窗口的上限。意味着B在未收到A的确认情况下，最多发送的字节数。 选项：最大报文段长度MSS，MSS告诉对方TCP：我的缓存所能接收的报文段的数据字段的最大长度是MSS个字节。若主机未填写，默认为536字节。 TCP的可靠是使用了序号和确认。当TCP发送一个报文时，在自己的重传队列中存放一个副本。若收到确认，删除副本。 TCP使用捎带确认。 TCP报文段的发送时机：1.维持一个变量等于MSS，发送缓存达到MSS就发送 2.发送端应用进程指明要发送，即TCP支持的PUSH操作。3.设定计时器 TCP的拥塞控制：TCP使用慢开始和拥塞避免算法进行拥塞控制 慢开始和拥塞避免 接收端根据自身资源情况控制发送端发送窗口的大小。 每个TCP连接需要维持一下2个状态变量： 接收端窗口rwnd（receiver window）：接收端根据目前接收缓存大小设置的窗口值，是来自接收端的流量控制 拥塞窗口cwnd（congestion window）：是发送端根据自己估计的网络拥塞程度设置的窗口值，是来自发送端的流量控制 发送端的窗口上限值=Min(rwnd, cwnd) 慢开始算法原理：主机刚开始发送数据时，如果立即将较大的发送窗口的全部字节注入网络，由于不清楚网络状况，可能会引起拥塞。通常的做法是将cwnd设置为1个MSS，每收到一个确认，将cwnd+1，由小到大逐步增大cwnd，使分组注入网络的速率更加合理。为了防止拥塞窗口增长引起网络拥塞，还需设置一个状态变量ssthresh，即慢开始门限。 慢开始门限：ssthresh，当cwnd &lt; ssthresh,执行慢开始算法；cwnd &gt; ssthresh，改用拥塞避免算法。 cwnd = ssthresh时，都可以。 拥塞避免算法使发送端的拥塞窗口每经过一个RTT增加一个MSS（而不管在此期间收到多少ACK），这样，拥塞窗口cwnd按线性规律增长，拥塞窗口此时比慢开始增长速率缓慢很多。这一过程称为加法增大，目的在于使拥塞窗口缓慢增长，防止网络过早拥塞。 无论是慢开始还是拥塞避免，只要发送端发现网络出现拥塞（根据是没有按时收到ACK或者收到重复ACK），就将慢开始门限ssthresh设置为拥塞窗口值的一半并将拥塞窗口cwnd置为1，重新执行慢开始算法。这一过程称为乘法减小。目的在于迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完毕。 上述TCP确认都是通过捎带确认执行的。 快重传和快恢复 上述的慢开始和拥塞避免算法是早期TCP使用的拥塞控制算法。因为有时TCP连接会在重传时因等待重传计时器的超时时间而空闲。为此在快重传中规定：只要发送端一连收到三个重复的ACK,即可断定分组丢失，不必等待重传计数器，立即重传丢失的报文。 与快重传搭配使用的还有快恢复：当不使用快恢复时，发送端若发现网络拥塞就将拥塞窗口降为1，然后执行慢开始算法，这样的缺点是网络不能很快恢复到正常状态。快恢复是指当发送端收到3个重复的ACK时，执行乘法减小，ssthresh变为拥塞窗口值的一半。但是cwnd不是置为1，而是ssthresh+3xMSS。若收到的重复ACK为n(n &gt; 3)，则cwnd=ssthresh+n*MSS.这样做的理由是基于发送端已经收到3个重复的ACK，它表明已经有3个分组离开了网络，它们不在消耗网络的资源。 注意的是：在使用快恢复算法时，慢开始算法只在TCP连接建立时使用。 TCP的重传机制 每发送一个报文段，就对这个报文段设置一次计时器。新的重传时间=γ*旧的重传时间。 TCP连接建立和释放的过程 SYN置1和FIN的报文段要消耗一个序号。 客户端连接状态变迁：CLOSED -&gt; 主动打开,发送SYN=1 -&gt; SYN_SENT -&gt; 收到服务器的SYN=1和ACK时,发送三次握手的最后一个ACK-&gt; ESTABLISHED -&gt; 数据传送 -&gt; 主动关闭 -&gt; 发送FIN=1,等待确认ACK的到达 -&gt; FIN_WAIT_1 -&gt; 收到确认ACK后 -&gt; FIN_WAIT_2-&gt; 收到服务器发送的FIN=1报文，响应，发送四次挥手的的最后一个确认ACK -&gt; 进入TIME_WAIT状态-&gt; 经过2倍报文寿命，TCP删除连接记录 -&gt; 回到CLOSED状态 客户端状态：CLOSED - SYN_SENT- ESTABLISHED - FIN_WAIT_1 - FIN_WAIT_2 - TIME_WAIT - CLOSED 服务器端连接状态变迁：CLOSED -&gt; 被动打开 -&gt; LISTEN -&gt; 收到SYN=1的报文，发送SYN=1和确认ACK -&gt; 进入SYN_RCVD -&gt; 收到三次握手的最后一个确认ACK -&gt; ESTABLISHED -&gt; 数据传送 -&gt; 数据传送完毕，收到FIN=1 -&gt; 发送确认ACK并进入CLOSED_WAIT -&gt; 发送FIN=1给客户端 -&gt; LAST_ACK-&gt; 收到客户端四次挥手的最后一个确认ACK -&gt; 删除连接记录 -&gt; 回到CLOSED状态 服务器端：CLOSED - LISTEN - SYN_RCVD - ESTABLISHED - CLOSED_WAIT - LAST_ACK - CLOSED 应用层应用层位于TCP/IP体系结构的最高一层，也是直接为应用进程服务的一层，即当不同的应用进程数据交换时，就去调用应用层的不同协议实体，让这些实体去调用传输层的TCP或者UDP来进行网络传输。具体的应用层协议有，SMTP 25、DNS 53、HTTP 80、FTP 20数据端口 21控制端口、TFTP 69、TELNET 23、SNMP 161等 网络的划分按网络拓扑结构：总线、星型、环型、树型、网状结构和混合型。 按覆盖范围：局域网、城域网、广域网 按传播方式：广播网络和点对点网络 广播式网络是指网络中的计算机使用一个共享信道进行数据传播，网络中的所有结点都能收到某一结点发出的数据信息。 单播：一对一的发送形式。 组播：采用一对一组的发送形式，将数据发送给网络中的某一组主机。 广播：采用一对所有，将数据发送给网络所有目的结点。 点对点网络中两个结点间的通信方式是点对点的。如果两台计算机之间没有直连的线路，则需要中间结点的接收、存储、转发直至目的结点。 6. TCP的三次握手和四次挥手的过程以客户端为例 连接建立（三次握手）：首先Client端发送连接请求报文SYN并进入SYN_SENT状态，Server收到后发送ACK+SYN报文，并为这次连接分配资源。Client端接收到Server端的SYN+ACK后发送三次握手的最后一个ACK，并分配资源，连接建立。 连接释放（四次挥手）：假设Client端发起断开连接请求，首先发送FIN=1,等待确认ACK的到达 -&gt; FIN_WAIT_1 -&gt; 收到Server端的确认ACK后时 -&gt; FIN_WAIT_2-&gt;收到服务器发送的FIN=1报文，响应，发送四次挥手的的最后一个确认ACK -&gt;进入TIME_WAIT状态-&gt; 经过2倍报文寿命，TCP删除连接记录 -&gt; 回到CLOSED状态 7. 为什么连接建立是三次握手，而连接释放要四次挥手？因为当Server端收到Client端发送的SYN连接请求报文后，可以直接发送SYN+ACK报文，其中ACK用来应答，SYN用来同步。但是关闭连接时，当Server端收到FIN报文后，并不会立即关闭socket，所以先回复一个ACK，告诉Client端“你的FIN我收到了”，只有等Server端的所有报文发送完了，Server端才发送FIN报文，因此不能一起发送，故需要四次挥手。 8. 为什么TIME_WAIT状态需要2MSL（最大报文段生存时间）才能返回Closed状态？这是因为虽然双方都同意关闭连接了，而且四次挥手的报文也都协调发送完毕。但是我们必须假想网络是不可靠的，无法保证最后发送的ACK报文一定被对方收到，因此处于LAST_ACK状态下的Server端可能会因未收到ACK而重发FIN，所以TIME_WAIT状态的作用就是用来重发可能丢失的ACK报文。 9. Http报文格式Http请求报文格式：1.请求行 2.Http头 3.报文主体请求行由三部分组成，分别是请求方法，请求地址，Http版本Http头：有三种，分别为请求头（request header），普通头（General Header）和实体头（entity header）。Get方法没有实体头。报文主体：只在POST方法请求中存在。Http响应报文：1.状态行 2.Http头 3.返回内容状态行：第一部分为Http版本，第二部分为响应状态码 第三部分为状态码的描述其中第三部分为状态码的描述，信息类100-199 响应成功200-299 重定向类300-399 客户端错误400-499 服务器端错误500-599 常见的 12345678910111213100 continue 初始请求已接受，客户端应继续发送请求剩余部分200 OK202 Accepted 已接受，处理尚未完成 301 永久重定向302 临时重定向400 Bad Request401 Unauthorized403 Forbidden 资源不可用404 Not Found500 Internal Server Error 服务器错误502 Bad Gateway503 Service Unavailable 服务器负载过重504 Gateway Timeout 未能及时从远程服务器获得应答 Http头：响应头（Response Header），普通头（General Header）和实体头(Entity Header)返回内容：即Http请求的信息，可以是HTML也可以是图片等等。 10. Http和Https的区别Https即Secure Hypertext Transfer Protocol，即安全超文本传输协议，它是一个安全通信信道，基于Http开发，用于在客户机和服务器间交换信息。它使用安全套接字层SSL进行信息交换，是Http的安全版。Https协议需要到CA申请证书，一般免费证书很少，需要交费。Http是超文本传输协议，信息是明文传输，https则是具有安全性的tls/ssl加密传输协议。http是80端口，https是443端口 11. 浏览器输入一个URL的过程 浏览器向DNS服务器请求解析该URL中的域名所对应的IP地址 解析出IP地址后，根据IP地址和默认端口80和服务器建立TCP连接 浏览器发出Http请求，该请求报文作为TCP三次握手的第三个报文的数据发送给服务器 服务器做出响应，把对应的请求资源发送给浏览器 释放TCP连接 浏览器解析并显示内容 12. 中间人攻击中间人获取server发给client的公钥，自己伪造一对公私钥，然后伪造自己让client以为它是server，然后将伪造的公钥发给client，并拦截client发给server的密文，用伪造的私钥即可得到client发出去的内容，最后用真实的公钥对内容加密发给server。 解决办法：数字证书，证书链，可信任的中间人 13. 差错检测误码率：传输错误的比特与传输总比特数的比率CRC是检错方法并不能纠错，FCS（Frame Check Sequence）是冗余码。计算冗余码（余数R）的方法：先补0（n个）再对生成多项式取模。CRC只能表示以接近1的概率认为它没有差错。但不能做到可靠传输。可靠传输还需要确认和重传机制。生成多项式P(X)：CRC-16，CRC-CCITT，CRC-32 14. 数据链路层的协议停止等待协议 - 连续ARQ - 选择重传ARQ - PPP - 以太网协议- 帧中继 - ATM - HDLC 15. 截断二进制指数退避算法是以太网用于解决当发生碰撞时就停止发送然后重发再碰撞这一问题。 截断二进制指数退避算法：基本退避时间为2τ k=min{重传次数，10} r=random(0~2^k-1) 重传所需时延为r倍的基本退避时间 八、操作系统（OS基础、Linux等）1. 并发和并行“并行”是指无论从微观还是宏观，二者都是一起执行的，也就是同一时刻执行而“并发”在微观上不是同时执行的。是在同一时间间隔交替轮流执行 2. 进程间通信的方式 管道( pipe )：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。 有名管道 (named pipe) ： 有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。 信号量( semophore ) ：信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 消息队列( message queue ) 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 信号 ( sinal ) ： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。 共享内存( shared memory ) 共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号量配合使用，来实现进程间的同步和通信。 套接字( socket ) ：套接字也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同机器间的进程通信。 3. LinuxIO模型1、阻塞IO模型以socket为例，在进程空间调用recvfrom，其系统调用知道数据包到达且被复制到应用进程的缓冲区或者发生错误才返回，在此期间一直等待，进程从调用recvfrom开始到它返回的整段时间内都是被阻塞的，因此称为阻塞IO 2、非阻塞IO模型应用进程调用recvfrom，如果缓冲区没有数据直接返回EWOULDBLOCK错误。一般对非阻塞IO进行轮询，以确定是否有数据到来。 3、IO多路复用模型 Linux提供select/poll，通过将一个或多个fd传递给select或poll系统调用，阻塞在select上。select/poll顺序扫描fd是否就绪。 4、信号驱动IO 开启套接字接口信号驱动IO功能，并通过系统调用sigaction执行信号处理函数。当数据准备就绪时，为该进程生成SIGIO信号，通过信号回调通知应用程序调用recvfrom来读取数据，并通知主函数处理数据。 5、异步IO 告知内核启动某个操作，并让内核在整个操作完成后通知我们。它与信号驱动IO的区别在于信号驱动IO由内核通知我们何时可以开始IO操作。而异步IO模型由内核通知我们IO操作已经完成。 九、其他1. 开源软件有哪些？Eclipse、Linux及其Linux下的大多数软件、Git等。 Apache下的众多软件：Lucene、Velocity、Maven、高性能Java网络框架MINA、版本控制系统SVN、应用服务器Tomcat、Http服务器Apache、MVC框架Struts、持久层框架iBATIS、Apache SPARK、ActiveMQ 2. 开源协议 MIT：相对宽松。适用：JQuery Apache：相对宽松与MIT类似的协议，考虑有专利的情况。适用：Apache服务器、SVN GPL：GPLV2和GPLV3，如果你在乎作品的传播和别人的修改，希望别人也以相同的协议分享出来。 LGPL：主要用于一些代码库。衍生代码可以以此协议发布（言下之意你可以用其他协议），但与此协议相关的代码必需遵循此协议。 BSD：较为宽松的协议，包含两个变种BSD 2-Clause 和BSD 3-Clause，两者都与MIT协议只存在细微差异。 上面各协议只是针对软件或代码作品，如果你的作品不是代码，比如视频，音乐，图片，文章等，共享于公众之前，也最好声明一下协议以保证自己的权益不被侵犯，CC协议。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>知识点总结</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis学习笔记]]></title>
    <url>%2Fblog%2F2016%2F10%2F02%2F2016-10-02-Redis%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Redis 是一个开源（BSD许可）的，非关系型内存数据库，它可以用作数据库、缓存和消息中间件。 它支持多种类型的数据结构，如 字符串（strings）， 散列（hashes）， 列表（lists）， 集合（sets）， 有序集合（sorted sets） 与范围查询， bitmaps， hyperloglogs 和 地理空间（geospatial） 索引半径查询。 Redis 内置了 复制（replication），LUA脚本（Lua scripting）， LRU驱动事件（LRU eviction），事务（transactions） 和不同级别的 磁盘持久化（persistence）， 并通过 Redis哨兵（Sentinel）和自动 分区（Cluster）提供高可用性（high availability）。 数据类型参考之前写的Redis的五种数据类型 事务1. 事务被正常执行： 123456789101112131415#在Shell命令行下执行Redis的客户端工具。/&gt; redis-cli#在当前连接上启动一个新的事务。redis 127.0.0.1:6379&gt; multiOK#执行事务中的第一条命令，从该命令的返回结果可以看出，该命令并没有立即执行，而是存于事务的命令队列。redis 127.0.0.1:6379&gt; incr t1QUEUED#又执行一个新的命令，从结果可以看出，该命令也被存于事务的命令队列。redis 127.0.0.1:6379&gt; incr t2QUEUED#执行事务命令队列中的所有命令，从结果可以看出，队列中命令的结果得到返回。redis 127.0.0.1:6379&gt; exec1) (integer) 12) (integer) 1 2. 事务中存在失败的命令： 123456789101112131415161718192021#开启一个新的事务。redis 127.0.0.1:6379&gt; multiOK#设置键a的值为string类型的3。redis 127.0.0.1:6379&gt; set a 3QUEUED#从键a所关联的值的头部弹出元素，由于该值是字符串类型，而lpop命令仅能用于List类型，因此在执行exec命令时，该命令将会失败。redis 127.0.0.1:6379&gt; lpop aQUEUED#再次设置键a的值为字符串4。redis 127.0.0.1:6379&gt; set a 4QUEUED#获取键a的值，以便确认该值是否被事务中的第二个set命令设置成功。redis 127.0.0.1:6379&gt; get aQUEUED#从结果中可以看出，事务中的第二条命令lpop执行失败，而其后的set和get命令均执行成功，这一点是Redis的事务与关系型数据库中的事务之间最为重要的差别。redis 127.0.0.1:6379&gt; exec1) OK2) (error) ERR Operation against a key holding the wrong kind of value3) OK4) "4" 3. 回滚事务： 123456789101112131415#为键t2设置一个事务执行前的值。redis 127.0.0.1:6379&gt; set t2 ttOK#开启一个事务。redis 127.0.0.1:6379&gt; multiOK#在事务内为该键设置一个新值。redis 127.0.0.1:6379&gt; set t2 ttnewQUEUED#放弃事务。redis 127.0.0.1:6379&gt; discardOK#查看键t2的值，从结果中可以看出该键的值仍为事务开始之前的值。redis 127.0.0.1:6379&gt; get t2"tt" 4. WATCH命令和基于CAS的乐观锁： 在Redis的事务中，WATCH命令可用于提供CAS(check-and-set)功能。假设我们通过WATCH命令在事务执行之前监控了多个Keys，倘若在WATCH之后有任何Key的值发生了变化，EXEC命令执行的事务都将被放弃，同时返回Null multi-bulk应答以通知调用者事务执行失败。例如，我们再次假设Redis中并未提供incr命令来完成键值的原子性递增，如果要实现该功能，我们只能自行编写相应的代码。其伪码如下： 123val = GET mykeyval = val + 1SET mykey $val 以上代码只有在单连接的情况下才可以保证执行结果是正确的，因为如果在同一时刻有多个客户端在同时执行该段代码，那么就会出现多线程程序中经常出现的一种错误场景–竞态争用(race condition)。比如，客户端A和B都在同一时刻读取了mykey的原有值，假设该值为10，此后两个客户端又均将该值加一后set回Redis服务器，这样就会导致mykey的结果为11，而不是我们认为的12。为了解决类似的问题，我们需要借助WATCH命令的帮助，见如下代码： 123456WATCH mykeyval = GET mykeyval = val + 1MULTISET mykey $valEXEC 和此前代码不同的是，新代码在获取mykey的值之前先通过WATCH命令监控了该键，此后又将set命令包围在事务中，这样就可以有效的保证每个连接在执行EXEC之前，如果当前连接获取的mykey的值被其它连接的客户端修改，那么当前连接的EXEC命令将执行失败。这样调用者在判断返回值后就可以获悉val是否被重新设置成功。 主从复制一、Redis的Replication：这里首先需要说明的是，在Redis中配置Master-Slave模式真是太简单了。相信在阅读完这篇Blog之后你也可以轻松做到。这里我们还是先列出一些理论性的知识，后面给出实际操作的案例。下面的列表清楚的解释了Redis Replication的特点和优势。 1). 同一个Master可以同步多个Slaves。 2). Slave同样可以接受其它Slaves的连接和同步请求，这样可以有效的分载Master的同步压力。因此我们可以将Redis的Replication架构视为图结构。 3). Master Server是以非阻塞的方式为Slaves提供服务。所以在Master-Slave同步期间，客户端仍然可以提交查询或修改请求。 4). Slave Server同样是以非阻塞的方式完成数据同步。在同步期间，如果有客户端提交查询请求，Redis则返回同步之前的数据。 5). 为了分载Master的读操作压力，Slave服务器可以为客户端提供只读操作的服务，写服务仍然必须由Master来完成。即便如此，系统的伸缩性还是得到了很大的提高。 6). Master可以将数据保存操作交给Slaves完成，从而避免了在Master中要有独立的进程来完成此操作。 二、Replication的工作原理：在Slave启动并连接到Master之后，它将主动发送一个SYNC命令。此后Master将启动后台存盘进程，同时收集所有接收到的用于修改数据集的命令，在后台进程执行完毕后，Master将传送整个数据库文件到Slave，以完成一次完全同步。而Slave服务器在接收到数据库文件数据之后将其存盘并加载到内存中。此后，Master继续将所有已经收集到的修改命令，和新的修改命令依次传送给Slaves，Slave将在本次执行这些数据修改命令，从而达到最终的数据同步。 如果Master和Slave之间的链接出现断连现象，Slave可以自动重连Master，但是在连接成功之后，一次完全同步将被自动执行。 三、如何配置Replication：见如下步骤： 1). 同时启动两个Redis服务器，可以考虑在同一台机器上启动两个Redis服务器，分别监听不同的端口，如6379和6380。 2). 在Slave服务器上执行一下命令： 123/&gt; redis-cli -p 6380 #这里我们假设Slave的端口号是6380redis 127.0.0.1:6380&gt; slaveof 127.0.0.1 6379 #我们假设Master和Slave在同一台主机，Master的端口为6379OK 上面的方式只是保证了在执行slaveof命令之后，redis_6380成为了redis_6379的slave，一旦服务(redis_6380)重新启动之后，他们之间的复制关系将终止。如果希望长期保证这两个服务器之间的Replication关系，可以在redis_6380的配置文件中做如下修改： 12345678/&gt; cd /etc/redis #切换Redis服务器配置文件所在的目录。/&gt; ls6379.conf 6380.conf/&gt; vi 6380.conf将# slaveof &lt;masterip&gt; &lt;masterport&gt;改为slaveof 127.0.0.1 6379 保存退出。这样就可以保证Redis_6380服务程序在每次启动后都会主动建立与Redis_6379的Replication连接了。 四、应用示例：这里我们假设Master-Slave已经建立。 1234567891011121314151617181920212223242526272829303132#启动master服务器。[root@dijia478 redis]# redis-cli -p 6379redis 127.0.0.1:6379&gt;#情况Master当前数据库中的所有Keys。redis 127.0.0.1:6379&gt; flushdbOK#在Master中创建新的Keys作为测试数据。redis 127.0.0.1:6379&gt; set mykey helloOKredis 127.0.0.1:6379&gt; set mykey2 worldOK#查看Master中存在哪些Keys。redis 127.0.0.1:6379&gt; keys *1) "mykey"2) "mykey2"#启动slave服务器。[root@dijia478 redis]# redis-cli -p 6380#查看Slave中的Keys是否和Master中一致，从结果看，他们是相等的。redis 127.0.0.1:6380&gt; keys *1) "mykey"2) "mykey2"#在Master中删除其中一个测试Key，并查看删除后的结果。redis 127.0.0.1:6379&gt; del mykey2(integer) 1redis 127.0.0.1:6379&gt; keys *1) "mykey"#在Slave中查看是否mykey2也已经在Slave中被删除。redis 127.0.0.1:6380&gt; keys *1) "mykey" 数据持久化一、Redis提供了哪些持久化机制：1). RDB持久化： 该机制是指在指定的时间间隔内将内存中的数据集快照写入磁盘。 2). AOF持久化: 该机制将以日志的形式记录服务器所处理的每一个写操作，在Redis服务器启动之初会读取该文件来重新构建数据库，以保证启动后数据库中的数据是完整的。 3). 无持久化： 我们可以通过配置的方式禁用Redis服务器的持久化功能，这样我们就可以将Redis视为一个功能加强版的memcached了。 4). 同时应用AOF和RDB。 二、RDB机制的优势和劣势：RDB存在哪些优势呢？ 1). 一旦采用该方式，那么你的整个Redis数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近24小时的数据，同时还要每天归档一次最近30天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 2). 对于灾难恢复而言，RDB是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。 3). 性能最大化。对于Redis的服务进程而言，在开始持久化时，它唯一需要做的只是fork出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行IO操作了。 4). 相比于AOF机制，如果数据集很大，RDB的启动效率会更高。 RDB又存在哪些劣势呢？ 1). 如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么RDB将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。 2). 由于RDB是通过fork子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是1秒钟。 三、AOF机制的优势和劣势：AOF的优势有哪些呢？ 1). 该机制可以带来更高的数据安全性，即数据持久性。Redis中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。至于无同步，无需多言，我想大家都能正确的理解它。 2). 由于该机制对日志文件的写入操作采用的是append模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在Redis下一次启动之前，我们可以通过redis-check-aof工具来帮助我们解决数据一致性的问题。 3). 如果日志过大，Redis可以自动启用rewrite机制。即Redis以append模式不断的将修改数据写入到老的磁盘文件中，同时Redis还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行rewrite切换时可以更好的保证数据安全性。 4). AOF包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。 AOF的劣势有哪些呢？ 1). 对于相同数量的数据集而言，AOF文件通常要大于RDB文件。 2). 根据同步策略的不同，AOF在运行效率上往往会慢于RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和RDB一样高效。 四、其它：1. Snapshotting: 缺省情况下，Redis会将数据集的快照dump到dump.rdb文件中。此外，我们也可以通过配置文件来修改Redis服务器dump快照的频率，在打开6379.conf文件之后，我们搜索save，可以看到下面的配置信息： 123save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，则dump内存快照。save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，则dump内存快照。save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，则dump内存快照。 2. Dump快照的机制： 1231). Redis先fork子进程。2). 子进程将快照数据写入到临时RDB文件中。3). 当子进程完成数据写入操作后，再用临时文件替换老的文件。 3. AOF文件： 上面已经多次讲过，RDB的快照定时dump机制无法保证很好的数据持久性。如果我们的应用确实非常关注此点，我们可以考虑使用Redis中的AOF机制。对于Redis服务器而言，其缺省的机制是RDB，如果需要使用AOF，则需要修改配置文件中的以下条目：将appendonly no改为appendonly yes从现在起，Redis在每一次接收到数据修改的命令之后，都会将其追加到AOF文件中。在Redis下一次重新启动时，需要加载AOF文件中的信息来构建最新的数据到内存中。 4. AOF的配置： 在Redis的配置文件中存在三种同步方式，它们分别是： 123appendfsync always #每次有数据修改发生时都会写入AOF文件。appendfsync everysec #每秒钟同步一次，该策略为AOF的缺省策略。appendfsync no #从不同步。高效但是数据不会被持久化。 5. 如何修复坏损的AOF文件： 1231). 将现有已经坏损的AOF文件额外拷贝出来一份。2). 执行"redis-check-aof --fix &lt;filename&gt;"命令来修复坏损的AOF文件。3). 用修复后的AOF文件重新启动Redis服务器。 6. Redis的数据备份： 在Redis中我们可以通过copy的方式在线备份正在运行的Redis数据文件。这是因为RDB文件一旦被生成之后就不会再被修改。Redis每次都是将最新的数据dump到一个临时文件中，之后在利用rename函数原子性的将临时文件改名为原有的数据文件名。因此我们可以说，在任意时刻copy数据文件都是安全的和一致的。鉴于此，我们就可以通过创建cron job的方式定时备份Redis的数据文件，并将备份文件copy到安全的磁盘介质中。 管道一、请求应答协议和RTT：Redis是一种典型的基于C/S模型的TCP服务器。在客户端与服务器的通讯过程中，通常都是客户端率先发起请求，服务器在接收到请求后执行相应的任务，最后再将获取的数据或处理结果以应答的方式发送给客户端。在此过程中，客户端都会以阻塞的方式等待服务器返回的结果。见如下命令序列： 12345678Client: INCR XServer: 1Client: INCR XServer: 2Client: INCR XServer: 3Client: INCR XServer: 4 在每一对请求与应答的过程中，我们都不得不承受网络传输所带来的额外开销。我们通常将这种开销称为RTT(Round Trip Time)。现在我们假设每一次请求与应答的RTT为250毫秒，而我们的服务器可以在一秒内处理100k的数据，可结果则是我们的服务器每秒至多处理4条请求。要想解决这一性能问题，我们该如何进行优化呢？ 二、管线(pipelining)：Redis在很早的版本中就已经提供了对命令管线的支持。在给出具体解释之前，我们先将上面的同步应答方式的例子改造为基于命令管线的异步应答方式，这样可以让大家有一个更好的感性认识。 12345678Client: INCR XClient: INCR XClient: INCR XClient: INCR XServer: 1Server: 2Server: 3Server: 4 从以上示例可以看出，客户端在发送命令之后，不用立刻等待来自服务器的应答，而是可以继续发送后面的命令。在命令发送完毕后，再一次性的读取之前所有命令的应答。这样便节省了同步方式中RTT的开销。 最后需要说明的是，如果Redis服务器发现客户端的请求是基于管线的，那么服务器端在接受到请求并处理之后，会将每条命令的应答数据存入队列，之后再发送到客户端。]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Redis的五种数据类型及Jedis方法]]></title>
    <url>%2Fblog%2F2016%2F09%2F04%2F2016-09-04-Redis%E7%9A%84%E4%BA%94%E7%A7%8D%E6%95%B0%E6%8D%AE%E7%B1%BB%E5%9E%8B%E5%8F%8A%E6%96%B9%E6%B3%95%2F</url>
    <content type="text"><![CDATA[字符串string：字符串string是Redis中最为基础的数据存储类型，是一个由字节组成的序列，他在Redis中是二进制安全的，这便意味着该类型可以接受任何格式的数据，如JPEG图像数据货Json对象描述信息等，是标准的key-value，一般来存字符串，整数和浮点数。Value最多可以容纳的数据长度为512MB应用场景：很常见的场景用于统计网站访问数量，当前在线人数等。incr命令(++操作) 123456789101112131415set(key, value) // set设置相同的键时,后一个值会覆盖前面的值get(key) // 获取key对应的valuesetnx(key, value) // nx:表示not exists,如果key不存在就设置,如果存在就不设置setrange(key, startIndex, vlaue) // 替换字符串,表示把指定的key的值从开始索引开始是value这个值进行替换mset(key1, value1, key2, value2, ...) // 批量设置键值对msetnx(key1, value1, key2, value2, ...) // 如果key已存在那么设置失败getset(key, newValue) // 获取key的值然后设置新的值getrange(key, startIndex, endIndex) // 获取数据,最后一个值的索引为-1mget(key1, key2, key3, ...) // 批量获取incr(key) // 自增1incrby(key, num) // 指定增减的数量decr(key) // 自减1decrby(key, num) // 指定自增减的数量append(key, value) // 给指定的字符串追加value的值strlen(key) // 获取指定的key对应的值得长度 列表list：Redis的列表list允许用户从序列的两端推入或者弹出元素，列表由多个字符串值组成的有序可重复的序列，是链表结构，类似于Java中的List集合，所以向列表两端添加元素的时间复杂度为0(1)，获取越接近两端的元素速度就越快。这意味着即使是一个有几千万个元素的列表，获取头部或尾部的10条记录也是极快的。List中可以包含的最大元素数量是4294967295。应用场景：1.最新消息排行榜。2.消息队列，以完成多程序之间的消息交换。可以用push操作将任务存在list中（生产者），然后线程在用pop操作将任务取出进行执行。（消费者） 1234567891011lpush(key, value1, value2, ...) // 往list集合中压入元素linsert(key, before/after, value, newValue) // 在指定的元素前或者后插入元素lset(key, index, newValue) // 设置指定下标的值lrem(key, count, value) // 删除count个于value相同的元素,count&gt;0从开始位置进行删除,count&lt;0从末尾开始删除,count=0删除所有的ltrim(key, startIndex, endIndex) // 删除指定范围内以外的元素,保留指定范围内的元素lpop(key) // 从list的头部删除元素lindex(key, index) // 返回指定索引处的元素llen(key) // 返回列表的长度rpush(key, value) // 从末尾压入元素rpop(key) // 从末尾删除元素rpoplpush(key1, key2) // 从key1链表中弹出最后一个元素然后压入到key2链表中 散列hash：Redis中的散列hash可以看成具有String key和String value的map容器，可以将多个key-value存储到一个key中。每一个Hash可以存储4294967295个键值对。hash特别适用于存储对象, 将一个对象存在hash中相对于存储在String可以节省更多的内存。应用场景：例如存储、读取、修改用户属性（name，age，pwd等） 12345678910111213hset(key, field, value) // 给指定的key添加key-value元素hget(key, field) // 获取指定的key中filed字段的值hsetnx(key, field, value) // 如果key不存在进行插入,如果key和field都存在不进行插入hexists(key, field) // 判断指定的key中是否存在field这个字段hlen(key) // 获取指定的key中字段的数量hdel(key, field1, field2, ...) // 删除指定的key中的指定的字段的和对应的值hincrby(key, filed, count) // 给指定的key的field的字段添加或者减去count这个值hgetall(key) // 获取key中所有的键值对,返回的是一个键一个值hkeys(key) // 获取指定的key中所有的字段hmget(key, field1, field2, ...) // 获取指定的key中指定字段的值hmset(key, field1, value1, field2, value2, ...) // 同时设置多个键值对数据hvals(key) // 获取指定的key中所有的valuehincrbyfloat(key field value) // 给指定的字段添加浮点数的值 集合set：Redis的集合set是无序不可重复的，和列表一样，在执行插入和删除和判断是否存在某元素时，效率是很高的。集合最大的优势在于可以进行交集并集差集操作。Set可包含的最大元素数量是4294967295。应用场景：1.利用交集求共同好友。2.利用唯一性，可以统计访问网站的所有独立IP。3.好友推荐的时候根据tag求交集，大于某个threshold（临界值的）就可以推荐。 1234567891011121314sadd(key, member1, member2, ...) // 添加元素scard(key) // 获取成员的数量sismember(key, member) // 判断是否存在member这个成员smembers(key) // 获取所有的成员spop(key) // 随机弹出一个成员srandmember(key, [count]) // 随机获取一个或者多个成员 srem(key, member1, member2, ...) // 删除一个或者多个成员,如果成员不存在则忽略smove(source, desition, member) // 移动一个成员到指定的set中sdiff(first-key, key1, key2, ...) // 返回给定集合之间的差集。不存在的集合 key 将视为空集sdiffstore(destionset, key1, key2, ...) // 把获取到的差集保存到目标set中sinter(key1, key2, ...) // 获取交集sinterstore(destionset, key1, key2, ...) // 把获取到的交集存储到目标set中sunion(key1, key2, ...) // 获取并集sunionstore(destionset, key1, key2, ...) // 把获取到的并集存储到目标set中 有序集合sorted set：Redis的有序集合sorted set和集合set一样，都是字符串的集合，都不允许重复的成员出现在一个set中。他们之间差别在于有序集合中每一个成员都会有一个double类型的分数(score)与之关联，Redis正是通过分数来为集合中的成员进行从小到大的排序。尽管有序集合中的成员必须是唯一的，但是分数(score)却可以重复。应用场景：可以用于一个大型在线游戏的积分排行榜，每当玩家的分数发生变化时，可以执行zadd更新玩家分数(score)，此后在通过zrange获取几分top ten的用户信息。 1234567891011121314zadd(key, score1, member1, score2, member2, ...) // 用于将一个或多个成员元素及其分数值加入到有序集当中。如果某个成员已经是有序集的成员，那么更新这个成员的分数值，并通过重新插入这个成员元素，来保证该成员在正确的位置上。分数值可以是整数值或双精度浮点数。如果有序集合 key 不存在，则创建一个空的有序集并执行 ZADD 操作。当 key 存在但不是有序集类型时，返回一个错误。注意： 在 Redis 2.4 版本以前， ZADD 每次只能添加一个元素。zcard(key) // 计算元素个数zincrby(key, number, member) // 给指定的member的分数添加或者减去number这个值，当 key 不存在，或分数不是 key 的成员时，ZINCRBY key number member 等同于 ZADD key number member 。当 key 不是有序集类型时，返回一个错误。分数值可以是整数值或双精度浮点数。zcount(key, min, max) // 获取分数在min和max之间的成员和数量; 默认是闭区间; 想不包含可以: (min (maxzrange(key, start, stop, [WITHSCORES]) // 返回指定排名之间的成员(结果是分数由低到高)，排名以0开始zrevrange(key, start, stop, [WITHSCORES]) // 返回指定排名之间的成员(结果是分数由高到低)zrangebyscore(key, min, max, [withscores], [limit offset count]) // 根据分数的范围获取成员(按照分数: 从低到高)zrevrangebyscore(key, max, min, [withscores], [limit offset count]) // 根据分数的范围获取成员(从高到低)zrank(key, member) // 返回一个成员的排名(从低到高的顺序)zrevrank(key, member) // 返回一个成员的排名(从高到低)zscore(key, member) // 获取一个成员的分数zrem(key, member1, member2...) // 删除指定的成员zremrangebyrank(key, start, stop) // 根据排名进行删除zremrangebyscore(key, min, max) // 根据分数的范围进行删除 对key的通用操作1234567891011121314151617keys(pattern) // 根据指定的规则返回符合条件的keydel(key1, key2, ...) // 删除指定的keyexists(key) // 判断是否存在指定的keymove(key, db) // 将指定的key移入到指定的数据库中，redis默认存在16个库select 10:表示选择9号库rename(key, newkey) // 对key进行重命名renamenx(key, newkey) // 仅当 newkey 不存在时，将 key 改名为 newkeytype(key) // 返回key的类型expire(key, second) // 给指定的key设置失效时间expireat(key, timestamp) // 以时间戳的形式设置key的失效时间pexpireat(key, timestamp) // 以毫秒为单位设置key的失效时间persist(key) // 移除key的失效时间ttl(key) // 以秒为单位返回key的剩余时间(返回-2表示key不存在, 返回-1表示永远不过时)pttl(key) // 一毫秒为单位返回key的失效时间randomkey() // 随机返回一个keydump(key) // 序列化给定 keyflushdb() // 删除当前选择数据库中的所有keyflushall() // 删除所有数据库中的所有key Jedis API]]></content>
      <categories>
        <category>Redis</category>
      </categories>
      <tags>
        <tag>Jedis方法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何抽取代码后，在父类实例化泛型的问题]]></title>
    <url>%2Fblog%2F2016%2F08%2F18%2F2016-08-18-%E5%A6%82%E4%BD%95%E6%8A%BD%E5%8F%96%E4%BB%A3%E7%A0%81%E5%90%8E%EF%BC%8C%E5%9C%A8%E7%88%B6%E7%B1%BB%E5%AE%9E%E4%BE%8B%E5%8C%96%E6%B3%9B%E5%9E%8B%E7%9A%84%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[代码背景：有大量的action类中需要使用模型驱动，所以会抽取公共部分代码， 但由于action类要实现ModelDriven接口，抽取成BaseAction里面的泛型T如何在子类实例化的问题。 123456789101112131415161718192021222324252627282930313233343536/** * 抽取Action的公共代码 ，简化开发 * * @author dijia478 * @version 1.0，2017-8-30 20:36 */public abstract class BaseAction&lt;T&gt; extends ActionSupport implements ModelDriven&lt;T&gt; &#123; /** 模型驱动 */ protected T model; @Override public T getModel() &#123; return model; &#125; /** * 构造器 完成model实例化 */ public BaseAction() &#123; // 构造子类Action对象 ，获取继承父类型的泛型 // AreaAction extends BaseAction&lt;Area&gt; // BaseAction&lt;Area&gt; Type genericSuperclass = this.getClass().getGenericSuperclass(); // 获取类型第一个泛型参数 ParameterizedType parameterizedType = (ParameterizedType) genericSuperclass; Class&lt;T&gt; modelClass = (Class&lt;T&gt;) parameterizedType.getActualTypeArguments()[0]; try &#123; model = modelClass.newInstance(); &#125; catch (InstantiationException | IllegalAccessException e) &#123; e.printStackTrace(); System.out.println("模型构造失败..."); &#125; &#125;&#125; 这段代码是通过构造器来实例化泛型T的，子类要继承BaseAction，加载子类的时候，子类构造默认会调用supper();，所以会将泛型T实例化]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>泛型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[关于Java泛型]]></title>
    <url>%2Fblog%2F2016%2F07%2F31%2F2016-07-31-%E5%85%B3%E4%BA%8EJava%E6%B3%9B%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[泛型概念的提出（为什么需要泛型）？首先，我们看下下面这段简短的代码: 1234567891011121314public class GenericTest &#123; public static void main(String[] args) &#123; List list = new ArrayList(); list.add("dijia478"); list.add("corn"); list.add(100); for (int i = 0; i &lt; list.size(); i++) &#123; String name = (String) list.get(i); // 1 System.out.println("name:" + name); &#125; &#125;&#125; 定义了一个List类型的集合，先向其中加入了两个字符串类型的值，随后加入一个Integer类型的值。这是完全允许的，因为此时list默认的类型为Object类型。在之后的循环中，由于忘记了之前在list中也加入了Integer类型的值或其他编码原因，很容易出现类似于//1中的错误。因为编译阶段正常，而运行时会出现“java.lang.ClassCastException”异常。因此，导致此类错误编码过程中不易发现。 在如上的编码过程中，我们发现主要存在两个问题： 1.当我们将一个对象放入集合中，集合不会记住此对象的类型，当再次从集合中取出此对象时，改对象的编译类型变成了Object类型，但其运行时类型任然为其本身类型。 2.因此，//1处取出集合元素时需要人为的强制类型转化到具体的目标类型，且很容易出现“java.lang.ClassCastException”异常。 那么有没有什么办法可以使集合能够记住集合内元素各类型，且能够达到只要编译时不出现问题，运行时就不会出现“java.lang.ClassCastException”异常呢？答案就是使用泛型。 什么是泛型？泛型，即“参数化类型”。一提到参数，最熟悉的就是定义方法时有形参，然后调用此方法时传递实参。那么参数化类型怎么理解呢？顾名思义，就是将类型由原来的具体的类型参数化，类似于方法中的变量参数，此时类型也定义成参数形式（可以称之为类型形参），然后在使用/调用时传入具体的类型（类型实参）。 看着好像有点复杂，首先我们看下上面那个例子采用泛型的写法。 123456789101112131415161718192021public class GenericTest &#123; public static void main(String[] args) &#123; /* List list = new ArrayList(); list.add("dijia478"); list.add("corn"); list.add(100); */ List&lt;String&gt; list = new ArrayList&lt;String&gt;(); list.add("dijia478"); list.add("corn"); //list.add(100); // 1 提示编译错误 for (int i = 0; i &lt; list.size(); i++) &#123; String name = list.get(i); // 2 System.out.println("name:" + name); &#125; &#125;&#125; 采用泛型写法后，在//1处想加入一个Integer类型的对象时会出现编译错误，通过List\，直接限定了list集合中只能含有String类型的元素，从而在//2处无须进行强制类型转换，因为此时，集合能够记住元素的类型信息，编译器已经能够确认它是String类型了。 结合上面的泛型定义，我们知道在List\中，String是类型实参，也就是说，相应的List接口中肯定含有类型形参。且get()方法的返回结果也直接是此形参类型（也就是对应的传入的类型实参）。下面就来看看List接口的的具体定义： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public interface List&lt;E&gt; extends Collection&lt;E&gt; &#123; int size(); boolean isEmpty(); boolean contains(Object o); Iterator&lt;E&gt; iterator(); Object[] toArray(); &lt;T&gt; T[] toArray(T[] a); boolean add(E e); boolean remove(Object o); boolean containsAll(Collection&lt;?&gt; c); boolean addAll(Collection&lt;? extends E&gt; c); boolean addAll(int index, Collection&lt;? extends E&gt; c); boolean removeAll(Collection&lt;?&gt; c); boolean retainAll(Collection&lt;?&gt; c); void clear(); boolean equals(Object o); int hashCode(); E get(int index); E set(int index, E element); void add(int index, E element); E remove(int index); int indexOf(Object o); int lastIndexOf(Object o); ListIterator&lt;E&gt; listIterator(); ListIterator&lt;E&gt; listIterator(int index); List&lt;E&gt; subList(int fromIndex, int toIndex);&#125; 我们可以看到，在List接口中采用泛型化定义之后，\中的E表示类型形参，可以接收具体的类型实参，并且此接口定义中，凡是出现E的地方均表示相同的接受自外部的类型实参。自然的，ArrayList作为List接口的实现类，其定义形式是： 123456789101112131415161718public class ArrayList&lt;E&gt; extends AbstractList&lt;E&gt; implements List&lt;E&gt;, RandomAccess, Cloneable, java.io.Serializable &#123; public boolean add(E e) &#123; ensureCapacityInternal(size + 1); // Increments modCount!! elementData[size++] = e; return true; &#125; public E get(int index) &#123; rangeCheck(index); checkForComodification(); return ArrayList.this.elementData(offset + index); &#125; //...省略掉其他具体的定义过程&#125; 由此，我们从源代码角度明白了为什么//1处加入Integer类型对象编译错误，且//2处get()到的类型直接就是String类型了。 自定义泛型接口、泛型类和泛型方法从上面的内容中，大家已经明白了泛型的具体运作过程。也知道了接口、类和方法也都可以使用泛型去定义，以及相应的使用。是的，在具体使用时，可以分为泛型接口、泛型类和泛型方法。 自定义泛型接口、泛型类和泛型方法与上述Java源码中的List、ArrayList类似。如下，我们看一个最简单的泛型类和方法定义： 123456789101112131415161718192021222324252627public class GenericTest &#123; public static void main(String[] args) &#123; Box&lt;String&gt; name = new Box&lt;String&gt;("corn"); System.out.println("name:" + name.getData()); &#125;&#125;class Box&lt;T&gt; &#123; private T data; public Box() &#123; &#125; public Box(T data) &#123; this.data = data; &#125; public T getData() &#123; return data; &#125;&#125; 在泛型接口、泛型类和泛型方法的定义过程中，我们常见的如T、E、K、V等形式的参数常用于表示泛型形参，由于接收来自外部使用时候传入的类型实参。那么对于不同传入的类型实参，生成的相应对象实例的类型是不是一样的呢？ 1234567891011121314public class GenericTest &#123; public static void main(String[] args) &#123; Box&lt;String&gt; name = new Box&lt;String&gt;("corn"); Box&lt;Integer&gt; age = new Box&lt;Integer&gt;(712); System.out.println("name class:" + name.getClass()); // com.dijia478.Box System.out.println("age class:" + age.getClass()); // com.dijia478.Box System.out.println(name.getClass() == age.getClass()); // true &#125;&#125; 由此，我们发现，在使用泛型类时，虽然传入了不同的泛型实参，但并没有真正意义上生成不同的类型，传入不同泛型实参的泛型类在内存上只有一个，即还是原来的最基本的类型（本实例中为Box），当然，在逻辑上我们可以理解成多个不同的泛型类型。 究其原因，在于Java中的泛型这一概念提出的目的，导致其只是作用于代码编译阶段，在编译过程中，对于正确检验泛型结果后，会将泛型的相关信息擦出，也就是说，成功编译过后的class文件中是不包含任何泛型信息的。泛型信息不会进入到运行时阶段。 对此总结成一句话：泛型类型在逻辑上看以看成是多个不同的类型，实际上都是相同的基本类型。 类型通配符接着上面的结论，我们知道，Box\和Box\实际上都是Box类型，现在需要继续探讨一个问题，那么在逻辑上，类似于Box\和Box\是否可以看成具有父子关系的泛型类型呢？ 为了弄清这个问题，我们继续看下下面这个例子: 1234567891011121314151617181920public class GenericTest &#123; public static void main(String[] args) &#123; Box&lt;Number&gt; name = new Box&lt;Number&gt;(99); Box&lt;Integer&gt; age = new Box&lt;Integer&gt;(712); getData(name); //The method getData(Box&lt;Number&gt;) in the type GenericTest is //not applicable for the arguments (Box&lt;Integer&gt;) getData(age); // 1 &#125; public static void getData(Box&lt;Number&gt; data)&#123; System.out.println("data :" + data.getData()); &#125;&#125; 我们发现，在代码//1处出现了错误提示信息：The method getData(Box\) in the t ype GenericTest is not applicable for the arguments (Box\)。显然，通过提示信息，我们知道Box\在逻辑上不能视为Box\的父类。那么，原因何在呢？ 1234567891011121314151617181920212223242526272829303132333435363738public class GenericTest &#123; public static void main(String[] args) &#123; Box&lt;Integer&gt; a = new Box&lt;Integer&gt;(712); Box&lt;Number&gt; b = a; // 1 Box&lt;Float&gt; f = new Box&lt;Float&gt;(3.14f); b.setData(f); // 2 &#125; public static void getData(Box&lt;Number&gt; data) &#123; System.out.println("data :" + data.getData()); &#125;&#125;class Box&lt;T&gt; &#123; private T data; public Box() &#123; &#125; public Box(T data) &#123; setData(data); &#125; public T getData() &#123; return data; &#125; public void setData(T data) &#123; this.data = data; &#125;&#125; 这个例子中，显然//1和//2处肯定会出现错误提示的。在此我们可以使用反证法来进行说明。 假设Box\在逻辑上可以视为Box\的父类，那么//1和//2处将不会有错误提示了，那么问题就出来了，通过getData()方法取出数据时到底是什么类型呢？Integer? Float? 还是Number？且由于在编程过程中的顺序不可控性，导致在必要的时候必须要进行类型判断，且进行强制类型转换。显然，这与泛型的理念矛盾，因此，在逻辑上Box\不能视为Box\的父类。 好，那我们回过头来继续看“类型通配符”中的第一个例子，我们知道其具体的错误提示的深层次原因了。那么如何解决呢？总部能再定义一个新的函数吧。这和Java中的多态理念显然是违背的，因此，我们需要一个在逻辑上可以用来表示同时是Box\和Box\的父类的一个引用类型，由此，类型通配符应运而生。 类型通配符一般是使用 ? 代替具体的类型实参。注意了，此处是类型实参，而不是类型形参！且Box\&lt;?&gt;在逻辑上是Box\、Box\…等所有Box\&lt;具体类型实参&gt;的父类。由此，我们依然可以定义泛型方法，来完成此类需求。 123456789101112131415161718public class GenericTest &#123; public static void main(String[] args) &#123; Box&lt;String&gt; name = new Box&lt;String&gt;("corn"); Box&lt;Integer&gt; age = new Box&lt;Integer&gt;(712); Box&lt;Number&gt; number = new Box&lt;Number&gt;(314); getData(name); getData(age); getData(number); &#125; public static void getData(Box&lt;?&gt; data) &#123; System.out.println("data :" + data.getData()); &#125;&#125; 有时候，我们还可能听到类型通配符上限和类型通配符下限。具体有是怎么样的呢？ 在上面的例子中，如果需要定义一个功能类似于getData()的方法，但对类型实参又有进一步的限制：只能是Number类及其子类。此时，需要用到类型通配符上限。 1234567891011121314151617181920212223242526public class GenericTest &#123; public static void main(String[] args) &#123; Box&lt;String&gt; name = new Box&lt;String&gt;("corn"); Box&lt;Integer&gt; age = new Box&lt;Integer&gt;(712); Box&lt;Number&gt; number = new Box&lt;Number&gt;(314); getData(name); getData(age); getData(number); //getUpperNumberData(name); // 1 getUpperNumberData(age); // 2 getUpperNumberData(number); // 3 &#125; public static void getData(Box&lt;?&gt; data) &#123; System.out.println("data :" + data.getData()); &#125; public static void getUpperNumberData(Box&lt;? extends Number&gt; data)&#123; System.out.println("data :" + data.getData()); &#125;&#125; 此时，显然，在代码//1处调用将出现错误提示，而//2 //3处调用正常。 类型通配符上限通过形如Box\&lt;? extends Number&gt;形式定义，相对应的，类型通配符下限为Box\&lt;? super Number&gt;形式，其含义与类型通配符上限正好相反，在此不作过多阐述了。 话外篇本文中的例子主要是为了阐述泛型中的一些思想而简单举出的，并不一定有着实际的可用性。另外，一提到泛型，相信大家用到最多的就是在集合中，其实，在实际的编程过程中，自己可以使用泛型去简化开发，且能很好的保证代码质量。并且还要注意的一点是，Java中没有所谓的泛型数组一说。 对于泛型，最主要的还是需要理解其背后的思想和目的。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>泛型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux常用命令]]></title>
    <url>%2Fblog%2F2016%2F05%2F13%2F2016-05-13-Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[一、系统目录结构 约定俗成： bin (binaries)存放二进制可执行文件 etc (etcetera)存放系统配置文件 usr (unix shared resources)用于存放共享的系统资源 home 存放用户文件的根目录 root 超级用户目录 sbin (super user binaries)存放二进制可执行文件，只有root才能访问 dev (devices)用于存放设备文件 lib (library)存放跟文件系统中的程序运行所需要的共享库及内核模块 mnt (mount)系统管理员安装临时文件系统的安装点 boot 存放用于系统引导时使用的各种文件 tmp (temporary)用于存放各种临时文件 var (variable)用于存放运行时需要改变数据的文件 二、基本命令加粗命令为本人比较常用的命令（会经常更新） 1.目录操作相对路径和绝对路径说明： 现在在/home/dijia478/下./a/b.txt和a/b.txt都表示相对路径，当前目录下的a文件夹下的b.txt文件/home/dijia478/a/b.txt表示绝对路径，根目录下的home文件夹下的dijia478文件夹下的a文件夹下的b.txt文件 cd ./a 切换到当前目录的a文件夹cd .. 切换到上一层目录cd / 切换到系统根目录cd ~ 切换到用户主目录cd - 切换到上一个所在目录pwd 显示当前所在目录的绝对路径 2.查看文件列表ls /path/ 显示该目录所有文件或文件夹名ls -a 显示所有文件或文件夹名（包含隐藏的）ls -l 按列表显示所有文件或文件夹，缩写成ll ll -h 友好的显示文件大小（显示成K，MB，GB） 3.创建和删除文件夹mkdir app 创建app文件夹mkdir –p app2/test 级联创建aap2以及test文件夹 rmdir app 删除app目文件夹（需要是空文件夹） 4.文件操作rm a.txt 删除a.txt文件，删除需要用户确认，y/nrm -f a.txt 不询问，直接删除a.txt文件rm -r a 递归删除a文件夹（无论是否有内容）rm -rf a 不询问递归删除a文件夹（慎用）rm -rf * 删除当前目录下所有内容（最好别用）rm -rf /\ no 作 no die（Linux系统就玩完了）* cp a.txt b.txt 将a.txt复制为b.txt文件cp -r dir/ ../ 将dir文件夹及子目录和文件复制到上一层目录中 mv a.txt ../ 将a.txt文件移动到上一层目录中mv a.txt b.txt 将a.txt文件重命名为b.txt touch a.txt 创建一个空的a.txt文件echo “good good study” &gt; a.txt 把”&gt;”左边的输出内容放到右边的文件里去，如果存在就覆盖，如果不存在就创建vim a.txt 用文本编辑器编辑一个文件，如果不存在就创建 5.文件打包归档和压缩tar -cvf file.tar dirpath filepath 将dir文件夹和file文件在当前目录下打包成file.tartar –xvf file.tar 解包到当前目录 gzip file.tar 压缩文件或文件夹gzip –d file.tar.gz 解压文件或文件夹 tar -czvf file.tar.gz dirpath filepath 将dir文件夹和file文件在当前目录下打包并压缩成file.tar.gztar -xzvf file.tar.gz 解压并解包到当前目录下tar -xzvf file.tar.gz -C /home/dijia478/ 解压并解包到/home/dijia478/目录下 常用参数：-c：创建一个新tar文件-v：显示运行过程的信息-f：指定文件名-z：调用gzip压缩命令进行压缩-t：查看压缩文件的内容-x：解开tar文件 zip test.txt.zip test.txt 也是打包并压缩unzip test.txt.zip 解包并解包 .rpm结尾的包，用rpm -ivh 命令安装 6.查看文本文件cat a.txt 一次性显示整个文件内容more a.txt 可以分页看（翻页：空格,往回翻：b ，退出： q或者 Ctrl+C）less a.txt 不仅可以分页，还可以方便地搜索，回翻等操作（翻页：空格,往回翻：↑,往下翻:↓,退出：q或者 Ctrl+C） tail -10 a.txt 查看文件的尾部的10行tail -f user.log 实时刷新显示文件的尾部，这条命令对于观察调试程序的运行非常重要 head -20 a.txt 查看文件的头部20行注：ctrl+c 结束查看 7.搜索查找命令grep ‘haha’ ./* 打印当前目录下所有文件中含有’haha’的地方（支持正则表达式）grep -c ‘haha’ ./* 显示匹配到的行数grep -r ‘haha’ ./* 对子目录也进行遍历搜索grep -l ‘haha’ ./* 只显示命中的文件名grep -n ‘haha’ ./* 显示命中的行号grep -ld skip ‘haha’ ./* 显示命中的文件名，不要搜索子目录 参数含义：-r 递归搜索子目录-l 只列出有匹配行的文件名-n 列出匹配行的行号-d skip 不搜索子文件夹 常用grep跟其他命令组合使用来查找我们关心的信息（管道）示例：service –status-all | grep ‘httpd’ 在当前系统所有服务中查找’httpd’netstat -nltp | grep ‘22’ 查找监听’22’端口的服务程序ps –ef | grep java 查找系统中当前运行的java进程 find ./ -name ‘*.txt’ 查找以.txt结尾的文件（会遍历当前目录）find ./ -name ‘install*’ 查找以install开头的文件或文件夹find ./ -type f 查找普通文件find ./ -type l 查找连接文件(快捷方式) 8.文本命令> 重定向输出，覆盖原有内容；>&gt; 重定向输出，又追加功能；cat /etc/passwd &gt; a.txt 将密码文件输出定向到a.txt中cat /etc/passwd &gt;&gt; a.txt 输出并且追加ifconfig &gt; ifconfig.txt 保存ip信息到文件中 wc -l a.txt 统计文本行数wc -w a.txt统计文本单词数wc -m a.txt统计文本字符数wc -c a.txt统计文本字节数 vim编辑器，这里全都很常用vim filepath打开文件按Esc键切换到命令行模式切换到插入模式：i在当前位置生前插入I在当前行首插入a在当前位置后插入A在当前行尾插入o在当前行之后插入一行O在当前行之前插入一行dd删除整行7 dd向上删除7行U回退(类似于windows 中 ctrl + z)R替换:（冒号）切换到底行模式:q退出:wq保存并退出（shift + zz也可以保存）:q!不保存退出 9.其他常用命令echo $JAVA_HOME 输出变量JAVA_HOME的值 rpm -qa | grep tomcat 查看当前系统是否安装tomcatrpm -e tomcat 卸载tomcatrpm 的其他附加命令–force 强制操作 如强制安装删除等；–requires 显示该包的依赖关系；–nodeps 忽略依赖关系并继续操作； whoami查询当前登陆的用户名which ls查询ls命令的$PATH路径 mkdir test &amp;&amp; cd test只有在 &amp;&amp; 左边的命令返回真（命令返回值 $? == 0），&amp;&amp; 右边的命令才会被执行。只要有一个命令返回假（命令返回值 $? == 1），后面的命令就不会被执行。 三、用户管理命令1.添加用户基本示例：useradd user001 passwd 123456需要设置密码 参数手册：-u 指定组ID（uid）-g 指定所属的组名（gid）-G 指定多个组，用逗号“，”分开（Groups）-c 用户描述（comment）-d 指定用户目录-e 失效时间（expire date） 2.删除用户userdel user002这样删除的时候，用户的主目录会被保留userdel -r user002删除用户的同时删除用户的主目录 3.修改用户属性指令：usermod参数：-l 修改用户名 （login）usermod -l a b（b改为a）-g 修改组 usermod -g sys tom-d 修改用户的宿主目录-G 添加多个组 usermod -G sys,root tom-L 锁定用户账号密码（Lock）-U 解锁用户账号（Unlock）示例：usermod -l user002 user001将user001的登陆名改为user002usermod -g root user002将user002的组改为root组usermod -G hello1,hello2 user002给user002添加两个组hello1,hello2usermod -d /home/dijia478 user002将user002的主目录改成/home/dijia478（要事先创建dijia478目录，并且拷入环境变量文件） 4.用户组管理用户组相关属性：每个用户至少属于一个用户组（创建新用户时如果不指定所属组，则会自动创建并归属到一个跟用户名同名的组）每个用户组可以包含多个用户同一个用户组的用户享有该组共有的权限用户组管理操作命令：groupadd java创建用户组groupdel hello2删除用户组groupmod –n newname oldname修改用户组名称groups user002查看用所属的组 5.用户及用户组相关配置文件用户配置信息存放位置：保存用户信息的文件：/etc/passwd保存密码的文件：/etc/shadow passwd文件示例：user002:x:500:500:user002:/home/user002:/bin/bashpasswd文件各字段含义：account:password:UID:GID:GECOS:directory:shell shadow文件示例：user002:$1$vRug41$UUxYzdP0i6s6wtUPieGDQ/:18617:0:99999:7:::shadow文件各字段含义： 用户名 登陆系统的用户名 密码 加密密码 最后一次修改时间 用户最后一次修改密码距现在的天数，从1970-1-1起 最小时间间隔 两次修改密码之间的最小天数 最大时间间隔 密码有效天数 警告时间 从系统警告到密码失效的天数 账号闲置时间 账号闲置时间 失效时间 密码失效的天数 标志 标志 用户组配置信息存放位置：保存用户组的文件：/etc/group保存用户组密码的文件：/etc/gshadow（设置组管理员时才有用） 6.其他的用户管理命令id user002查看一个用户的UID和GIDsu user001切换到user001用户su - user001切换到user001用户，并且将环境也切换到user001用户的环境（推荐使用）exis退出当前shell（会退出当前登录用户） 四、网络配置管理1.网卡地址配置检查网络连通性:ping 192.168.25.168 测试当前服务器和指定ip是否能进行网络通信 查看ip地址ifconfig 查看所有网络设备的地址信息ifconfig eth0 查看指定的eth0以太网卡的地址信息 修改ip地址ifconfig eth0 192.168.2.150 netmask 255.255.255.0 通过setup修改网络配置在root权限下执行setup指令可以打开一个带菜单的伪图形界面来修改网络配置 通过配置文件修改ip地址vi /etc/sysconfig/network-scripts/ifcfg-eth0 修改该配置文件即可改ip地址 2.主机名配置管理查看主机名在shell提示符的@后有显示或者用hostname指令打印出主机名 修改主机名vi /etc/sysconfig/network 修改其中的hostname配置项:HOSTNAME=newname修改完成之后要重启服务器才能生效要想立即生效，可以执行指令 hostname newname，然后注销重登陆 主机名-IP映射，服务器本地映射服务器网络寻址时默认是现在本地的hosts文件中查找IP映射，通过修改hosts来映射局域网内部的主机名非常方便实现方法，将局域网内的每一台主机的“hostnamip”写入每一台主机的hosts文件中：vi /etc/hosts192.168.2.150 user001-server-01192.168.2.151 user001-server-02192.168.2.152 user001-server-03 3.网络服务启动与停止列出系统所有应用服务状态：service –status-all查看指定服务运行状态：service servicename status启动服务：service servicename start停止服务：service servicename stop 列出所有服务的随机自起配置：chkconfig –list关闭服务的随机自起：chkconfig servicename off开启服务的随机自起：chkconfig servicename on 常用示例：重启网络服务：service network restart停止httpd：service httpd stop启动httpd：service httpd start关闭防火墙服务：service iptables stop关闭防火墙自动启动：chkconfig iptables off 4.查看网络连接信息指令：netstatnetstat常用示例：netstat -natpnetstat -nltpnetstat -naup netstat -an | grep 3306 查询3306端口占用情况 常用参数解释：-a 显示所有连接和监听端口-l 只显示监听进程-t (tcp)仅显示tcp相关选项-u (udp)仅显示udp相关选项-n 拒绝显示别名，能显示数字的全部转化成数字。-p 显示建立相关链接的程序名 lsof命令其实要比netstat强大常用参数：-i 显示所有连接-i 6 仅获取IPv6流量-iTCP 仅显示TCP连接-i:8080 显示与8080端口相关的网络信息-i@192.168.128.128 显示指定到192.168.128.128的连接-i@192.168.128.128:22 显示基于主机与端口的连接-i -sTCP:LISTEN 找出正等候连接的端口-i -sTCP:ESTABLISHED 找出已建立的连接-u 用户名 显示指定用户打开了什么-u ^用户名 显示除指定用户以外的其它所有用户所做的事情-p 10075 查看指定进程ID已打开的内容其他用法：kill -9 ‘lsof -t -u 用户名’ 杀死指定用户所做的一切事情lsof /var/log/messages/ 显示与指定目录或文件交互的所有一切lsof -u dijia478 -i @1.1.1.1 显示dijia478连接到1.1.1.1所做的一切 五、常用系统管理命令1.磁盘/内存使用信息查看df -h 查看磁盘空间状态信息du -sh * 查看指定目录下所有子目录和文件的汇总大小free 查看内存使用状况 2.进程管理top 查看实时刷新的系统进程信息ps -ef 查看系统中当前瞬间的进程信息快照ps -ef | grep myshell 搜索myshell进程的信息kill -9 pid 杀掉指定pid的进程（-9 表示强制杀死） 3.sudo权限的配置root用户因为具有不受限制的权限，使用不慎可能对系统造成不可估量的损害，因而，生产实际中，轻易不要使用su去切换到root的身份如果普通用户需要使用一些系统级管理命令，可以使用sudo来执行，比如 sudo vim /etc/profile给普通用户赋予sudo权限，配置方法如下：例如，要给hadoop用户赋予sudo任何指令（或某条指定的命令）的权利，则编辑sudoers文件 vim /etc/sudoers在其中加入需要赋予权限的用户&gt; root ALL=(ALL) ALL#让hadoop用户可以用root身份执行所有指令hadoop ALL=(ALL) ALL#让user002用户可以用root身份执行useradd,passwd命令user002 ALL=(root) /usr/sbin/useradd, /usr/bin/passwd 检查是否生效:[root@user001-server-01 user002]# sudo -lU user002User user002 is not allowed to run sudo on user001-server-01. 4.修改系统的默认启动级别vim /etc/inittab &gt; # 0 - halt (Do NOT set initdefault to this)# 1 - Single user mode# 2 - Multiuser, without NFS (The same as 3, if you do not have networking)# 3 - Full multiuser mode# 4 - unused# 5 - X11# 6 - reboot (Do NOT set initdefault to this)#id:3:initdefault:~ 用level 3 就启动全功能状态的命令行界面，5是图形界面。不要设置其他的，容易作死。在命令行模式下，用startx可以手动启动图形界面(在服务器上操作) 5.文件权限管理 Linux三种文件类型：普通文件： 包括文本文件、数据文件、可执行的二进制程序文件等。目录文件： Linux系统把目录看成是一种特殊的文件，利用它构成文件系统的树型结构。设备文件： Linux系统把每一个设备都看成是一个文件文件类型标识：普通文件（-）目录（d）符号链接（l）* 进入etc可以查看，相当于快捷方式字符设备文件（c）块设备文件（s）套接字（s）命名管道（p） u 表示“用户（user）”，即文件或目录的所有者。g 表示“同组（group）用户”，即与文件属主有相同组ID的所有用户。o 表示“其他（others）用户”。a 表示“所有（all）用户”。它是系统默认值。操作符号可以是：+ 添加某个权限。- 取消某个权限。= 赋予给定权限并取消其他所有权限（如果有的话）。设置mode所表示的权限可用下述字母的任意组合：r 可读。w 可写。x 可执行。 文件权限管理：chmod u+rwx a.txt 为a.txt添加所属用户的rwx权限chmod 755 a.txt 为a.txt设置所属用户rwx权限，所属组rx权限，其他用户rx权限（r-4，w-2，x-1）chmod u=rwx,g=rx,o=rx a.txt （u代表所属用户 g代表所属组的成员用户 o代表其他用户）chown user001:hello1 a.txt 将a.txt的所有者改成user001用户，所属组改成hello1组（需要root权限）chown -R user001:hello1 dir 将dir文件夹的所有者改成user001用户，所属组改成hello1组（需要root权限） 6.其他系统管理命令date “+%Y%m%d”按格式显示当前系统时间date -s “2020-01-01 10:10:10”设置系统时间clear清屏幕(只是滚到上面看不到了)uname 显示系统信息uname -a 显示本机详细信息。依次为：内核名称(类别)，主机名，内核版本号，内核版本，内核编译日期，硬件名，处理器类型，硬件平台类型，操作系统名称]]></content>
      <categories>
        <category>Linux</category>
      </categories>
      <tags>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[IDEA常用快捷键]]></title>
    <url>%2Fblog%2F2016%2F04%2F09%2F2016-04-09-IDEA%E5%B8%B8%E7%94%A8%E5%BF%AB%E6%8D%B7%E9%94%AE%2F</url>
    <content type="text"><![CDATA[一、按键位排序Ctrl 键位 说明 Ctrl + / 注释 Ctrl + B 进入光标所在方法或变量的定义处（等效Ctrl + 左键单击） Ctrl + D 复制光标选中内容（可以更改设置为复制光标选中所在行） Ctrl + E 显示最近打开的文件 Ctrl + F 在当前文件进行文本查找 Ctrl + H 在右侧显示当前类或接口的继承或实现结构 Ctrl + I 选择可实现的方法 Ctrl + J 插入自定义的动态代码模板 Ctrl + N 打开指定类 Ctrl + O 选择可重写的方法 Ctrl + P 方法参数类型提示（参考Ctrl + Shift + P） Ctrl + Q 快速查看文档内容 Ctrl + R 在当前文件进行文本替换 Ctrl + S 全部保存 Ctrl + U 前往当前光标所在的方法的父类的方法 / 接口定义 Ctrl + W 递进式选择代码块 Ctrl + Y 删除当前行 Ctrl + F1 在光标所在的错误代码处显示错误信息 Ctrl + F3 调转到所选中的词的下一个引用位置 Ctrl + F4 关闭当前编辑文件 Ctrl + F7 搜索当前文件中使用的地方（参考Alt + F7） Ctrl + F8 在 Debug 模式下，设置光标当前行为断点，如果当前已经是断点则去掉断点 Ctrl + F12 弹出当前文件结构层（所有方法和成员），可以在弹出的层上直接输入，进行筛选 Ctrl + 空格 自动完成名称，建议将空格修改为逗号（参考Ctrl + Shift + Space，Ctrl + Alt + Space） Alt 键位 说明 Alt + Q 显示当前方法的定义声明 Alt + 1 快速打开或隐藏工程面板 Alt + F1 在各个面板定位当前元素 Alt + F3 选中或输入文本,按F3逐个向下查找相同内容 Alt + F7 在下方显示全项目中使用的地方，会显示结构（参考Ctrl + F7，类似Ctrl + Alt + F7） Alt + Enter 快速修复错误 Alt + Insert 生成构造方法，getter/setter，hashcode等代码，在左侧目录用上可以生成文件 Alt + 左/右 切换试图 Alt + 上/下 切换方法 Shift 键位 说明 Shift + F2 快速定位到下一个警告位置（等效F2） Shift + F3 在查找模式下，定位到上一个匹配处（参考F3） Shift + F6 重命名 Shift + F7 在 Debug 模式下，智能步入。断点所在行上有多个方法调用，会弹出进入哪个方法 Shift + F8 在 Debug 模式下，跳出当前方法 Shift + F9 等效于点击工具栏的 Debug 按钮 Shift + F10 等效于点击工具栏的 Run 按钮 Shift + ESC 隐藏最后一个打开的工具窗口 Shift + Tab 取消缩进 Shift + Enter 向下插入新行（参考Ctrl + Alt + Enter） Ctrl + Alt 键位 说明 Ctrl + Alt + C 提取作为常量 Ctrl + Alt + F 提取作为成员变量 Ctrl + Alt + H 调用层次（参考Ctrl + Shift + H） Ctrl + Alt + L 格式化代码 Ctrl + Alt + M 抽取选中代码变成方法 Ctrl + Alt + O 优化导入，就是将无效的import去除 Ctrl + Alt + P 提取作为所在方法的参数 Ctrl + Alt + S IDEA设置 Ctrl + Alt + T 对选中的代码进行环绕包裹，if,try等 Ctrl + Alt + V 快速引入变量，提取作为局部变量 Ctrl + Alt + F7 弹出选中元素在全项目中使用的地方，不显示结构（类似Alt + F7） Ctrl + Alt + F12 弹出当前文件的各级本地磁盘目录，可以快速打开 Ctrl + Alt + Enter 向上插入新行（参考Shift + Enter） Ctrl + Alt + Home 弹出跟当前文件有关联的文件弹出层 Ctrl + Alt + 空格 类名或接口名提示（包括jar包里的，还会自动导包，参考Ctrl + 空格，Ctrl + Shift + 空格） Ctrl + Alt + 左/右 退回到上一个浏览的位置（和Ctrl + Shift + Backspace不同） Ctrl + Shift 键位 说明 Ctrl + Shift + / 代码块注释 Ctrl + Shift + 加号 展开所有代码 Ctrl + Shift + 减号 折叠所有代码 Ctrl + Shift + B 跳转到当前元素类型的声明处 Ctrl + Shift + C 复制当前文件磁盘路径到剪贴板 Ctrl + Shift + F 在指定范围查找内容（搜全项目并不保险，有时候会搜不到，参考Ctrl + F） Ctrl + Shift + H 显示方法层次结构（参考Ctrl + Alt + H） Ctrl + Shift + I 快速查看当前元素的声明定义 Ctrl + Shift + N 打开指定文件 Ctrl + Shift + P 方法返回值类型提示（参考Ctrl + P） Ctrl + Shift + R 在指定范围查找内容（参考Ctrl + R） Ctrl + Shift + U 切换大小写 Ctrl + Shift + V 选择粘贴板缓存内容并粘贴 Ctrl + Shift + W 递进式取消选择代码块 Ctrl + Shift + Z 取消撤销 Ctrl + Shift + F7 选中文本，高亮显示所有相同内容（类似Alt + F3） Ctrl + Shift + 空格 智能代码提示（所期望类型的变量和方法列表供选择，参考Ctrl + 空格，Ctrl + Alt + 空格） Ctrl + Shift + Enter 自动结束代码（会在代码后面加上分号完成当前语句） Ctrl + Shift + Backspace 退回到上次修改的地方（和Ctrl + Alt + 左/右不同） Ctrl + Shift + 上/下 移动当前代码（以代码块为单位） Alt + Shift 键位 说明 Alt + Shift + 上/下 移动当前代码（以行为单位） Ctrl + Shift + Alt 键位 说明 Ctrl + Shift + Alt + V 无格式黏贴 Ctrl + Shift + Alt + N 全局查找变量或方法 Ctrl + Shift + Alt + S 项目结构设置 其它 键位 说明 F2 快速定位到下一个警告位置（等效Shift + F2） F3 在查找模式下，定位到下一个匹配处（参考Shift + F3） F4 编辑源（类似Ctrl + B） F7 在 Debug 模式下，进入下一步，如果当前行断点是一个方法，则进入当前方法体内，如果该方法体还有方法，则不会进入该内嵌的方法中（步入） F8 在 Debug 模式下，进入下一步，如果当前行断点是一个方法，则不进入当前方法体内（跳过） F9 在 Debug 模式下，恢复程序运行，但是如果该断点下面代码还有断点则停在下一个断点上 F11 添加书签 F12 回到前一个工具窗口 Tab 缩进 ESC 从工具窗口进入代码文件窗口 连按两次Shift 查找任何地方（搜全项目并不保险，有时候会搜不到） 二、按功能排序 三、设置Eclipse键位模式既然要学习IDEA，那么本人建议最好还是不要设置成Eclipse键位模式，因为这样你还是不熟悉IDEA。当然如果你是初学，但又急需要投入生产环境，那么可以先用Eclipse键位模式进行开发，大部分的快捷键和Eclipse是一样的。具体设置方法： 先按Ctrl + Alt + S，打开IDEA设置，在如图位置处设置快捷键风格]]></content>
      <categories>
        <category>工具</category>
      </categories>
      <tags>
        <tag>IDEA</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM内存泄漏及性能调优]]></title>
    <url>%2Fblog%2F2016%2F03%2F27%2F2016-03-27-JVM%E5%86%85%E5%AD%98%E6%B3%84%E6%BC%8F%E5%8F%8A%E6%80%A7%E8%83%BD%E8%B0%83%E4%BC%98%2F</url>
    <content type="text"><![CDATA[一、JVM内存模型及垃圾收集算法1.根据Java虚拟机规范，JVM将内存划分为： New（年轻代） Tenured（年老代） 永久代（Perm） 其中New和Tenured属于堆内存，堆内存会从JVM启动参数（-Xmx:3G）指定的内存中分配，Perm不属于堆内存，有虚拟机直接分配，但可以通过-XX:PermSize -XX:MaxPermSize 等参数调整其大小。 年轻代（New）：绝大多数最新被创建的对象会被分配到这里，由于大部分对象在创建后会很快变得不可到达，所以很多对象被创建在新生代，然后消失。对象从这个区域消失的过程我们称之为minor GC。新生代是用来保存那些第一次被创建的对象，他可以被分为三个空间: 一个伊甸园空间（Eden ） 两个幸存者空间（Survivor ） 绝大多数刚刚被创建的对象会存放在伊甸园空间（Eden）。在伊甸园空间执行了第一次GC之后，存活的对象被移动到其中一个幸存者空间。此后，在伊甸园空间执行GC之后，存活的对象会被堆积在同一个幸存者空间。当一个幸存者空间饱和，还在存活的对象会被移动到另一个幸存者空间。之后会清空已经饱和的那个幸存者空间。显然，Survivor只是增加了对象在年轻代中的逗留时间，增加了被垃圾回收的可能性。在以上的步骤中重复几次依然存活的对象，就会被移动到老年代。如果你仔细观察这些步骤就会发现，其中一个幸存者空间必须保持是空的。如果两个幸存者空间都有数据，或者两个空间都是空的，那一定标志着你的系统出现了某种错误。 年老代（Tenured)：对象没有变得不可达，并且从新生代中存活下来，会被拷贝到这里。其所占用的空间要比新生代多。也正由于其相对较大的空间，发生在老年代上的GC要比新生代少得多。对象从老年代中消失的过程，我们称之为major GC（或者full GC）。老年代空间的GC事件基本上是在空间已满时发生，执行的过程根据GC类型不同而不同，JDK7一共有5种GC类型： Serial GC Parallel GC Parallel Old GC (Parallel Compacting GC) Concurrent Mark &amp; Sweep GC (or “CMS”) Garbage First (G1) GC 永久代（Perm）：也被称为方法区（method area）。用来保存类常量以及字符串常量、Class、Method元信息，其大小跟项目的规模、类、方法的量有关，一般设置为128M就足够，设置原则是预留30%的空间。因此，这个区域不是用来永久的存储那些从老年代存活下来的对象。这个区域也可能发生GC。并且发生在这个区域上的GC事件也会被算为major GC。 2.垃圾回收算法垃圾回收算法可以分为三类，都基于标记-清除（复制）算法： Serial算法（单线程） 并行算法 并发算法 JVM会根据机器的硬件配置对每个内存代选择适合的回收算法，比如，如果机器多于1个核，会对年轻代选择并行算法，关于选择细节请参考JVM调优文档。 稍微解释下的是，并行算法是用多线程进行垃圾回收，回收期间会暂停程序的执行，而并发算法，也是多线程回收，但期间不停止应用执行。所以，并发算法适用于交互性高的一些程序。经过观察，并发算法会减少年轻代的大小，其实就是使用了一个大的年老代，这反过来跟并行算法相比吞吐量相对较低。 还有一个问题是，垃圾回收动作何时执行？ 当年轻代内存满时，会引发一次普通GC，该GC仅回收年轻代。需要强调的时，年轻代满是指Eden代满，Survivor满不会引发GC 当年老代满时会引发Full GC，Full GC将会同时回收年轻代、年老代 当永久代满时也会引发Full GC，会导致Class、Method元信息的卸载 另一个问题是，何时会抛出OutOfMemoryException，并不是内存被耗空的时候才抛出 JVM98%的时间都花费在内存回收 每次回收的内存小于2% 满足这两个条件将触发OutOfMemoryException，这将会留给系统一个微小的间隙以做一些Down之前的操作，比如手动打印Heap Dump。 二、内存泄漏及解决方法1.系统崩溃前的一些现象： 每次垃圾回收的时间越来越长，由之前的10ms延长到50ms左右，FullGC的时间也有之前的0.5s延长到4、5s FullGC的次数越来越多，最频繁时隔不到1分钟就进行一次FullGC 年老代的内存越来越大并且每次FullGC后年老代没有内存被释放 之后系统会无法响应新的请求，逐渐到达OutOfMemoryError的临界值。 2.生成堆的dump文件通过JMX的MBean生成当前的Heap信息，大小为一个3G（整个堆的大小）的hprof文件，如果没有启动JMX可以通过Java的jmap命令来生成该文件。 3.分析dump文件下面要考虑的是如何打开这个3G的堆信息文件，显然一般的Window系统没有这么大的内存，必须借助高配置的Linux。当然我们可以借助X-Window把Linux上的图形导入到Window。我们考虑用下面几种工具打开该文件： Visual VM IBM HeapAnalyzer JDK 自带的Hprof工具 使用这些工具时为了确保加载速度，建议设置最大内存为6G。使用后发现，这些工具都无法直观地观察到内存泄漏，Visual VM虽能观察到对象大小，但看不到调用堆栈；HeapAnalyzer虽然能看到调用堆栈，却无法正确打开一个3G的文件。因此，我们又选用了Eclipse专门的静态内存分析工具：Mat。 4.分析内存泄漏通过Mat我们能清楚地看到，哪些对象被怀疑为内存泄漏，哪些对象占的空间最大及对象的调用关系。针对本案，在ThreadLocal中有很多的JbpmContext实例，经过调查是JBPM的Context没有关闭所致。另外，通过Mat或JMX我们还可以分析线程状态，可以观察到线程被阻塞在哪个对象上，从而判断系统的瓶颈。 5.回归问题Q：为什么崩溃前垃圾回收的时间越来越长？A:根据内存模型和垃圾回收算法，垃圾回收分两部分：内存标记、清除（复制），标记部分只要内存大小固定时间是不变的，变的是复制部分，因为每次垃圾回收都有一些回收不掉的内存，所以增加了复制量，导致时间延长。所以，垃圾回收的时间也可以作为判断内存泄漏的依据 Q：为什么Full GC的次数越来越多？A：因此内存的积累，逐渐耗尽了年老代的内存，导致新对象分配没有更多的空间，从而导致频繁的垃圾回收 Q:为什么年老代占用的内存越来越大？A:因为年轻代的内存无法被回收，越来越多地被Copy到年老代 三、性能调优一切都是为了这一步，调优，在调优之前，我们需要记住下面的原则： 1、多数的Java应用不需要在服务器上进行GC优化；2、多数导致GC问题的Java应用，都不是因为我们参数设置错误，而是代码问题；3、在应用上线之前，先考虑将机器的JVM参数设置到最优（最适合）4、减少创建对象的数量；5、减少使用全局变量和大对象；6、GC优化是到最后不得已才采用的手段；7、在实际使用中，分析GC情况优化代码比优化GC参数要多得多； GC优化的目的有两个： 1、将转移到老年代的对象数量降低到最小；2、减少full GC的执行时间； 为了达到上面的目的，一般地，你需要做的事情有： 1、减少使用全局变量和大对象；2、调整新生代的大小到最合适；3、设置老年代的大小为最合适；4、选择合适的GC收集器。 除了上述内存泄漏外，我们还发现CPU长期不足3%，系统吞吐量不够，针对8core×16G、64bit的Linux服务器来说，是严重的资源浪费。 在CPU负载不足的同时，偶尔会有用户反映请求的时间过长，我们意识到必须对程序及JVM进行调优。从以下几个方面进行： 线程池：解决用户响应时间长的问题 连接池 JVM启动参数：调整各代的内存比例和垃圾回收算法，提高吞吐量 程序算法：改进程序逻辑算法提高性能（这里不做讲解） 1.Java线程池（java.util.concurrent.ThreadPoolExecutor）大多数JVM6上的应用采用的线程池都是JDK自带的线程池，之所以把成熟的Java线程池进行罗嗦说明，是因为该线程池的行为与我们想象的有点出入。Java线程池有几个重要的配置参数： corePoolSize：核心线程数（最新线程数） maximumPoolSize：最大线程数，超过这个数量的任务会被拒绝，用户可以通过RejectedExecutionHandler接口自定义处理方式 keepAliveTime：线程保持活动的时间 workQueue：工作队列，存放执行的任务 Java线程池需要传入一个Queue参数（workQueue）用来存放执行的任务，而对Queue的不同选择，线程池有完全不同的行为： SynchronousQueue：``一个无容量的等待队列，一个线程的insert操作必须等待另一线程的remove操作，采用这个Queue线程池将会为每个任务分配一个新线程 LinkedBlockingQueue ： 无界队列，采用该Queue，线程池将忽略 maximumPoolSize参数，仅用corePoolSize的线程处理所有的任务，未处理的任务便在LinkedBlockingQueue中排队 ArrayBlockingQueue： 有界队列，在有界队列和 maximumPoolSize的作用下，程序将很难被调优：更大的Queue和小的maximumPoolSize将导致CPU的低负载；小的Queue和大的池，Queue就没起动应有的作用。 其实我们的要求很简单，希望线程池能跟连接池一样，能设置最小线程数、最大线程数，当最小数&lt;任务&lt;最大数时，应该分配新的线程处理；当任务&gt;最大数时，应该等待有空闲线程再处理该任务。 但线程池的设计思路是，任务应该放到Queue中，当Queue放不下时再考虑用新线程处理，如果Queue满且无法派生新线程，就拒绝该任务。设计导致“先放等执行”、“放不下再执行”、“拒绝不等待”。所以，根据不同的Queue参数，要提高吞吐量不能一味地增大maximumPoolSize。 当然，要达到我们的目标，必须对线程池进行一定的封装，幸运的是ThreadPoolExecutor中留了足够的自定义接口以帮助我们达到目标。我们封装的方式是： 以SynchronousQueue作为参数，使maximumPoolSize发挥作用，以防止线程被无限制的分配，同时可以通过提高maximumPoolSize来提高系统吞吐量 自定义一个RejectedExecutionHandler，当线程数超过maximumPoolSize时进行处理，处理方式为隔一段时间检查线程池是否可以执行新Task，如果可以把拒绝的Task重新放入到线程池，检查的时间依赖keepAliveTime的大小。 2.连接池（org.apache.commons.dbcp.BasicDataSource）在使用org.apache.commons.dbcp.BasicDataSource的时候，因为之前采用了默认配置，所以当访问量大时，通过JMX观察到很多Tomcat线程都阻塞在BasicDataSource使用的Apache ObjectPool的锁上，直接原因当时是因为BasicDataSource连接池的最大连接数设置的太小，默认的BasicDataSource配置，仅使用8个最大连接。 我还观察到一个问题，当较长的时间不访问系统，比如2天，DB上的Mysql会断掉所以的连接，导致连接池中缓存的连接不能用。为了解决这些问题，我们充分研究了BasicDataSource，发现了一些优化的点： Mysql默认支持100个链接，所以每个连接池的配置要根据集群中的机器数进行，如有2台服务器，可每个设置为60 initialSize：参数是一直打开的连接数 minEvictableIdleTimeMillis：该参数设置每个连接的空闲时间，超过这个时间连接将被关闭 timeBetweenEvictionRunsMillis：后台线程的运行周期，用来检测过期连接 maxActive：最大能分配的连接数 maxIdle：最大空闲数，当连接使用完毕后发现连接数大于maxIdle，连接将被直接关闭。只有initialSize &lt; x &lt; maxIdle的连接将被定期检测是否超期。这个参数主要用来在峰值访问时提高吞吐量。 initialSize是如何保持的？经过研究代码发现，BasicDataSource会关闭所有超期的连接，然后再打开initialSize数量的连接，这个特性与minEvictableIdleTimeMillis、timeBetweenEvictionRunsMillis一起保证了所有超期的initialSize连接都会被重新连接，从而避免了Mysql长时间无动作会断掉连接的问题。 3.JVM参数在JVM启动参数中，可以设置跟内存、垃圾回收相关的一些参数设置，默认情况不做任何设置JVM会工作的很好，但对一些配置很好的Server和具体的应用必须仔细调优才能获得最佳性能。通过设置我们希望达到一些目标： GC的时间足够的小 GC的次数足够的少 发生Full GC的周期足够的长 前两个目前是相悖的，要想GC时间小必须要一个更小的堆，要保证GC次数足够少，必须保证一个更大的堆，我们只能取其平衡。 （1）针对JVM堆的设置一般，可以通过-Xms -Xmx限定其最小、最大值，为了防止垃圾收集器在最小、最大之间收缩堆而产生额外的时间，我们通常把最大、最小设置为相同的值 （2）年轻代和年老代将根据默认的比例（1：2）分配堆内存，可以通过调整二者之间的比率NewRadio来调整二者之间的大小，也可以针对回收代，比如年轻代，通过 -XX:newSize -XX:MaxNewSize来设置其绝对大小。同样，为了防止年轻代的堆收缩，我们通常会把-XX:newSize -XX:MaxNewSize设置为同样大小 （3）年轻代和年老代设置多大才算合理？这个我问题毫无疑问是没有答案的，否则也就不会有调优。我们观察一下二者大小变化有哪些影响： 更大的年轻代必然导致更小的年老代，大的年轻代会延长普通GC的周期，但会增加每次GC的时间；小的年老代会导致更频繁的Full GC 更小的年轻代必然导致更大年老代，小的年轻代会导致普通GC很频繁，但每次的GC时间会更短；大的年老代会减少Full GC的频率 如何选择应该依赖应用程序对象生命周期的分布情况：如果应用存在大量的临时对象，应该选择更大的年轻代；如果存在相对较多的持久对象，年老代应该适当增大。但很多应用都没有这样明显的特性，在抉择时应该根据以下两点：（A）本着Full GC尽量少的原则，让年老代尽量缓存常用对象，JVM的默认比例1：2也是这个道理 （B）通过观察应用一段时间，看其他在峰值时年老代会占多少内存，在不影响Full GC的前提下，根据实际情况加大年轻代，比如可以把比例控制在1：1。但应该给年老代至少预留1/3的增长空间 （4）在配置较好的机器上（比如多核、大内存），可以为年老代选择并行收集算法： -XX:+UseParallelOldGC ，默认为Serial收集 （5）线程堆栈的设置：每个线程默认会开启1M的堆栈，用于存放栈帧、调用参数、局部变量等，对大多数应用而言这个默认值太了，一般256K就足用。理论上，在内存不变的情况下，减少每个线程的堆栈，可以产生更多的线程，但这实际上还受限于操作系统。 （6）可以通过下面的参数打Heap Dump信息： -XX:HeapDumpPath -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/usr/aaa/dump/heap_trace.txt 通过下面参数可以控制OutOfMemoryError时打印堆的信息 -XX:+HeapDumpOnOutOfMemoryError 请看一下一个时间的Java参数配置：（服务器：Linux 64Bit，8Core×16G） JAVA_OPTS=”$JAVA_OPTS -server -Xms3G -Xmx3G -Xss256k -XX:PermSize=128m -XX:MaxPermSize=128m -XX:+UseParallelOldGC -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/usr/aaa/dump -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -Xloggc:/usr/aaa/dump/heap_trace.txt -XX:NewSize=1G -XX:MaxNewSize=1G” 经过观察该配置非常稳定，每次普通GC的时间在10ms左右，Full GC基本不发生，或隔很长很长的时间才发生一次 通过分析dump文件可以发现，每个1小时都会发生一次Full GC，经过多方求证，只要在JVM中开启了JMX服务，JMX将会1小时执行一次Full GC以清除引用，关于这点请参考官方文档]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM内存区域划分]]></title>
    <url>%2Fblog%2F2016%2F03%2F01%2F2016-03-01-JVM%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E5%88%92%E5%88%86%2F</url>
    <content type="text"><![CDATA[JVM内存由于Java程序是交由JVM执行的，所以我们在谈Java内存区域划分的时候事实上是指JVM内存区域划分。在讨论JVM内存区域划分之前，先来看一下Java程序具体执行的过程： 如上图所示，首先Java源代码文件(.java后缀)会被Java编译器编译为字节码文件(.class后缀)，然后由JVM中的类加载器加载各个类的字节码文件，加载完毕之后，交由JVM执行引擎执行。在整个程序执行过程中，JVM会用一段空间来存储程序执行期间需要用到的数据和相关信息，这段空间一般被称作为Runtime Data Area（运行时数据区），也就是我们常说的JVM内存。因此，在Java中我们常常说到的内存管理就是针对这段空间进行管理（如何分配和回收内存空间） Java虚拟机在执行Java程序时会将其管理的内存按照用于划分为若干个不同的数据区域，这些区域有着各自不同的生命周期。根据《JAVA虚拟机规范》，Java虚拟机管理的内存会包含以下几个区域。其中可以分为共享内存区以及线程隔离数据区两个部分。 在JVM规范中虽然规定了程序在执行期间运行时数据区应该包括这几部分，但是至于具体如何实现并没有做出规定，不同的虚拟机厂商可以有不同的实现方式。 JVM内存划分1.程序计数器也有称作为PC寄存器，用来指示执行哪条指令 当前线程所执行的字节码行号指示器 字节码解释器工作依赖计数器控制完成 通过执行线程行号记录，让线程轮流切换各条线程之间计数器互不影响 线程私有，生命周期与线程相同，随JVM启动而生，JVM关闭而死 线程执行Java方法时，记录其正在执行的虚拟机字节码指令地址 线程执行Nativan方法时，计数器记录为空（Undefined） 唯一在Java虚拟机规范中没有规定任何OutOfMemoryError情况区域 由于程序计数器中存储的数据所占空间的大小不会随程序的执行而发生改变，因此，对于程序计数器是不会发生内存溢出现象(OutOfMemory) 有两个线程，其中一个线程可以暂停使用，让其他线程运行，然后等自己获得cpu资源时，又能从暂停的地方开始运行，那么为什么能够记住暂停的位置的，这就依靠了程序计数器， 通过这个例子，大概了解一下程序计数器的功能。 2.虚拟机栈虚拟机栈（Java Vitual Machine Stack）也称作Java栈，也就是我们常常所说的栈。线程私有，它的生命周期和线程相同。虚拟机栈描述的是Java方法执行的内存模型：每个方法在执行的同时都会创建一个栈帧，用来存放存储： 局部变量表(Local Variables) 操作数栈(Operand Stack) 指向当前方法所属的类的运行时常量池的引用(Reference to runtime constant pool) 方法返回地址(Return Address) 一些额外的附加信息 当线程执行一个方法时，就会随之创建一个对应的栈帧，并将建立的栈帧压栈。当方法执行完毕之后，便会将栈帧出栈。 因此可知，线程当前执行的方法所对应的栈帧必定位于Java栈的顶部。对于所有的程序设计语言来说，栈这部分空间对程序员来说是不透明的。下图表示了一个Java栈的模型： 分别来作说明： 局部变量表，用来存储方法中的局部变量（包括在方法中声明的非静态变量以及函数形参）。对于基本数据类型的变量，则直接存储它的值，对于引用类型的变量，则存的是指向对象的引用。需要注意的是，局部变量表所需的内存空间在编译期内完成分配，当进入一个方法时，这个方法在栈帧中分配多大的内存空间是完全确定的，在程序执行期间局部变量表的大小是不会改变的。 操作数栈，栈最典型的一个应用就是用来对表达式求值。想想一个线程执行方法的过程中，实际上就是不断执行语句的过程，而归根到底就是进行计算的过程。因此可以这么说，程序中的所有计算过程都是在借助于操作数栈来完成的。 指向运行时常量池的引用（运行时常量池的概念在方法区部分会谈到），因为在方法执行的过程中有可能需要用到类中的常量，所以必须要有一个引用指向运行时常量。 方法返回地址，当一个方法执行完毕之后，要返回之前调用它的地方，因此在栈帧中必须保存一个方法返回地址。 由于每个线程正在执行的方法可能不同，因此每个线程都会有一个自己的Java栈，互不干扰。 -Xss参数设置栈容量 例：-Xss1m 虚拟机栈和本地方法栈的大小 = 线程允许最大内存 - 最大堆容量 - 最大方法区容量在多线程时，给每个线程分配的栈越大，越容易出现异常 3.本地方法栈本地方法栈与Java栈的作用和原理非常相似。区别只不过是Java栈是为执行Java方法服务的，而本地方法栈则是为执行本地方法（Native Method）服务的。在JVM规范中，并没有对本地方发展的具体实现方法以及数据结构作强制规定，虚拟机可以自由实现它。在HotSopt虚拟机中直接就把本地方法栈和Java栈合二为一。 以上说的JVM三个区域都是线程不共享的，也就是这部分内存，每个线程独有，不会让别的线程访问到，接下来的两个就是线程共享了，也就会出现线程安全问题。 4.堆Java中的堆是用来存储对象本身的以及数组（当然，数组引用是存放在Java栈中的）。同时堆也是垃圾收集器管理的主要区域。因此很多时候被称为”GC堆”。在Java中，程序员基本不用去关心空间释放的问题，Java的垃圾回收机制会自动进行处理。因此这部分空间也是Java垃圾收集器管理的主要区域。另外，堆是被所有线程共享的，在JVM中只有一个堆。 5.方法区方法区在JVM中也是一个非常重要的区域，它与堆一样，是被线程共享的区域。在方法区中，存储了每个类的信息（包括类的名称、方法信息、字段信息）、静态变量、常量以及编译器编译后的代码等。 在Class文件中除了类的字段、方法、接口等描述信息外，还有一项是常量池（Constant Pool Table）,用于存放编译期生成的各种字面量（比如int i=3，这个3就是字面量的意思）和符号引用，这部分内容将在类加载后进入方法区的运行时常量池中存放。 在方法区中有一个非常重要的部分就是运行时常量池(Runtime Constant Pool)，它是每一个类或接口的常量池的运行时表示形式，在类和接口被加载到JVM后，对应的运行时常量池就被创建出来。运行时常量池相对于Class文件常量池的一个重要特征是具备动态性：并非Class文件常量池中的内容才能进入运行时常量池，在运行期间也可将新的常量放入运行时常量池中，比如String.intern()方法，这个方法的作用就是：先从方法区的运行时常量池中查找看是否有该值，如果有，则返回该值的引用，如果没有，那么就会将该值加入运行时常量池中。 在JVM规范中，没有强制要求方法区必须实现垃圾回收。很多人习惯将方法区称为“永久代”，是因为HotSpot虚拟机以永久代来实现方法区，从而JVM的垃圾收集器可以像管理堆区一样管理这部分区域，从而不需要专门为这部分设计垃圾回收机制。 不过自从JDK7之后，Hotspot虚拟机便将运行时常量池从永久代移除了，在Java 堆中开辟了一块区域存放运行时常量池。在Java7之前，HotSpot虚拟机中将GC分代收集扩展到了方法区，使用永久代来实现了方法区。这个区域的内存回收目标主要是针对常量池的回收和对类型的卸载。而在Java8中，已经彻底没有了永久代，将方法区直接放在一个与堆不相连的本地内存区域，这个区域被叫做元空间。 常用JVM 参数选项： 参数 说明 -Xms 初始堆大小。如：-Xms256m -Xmx 最大堆大小。如：-Xmx512m 若-Xms=-Xmx,则可避免堆自动扩展 -Xmn 新生代大小。通常为 Xmx 的 1/3 或 1/4。新生代 = Eden + 2 个 Survivor 空间。实际可用空间为 = Eden + 1 个 Survivor，即 90% -Xss JDK1.5+ 每个线程堆栈大小为 1M，一般来说如果栈不是很深的话， 1M 是绝对够用了的。 -XX:NewRatio 新生代与老年代的比例，如 –XX:NewRatio=2，则新生代占整个堆空间的1/3，老年代占2/3 -XX:SurvivorRatio 新生代中 Eden 与 Survivor 的比值。默认值为 8。即 Eden 占新生代空间的 8/10，另外两个 Survivor 各占 1/10 -XX:PermSize 永久代(方法区)的初始大小 -XX:MaxPermSize 永久代(方法区)的最大值 -XX:+PrintGCDetails 打印 GC 信息 -XX:+HeapDumpOnOutOfMemoryError 让虚拟机在发生内存溢出时 Dump 出当前的内存堆转储快照，以便分析用]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java多线程相关问题汇总]]></title>
    <url>%2Fblog%2F2015%2F12%2F23%2F2015-12-23-Java%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%9B%B8%E5%85%B3%E9%97%AE%E9%A2%98%E6%B1%87%E6%80%BB%2F</url>
    <content type="text"><![CDATA[什么是线程？ 线程是操作系统能够进行运算调度的最小单位，它被包含在进程之中，是进程中的实际运作单位。程序员可以通过它进行多处理器编程，你可以使用多线程对运算密集型任务提速。比如，如果一个线程完成一个任务要100毫秒，那么用十个线程完成改任务只需10毫秒。Java在语言层面对多线程提供了卓越的支持，它也是一个很好的卖点。 多线程有什么用？ 发挥多核CPU的优势随着工业的进步，现在的笔记本、台式机乃至商用的应用服务器至少也都是双核的，4核、8核甚至16核的也都不少见，如果是单线程的程序，那么在双核CPU上就浪费了50%，在4核CPU上就浪费了75%。单核CPU上所谓的”多线程”那是假的多线程，同一时间处理器只会处理一段逻辑，只不过线程之间切换得比较快，看着像多个线程”同时”运行罢了。多核CPU上的多线程才是真正的多线程，它能让你的多段逻辑同时工作，多线程，可以真正发挥出多核CPU的优势来，达到充分利用CPU的目的。 防止阻塞从程序运行效率的角度来看，单核CPU不但不会发挥出多线程的优势，反而会因为在单核CPU上运行多线程导致线程上下文的切换，而降低程序整体的效率。但是单核CPU我们还是要应用多线程，就是为了防止阻塞。试想，如果单核CPU使用单线程，那么只要这个线程阻塞了，比方说远程读取某个数据吧，对端迟迟未返回又没有设置超时时间，那么你的整个程序在数据返回回来之前就停止运行了。多线程可以防止这个问题，多条线程同时运行，哪怕一条线程的代码执行读取数据阻塞，也不会影响其它任务的执行。 便于建模这是另外一个没有这么明显的优点了。假设有一个大的任务A，单线程编程，那么就要考虑很多，建立整个程序模型比较麻烦。但是如果把这个大的任务A分解成几个小任务，任务B、任务C、任务D，分别建立程序模型，并通过多线程分别运行这几个任务，那就简单很多了。 使用多线程的优势：更多的处理器核心；更快的响应时间；更好的编程模型。 线程和进程有什么区别？ 线程是进程的子集，一个进程可以有很多线程，每条线程并行执行不同的任务。不同的进程使用不同的内存空间，而所有的线程共享一片相同的内存空间。别把它和栈内存搞混，每个线程都拥有单独的栈内存用来存储本地数据。 如何在Java中实现多线程？ 在语言层面有两种方式。java.lang.Thread 类的实例就是一个线程，但是它需要调用java.lang.Runnable接口来执行，由于线程类本身就是调用的Runnable接口所以你可以继承java.lang.Thread 类，或者直接实现Runnable接口来重写run方法实现线程。至于哪个好，不用说肯定是后者好，因为实现接口的方式比继承类的方式更灵活，也能减少程序之间的耦合度，面向接口编程也是设计模式6大原则的核心。 用Runnable还是Thread？ 这个问题是上题的后续，大家都知道我们可以通过继承Thread类或者调用Runnable接口来实现线程，问题是，那个方法更好呢？什么情况下使用它？这个问题很容易回答，如果你知道Java不支持类的多重继承，但允许你调用多个接口。所以如果你要继承其他类，当然是调用Runnable接口好了。 Thread 类中的start和 run方法有什么区别？ start方法被用来启动新创建的线程，只有调用了start方法，才会表现出多线程的特性，而且start内部调用了run方法，不同线程的run方法里面的代码交替执行，这和直接调用run方法的效果不一样。 当你调用run方法的时候，只会是在原来的线程中调用，没有新的线程启动，start方法才会启动新线程。如果只是调用run方法，那么代码还是同步执行的，必须等待一个线程的run方法里面的代码全部执行完毕之后，另外一个线程才可以执行其run方法里面的代码。 Java中Runnable和Callable有什么不同？ Runnable和Callable都代表那些要在不同的线程中执行的任务。Runnable从JDK1.0开始就有了，Callable是在JDK1.5增加的。它们的主要区别是Callable的 call方法可以返回值（是一个泛型）和抛出异常，返回装载有计算结果的Future对象。和Future、FutureTask配合可以用来获取多线程执行的结果，可以在等待时间太长没获取到需要的数据的情况下取消该线程的任务，真的是非常有用。而Runnable接口中的run方法的返回值是void，它做的事情只是纯粹地去执行run方法中的代码而已。 Java中CyclicBarrier 和 CountDownLatch有什么不同？ CyclicBarrier 和 CountDownLatch 都可以用来让一组线程等待其它线程。 CyclicBarrier的某个线程运行到某个点上之后，该线程即停止运行，直到所有的线程都到达了这个点，所有线程才重新运行；CountDownLatch则不是，某线程运行到某个点上之后，只是给某个数值-1而已，该线程继续运行 CyclicBarrier只能唤起一个任务，CountDownLatch可以唤起多个任务CyclicBarrier可重用，CountDownLatch不可重用，计数值为0该CountDownLatch就不可再用了 Java中如何停止一个线程？ Java提供了很丰富的API但没有为停止线程提供API。JDK 1.0本来有一些像stop, suspend和 resume的控制方法但是由于潜在的死锁威胁因此在后续的JDK版本中他们被弃用了，之后Java API的设计者就没有提供一个兼容且线程安全的方法来停止一个线程。当run或者 call方法执行完的时候线程会自动结束,如果要手动结束一个线程，你可以用volatile 布尔变量来退出run方法的循环或者是取消任务来中断线程。 一个线程运行时发生异常会怎样？ 简单的说，如果异常没有被捕获，该线程将会停止执行。Thread.UncaughtExceptionHandler是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候JVM会使用Thread.getUncaughtExceptionHandler来查询线程的UncaughtExceptionHandler并将线程和异常作为参数传递给handler的uncaughtException方法进行处理。另外重要的一点是：如果这个线程持有某个某个对象的监视器，那么这个对象监视器会被立即释放。 如何在两个线程间共享数据？ 你可以通过共享对象来实现这个目的，然后通过wait/notify/notifyAll、await/signal/signalAll进行唤起和等待.或者是使用像阻塞队列这样并发的数据结构，比方说阻塞队列BlockingQueue就是为线程之间共享数据而设计的 什么是线程安全？Vector是一个线程安全类吗？ 如果你的代码所在的进程中有多个线程在同时运行，而这些线程可能会同时运行这段代码。如果每次运行结果和单线程运行的结果是一样的，而且其他的变量的值也和预期的是一样的，就是线程安全的。简单来说就是，如果你的代码在多线程下执行和在单线程下执行永远都能获得一样的结果，那么就是线程安全的。一个线程安全的计数器类的同一个实例对象在被多个线程使用的情况下也不会出现计算失误。很显然你可以将集合类分成两组，线程安全和非线程安全的。Vector 是用同步方法来实现线程安全的, 而和它相似的ArrayList不是线程安全的。 这个问题有值得一提的地方，就是线程安全也是有几个级别的： 不可变像String、Integer、Long这些，都是final类型的类，任何一个线程都改变不了它们的值，要改变除非新创建一个，因此这些不可变对象不需要任何同步手段就可以直接在多线程环境下使用 绝对线程安全不管运行时环境如何，调用者都不需要额外的同步措施。要做到这一点通常需要付出许多额外的代价，Java中标注自己是线程安全的类，实际上绝大多数都不是线程安全的，不过绝对线程安全的类，Java中也有，比方说CopyOnWriteArrayList、CopyOnWriteArraySet 相对线程安全相对线程安全也就是我们通常意义上所说的线程安全，像Vector这种，add、remove方法都是原子操作，不会被打断，但也仅限于此，如果有个线程在遍历某个Vector、有个线程同时在add这个Vector，99%的情况下都会出现ConcurrentModificationException，也就是fail-fast机制。 线程非安全这个就没什么好说的了，ArrayList、LinkedList、HashMap等都是线程非安全的类 如何强制启动一个线程？ 这个问题就像是如何强制进行Java垃圾回收，目前还没有觉得方法，虽然你可以使用System.gc来进行垃圾回收，但是不保证能成功。在Java里面没有办法强制启动一个线程，它是被线程调度器控制着且Java没有公布相关的API。 Java多线程中调用wait和 sleep方法有什么不同？ Java程序中wait和sleep都可以用来放弃CPU一定的时间，都会造成某种形式的暂停。wait方法用于线程间通信，如果等待条件为真且其它线程被唤醒时它会释放锁。而sleep方法仅仅释放CPU资源或者让当前线程停止执行一段时间，但不会释放锁。如果线程持有某个对象的监视器，sleep方法不会放弃这个对象的监视器，wait方法会放弃这个对象的监视器。 Thread.sleep(0)的作用是什么？ 这个问题和上面那个问题是相关的，我就连在一起了。由于Java采用抢占式的线程调度算法，因此可能会出现某条线程常常获取到CPU控制权的情况，为了让某些优先级比较低的线程也能获取到CPU控制权，可以使用Thread.sleep(0)手动触发一次操作系统分配时间片的操作，这也是平衡CPU控制权的一种操作。 怎么唤醒一个阻塞的线程？ 如果线程是因为调用了wait、sleep或者join方法而导致的阻塞，可以中断线程，并且通过抛出InterruptedException来唤醒它；如果线程遇到了IO阻塞，无能为力，因为IO是操作系统实现的，Java代码并没有办法直接接触到操作系统。 什么是阻塞式方法？ 阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket的accept方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。 什么是线程池？ 为什么要使用它？ 创建线程要花费昂贵的资源和时间，如果任务来了才创建线程那么响应时间会变长，而且一个进程能创建的线程数有限。为了避免这些问题，在程序启动的时候就创建若干线程来响应处理，它们被称为线程池，里面的线程叫工作线程。从JDK1.5开始，Java API提供了Executor框架让你可以创建不同的线程池。比如单线程池，每次处理一个任务；数目固定的线程池或者是缓存线程池（一个适合很多生存期短的任务的程序的可扩展线程池）。另外，使用线程池还可以根据项目灵活地控制并发的数目。 如果你提交任务时，线程池队列已满，这时会发生什么？ 这个问题问得很狡猾，许多程序员会认为该任务会阻塞直到线程池队列有空位。事实上如果一个任务不能被调度执行那么ThreadPoolExecutor’s submit方法将会抛出一个RejectedExecutionException异常。如果你使用的LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为LinkedBlockingQueue可以近乎认为是一个无穷大的队列，可以无限存放任务；如果你使用的是有界队列比方说ArrayBlockingQueue的话，任务首先会被添加到ArrayBlockingQueue中，ArrayBlockingQueue满了，则会使用拒绝策略RejectedExecutionHandler处理满了的任务，默认是AbortPolicy。 Java线程池中submit和 execute方法有什么区别？ 两个方法都可以向线程池提交任务，execute方法的返回类型是void，它定义在Executor接口中, 而submit方法可以返回持有计算结果的Future对象，它定义在ExecutorService接口中，它扩展了Executor接口，其它线程池类像ThreadPoolExecutor和ScheduledThreadPoolExecutor都有这些方法。 Java中用到的线程调度算法是什么？ 抢占式。一个线程用完CPU之后，操作系统会根据线程优先级、线程饥饿情况等数据算出一个总的优先级并分配下一个时间片给某个线程执行。 Java内存模型是什么？ Java内存模型规定和指引Java程序在不同的内存架构、CPU和操作系统间有确定性地行为。它定义了一种多线程访问Java内存的规范，在多线程的情况下尤其重要。Java内存模型对一个线程所做的变动能被其它线程可见提供了保证，它们之间是先行发生关系。这个关系定义了一些规则让程序员在并发编程时思路更清晰。我简单总结一下Java内存模型的几部分内容： Java内存模型将内存分为了主内存和工作内存。类的状态，也就是类之间共享的变量，是存储在主内存中的，每次Java线程用到这些主内存中的变量的时候，会读一次主内存中的变量，并让这些内存在自己的工作内存中有一份拷贝，运行自己线程代码的时候，用到这些变量，操作的都是自己工作内存中的那一份。在线程代码执行完毕之后，会将最新的值更新到主内存中去 定义了几个原子操作，用于操作主内存和工作内存中的变量 定义了volatile变量的使用规则 happens-before，即先行发生原则，定义了操作A必然先行发生于操作B的一些规则，比如在同一个线程内控制流前面的代码一定先行发生于控制流后面的代码、一个释放锁unlock的动作一定先行发生于后面对于同一个锁进行锁定lock的动作等等，只要符合这些规则，则不需要额外做同步措施，如果某段代码不符合所有的happens-before规则，则这段代码一定是线程非安全的 我强烈建议大家阅读《Java并发编程实践》第十六章来加深对Java内存模型的理解。 Java中的volatile 变量是什么？ volatile是一个特殊的修饰符，只有成员变量才能使用它。在Java并发程序缺少同步类的情况下，多线程对成员变量的操作对其它线程是透明的。volatile变量可以保证下一个读取操作会在前一个写操作之后发生，就是上一题的volatile变量规则。volatile关键字的作用主要有两个： 多线程主要围绕可见性和原子性两个特性而展开，使用volatile关键字修饰的变量，保证了其在多线程之间的可见性，即每次读取到volatile变量，一定是最新的数据代码底层执行不像我们看到的高级语言—Java程序这么简单，它的执行是Java代码–&gt;字节码–&gt;根据字节码执行对应的C/C++代码–&gt;C/C++代码被编译成汇编语言–&gt;和硬件电路交互，现实中，为了获取更好的性能JVM可能会对指令进行重排序，多线程下可能会出现一些意想不到的问题。使用volatile则会对禁止语义重排序，当然这也一定程度上降低了代码执行效率从实践角度而言，volatile的一个重要作用就是和CAS结合，保证了原子性，详细的可以参见java.util.concurrent.atomic包下的类，比如AtomicInteger。 volatile 变量和 atomic 变量有什么不同？ 这是个有趣的问题。首先，volatile 变量和 atomic 变量看起来很像，但功能却不一样。Volatile变量可以确保先行关系，即写操作会发生在后续的读操作之前, 但它并不能保证原子性。例如用volatile修饰count变量那么 count++ 操作就不是原子性的。而AtomicInteger类提供的atomic方法可以让这种操作具有原子性如getAndIncrement方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。 线程类的构造方法、静态块是被哪个线程调用的? 这是一个非常刁钻和狡猾的问题。请记住：线程类的构造方法、静态块是被new这个线程类所在的线程所调用的，而run方法里面的代码才是被线程自身所调用的。如果说上面的说法让你感到困惑，那么我举个例子，假设Thread2中new了Thread1，main函数中new了Thread2，那么： Thread2的构造方法、静态块是main线程调用的，Thread2的run方法是Thread2自己调用的 Thread1的构造方法、静态块是Thread2调用的，Thread1的run方法是Thread1自己调用的 如果同步块内的线程抛出异常会发生什么？ 这个问题坑了很多Java程序员，若你能想到锁是否释放这条线索来回答还有点希望答对。无论你的同步块是正常还是异常退出的，里面的线程都会释放锁，所以对比锁接口我更喜欢同步块，因为它不用我花费精力去释放锁，该功能可以在finally block里释放锁实现。 什么是乐观锁和悲观锁？ 乐观锁：就像它的名字一样，对于并发间操作产生的线程安全问题持乐观状态，乐观锁认为竞争不总是会发生，因此它不需要持有锁，将比较-替换这两个动作作为一个原子操作尝试去修改内存中的变量，如果失败则表示发生冲突，那么就应该有相应的重试逻辑。 悲观锁：还是像它的名字一样，对于并发间操作产生的线程安全问题持悲观状态，悲观锁认为竞争总是会发生，因此每次对某资源进行操作时，都会持有一个独占的锁，就像synchronized，不管三七二十一，直接上了锁就操作资源了。 什么是自旋？ 很多synchronized里面的代码只是一些很简单的代码，执行时间非常快，此时等待的线程都加锁可能是一种不太值得的操作，因为线程阻塞涉及到用户态和内核态切换的问题。既然synchronized里面的代码执行得非常快，不妨让等待锁的线程不要被阻塞，而是在synchronized的边界做忙循环，这就是自旋。如果做了多次忙循环发现还没有获得锁，再阻塞，这样可能是一种更好的策略。 什么是CAS？ CAS，全称为Compare and Swap，即比较-替换。假设有三个操作数：内存值V、旧的预期值A、要修改的值B，当且仅当预期值A和内存值V相同时，才会将内存值修改为B并返回true，否则什么都不做并返回false。当然CAS一定要volatile变量配合，这样才能保证每次拿到的变量是主内存中最新的那个值，否则旧的预期值A对某条线程来说，永远是一个不会变的值A，只要某次CAS操作失败，永远都不可能成功。 什么是AQS？ 简单说一下AQS，AQS全称为AbstractQueuedSychronizer，翻译过来应该是抽象队列同步器。如果说java.util.concurrent的基础是CAS的话，那么AQS就是整个Java并发包的核心了，ReentrantLock、CountDownLatch、Semaphore等等都用到了它。AQS实际上以双向队列的形式连接所有的Entry，比方说ReentrantLock，所有等待的线程都被放在一个Entry中并连成双向队列，前面一个线程使用ReentrantLock好了，则双向队列实际上的第一个Entry开始运行。AQS定义了对双向队列所有的操作，而只开放了tryLock和tryRelease方法给开发者使用，开发者可以根据自己的实现重写tryLock和tryRelease方法，以实现自己的并发功能。 什么是ThreadLocal变量？ ThreadLocal是Java里一种特殊的变量。每个线程都有一个ThreadLocal就是每个线程都拥有了自己独立的一个变量，竞争条件被彻底消除了。它是为创建代价高昂的对象获取线程安全的好方法，比如你可以用ThreadLocal让SimpleDateFormat变成线程安全的，因为那个类创建代价高昂且每次调用都需要创建不同的实例所以不值得在局部范围使用它，如果为每个线程提供一个自己独有的变量拷贝，将大大提高效率。首先，通过复用减少了代价高昂的对象的创建个数。其次，你在没有使用高代价的同步或者不变性的情况下获得了线程安全。线程局部变量的另一个不错的例子是ThreadLocalRandom类，它在多线程环境中减少了创建代价高昂的Random对象的个数。简单说ThreadLocal就是一种以空间换时间的做法，在每个Thread里面维护了一个以开地址法实现的ThreadLocal.ThreadLocalMap，把数据进行隔离，数据不共享，自然就没有线程安全方面的问题了。 什么是FutureTask？ 在Java并发程序中FutureTask表示一个可以取消的异步运算任务。FutureTask里面可以传入一个Callable的具体实现类，可以对这个异步运算的任务进行启动和取消运算、查询运算是否完成和取回运算结果等操作。只有当运算完成的时候结果才能取回，如果运算尚未完成get方法将会阻塞。一个FutureTask对象可以对调用了Callable和Runnable的对象进行包装，由于FutureTask也是实现了Runnable接口所以它可以提交给Executor线程池来执行。 不可变对象对多线程有什么帮助？ 不可变对象保证了对象的内存可见性，对不可变对象的读取不需要进行额外的同步手段，提升了代码执行效率。 Java中什么是竞态条件？ 举个例子说明。 竞态条件会导致程序在并发情况下出现一些bugs。多线程对一些资源的竞争的时候就会产生竞态条件，如果首先要执行的程序竞争失败排到后面执行了，那么整个程序就会出现一些不确定的bugs。这种bugs很难发现而且会重复出现，因为线程间的随机竞争。一个例子就是无序处理。 Java中notify 和 notifyAll有什么区别？ 这又是一个刁钻的问题，因为多线程可以等待单监控锁，Java API 的设计人员提供了一些方法当等待条件改变的时候通知它们，但是这些方法没有完全实现。notify方法不能唤醒某个具体的线程，所以只有一个线程在等待的时候它才有用武之地。而notifyAll唤醒所有线程并允许他们争夺锁确保了至少有一个线程能继续运行。 为什么wait, notify 和 notifyAll这些方法不在thread类里面？ 这是个设计相关的问题，它考察的是程序员对现有系统和一些普遍存在但看起来不合理的事物的看法。回答这些问题的时候，你要说明为什么把这些方法放在Object类里是有意义的，还有不把它放在Thread类里的原因。一个很明显的原因是JAVA提供的锁是对象级的而不是线程级的，每个对象都有锁，通过线程获得。如果线程需要等待某些锁那么调用对象中的wait方法就有意义了。如果wait方法定义在Thread类中，线程正在等待的是哪个锁就不明显了。简单的说，由于wait，notify和notifyAll都是锁级别的操作，所以把他们定义在Object类中因为锁属于对象。 wait, notify, notifyAll用法？ 只能在同步方法或者同步块中使用wait()方法。在执行wait()方法后，当前线程释放锁（这点与sleep和yield方法不同）。调用了wait函数的线程会一直等待，知道有其他线程调用了同一个对象的notify或者notifyAll方法才能被唤醒，需要注意的是：被唤醒并不代表立刻获得对象的锁，要等待执行notify()方法的线程执行完，即退出synchronized代码块后，当前线程才会释放锁，而呈wait状态的线程才可以获取该对象锁。如果调用wait()方法时没有持有适当的锁，则抛出IllegalMonitorStateException，它是RuntimeException的一个子类，因此，不需要try-catch语句进行捕获异常。notify方法只会（随机）唤醒一个正在等待的线程，而notifyAll方法会唤醒所有正在等待的线程。如果一个对象之前没有调用wait方法，那么调用notify方法是没有任何影响的。带参数的wait(long timeout)或者wait(long timeout, int nanos)方法的功能是等待某一时间内是否有线程对锁进行唤醒，如果超过这个时间则自动唤醒。注意，wait方法和notify/notifyAll方法要在同步块中调用，主要是因为Java API强制要求这样做，如果你不这么做，你的代码会抛出IllegalMonitorStateException异常。还有一个原因是为了避免wait和notify之间产生竞态条件。 wait方法和notify/notifyAll方法在放弃对象监视器时有什么区别？ wait方法立即释放对象监视器，notify/notifyAll方法则会等待线程剩余代码执行完毕才会放弃对象监视器。 多线程中的忙循环是什么? 忙循环就是程序员用循环让一个线程等待，不像传统方法wait, sleep或 yield它们都放弃了CPU控制，而忙循环不会放弃CPU，它就是在运行一个空循环。这么做的目的是为了保留CPU缓存，在多核系统中，一个等待线程醒来的时候可能会在另一个内核运行，这样会重建缓存。为了避免重建缓存和减少等待重建的时间就可以使用它了。 怎么检测一个线程是否持有对象监视器？ 我也是在网上看到一篇文章才知道有方法可以判断某个线程是否持有对象监视器：Thread类提供了一个holdsLock(Object obj.方法，当且仅当对象obj的监视器被某条线程持有的时候才会返回true，注意这是一个static方法，这意味着“某条线程”指的是当前线程。 Java中interrupted 和 isInterruptedd方法的区别？ interrupted和 isInterrupted的主要区别是前者会将中断状态清除而后者不会。Java多线程的中断机制是用内部标识来实现的，调用Thread.interrupt来中断一个线程就会设置中断标识为true。当中断线程调用静态方法Thread.interrupted来检查中断状态时，中断状态会被清零。而非静态方法isInterrupted用来查询其它线程的中断状态且不会改变中断状态标识。简单的说就是任何抛出InterruptedException异常的方法都会将中断状态清零。无论如何，一个线程的中断状态有有可能被其它线程调用中断来改变。 为什么你应该在循环中检查等待条件? 处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。因此，当一个等待线程醒来时，不能认为它原来的等待状态仍然是有效的，在notify方法调用之后和等待线程醒来之前这段时间它可能会改变。这就是在循环中使用wait方法效果更好的原因，你可以在Eclipse中创建模板调用wait和notify试一试。如果你想了解更多关于这个问题的内容，我推荐你阅读《Effective Java》这本书中的线程和同步章节。 Java中的同步集合与并发集合有什么区别？ 同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在Java1.5之前程序员们只有同步集合来用且在多线程并发的时候会导致争用，阻碍了系统的扩展性。Java5介绍了并发集合像ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分区等现代技术提高了可扩展性。 Java中ConcurrentHashMap的并发度是什么？ ConcurrentHashMap把实际map划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是ConcurrentHashMap类构造函数的一个可选参数segment的大小，默认值为16，这意味着最多同时可以有16条线程操作ConcurrentHashMap，这样在多线程情况下也能避免争用。这也是ConcurrentHashMap对Hashtable的最大优势，Hashtable不能同时有两条线程获取Hashtable中的数据。 Java中堆和栈有什么不同？ 为什么把这个问题归类在多线程和并发里？因为栈是一块和线程紧密相关的内存区域。每个线程都有自己的栈内存，用于存储本地变量，方法参数和栈调用，一个线程中存储的变量对其它线程是不可见的。而堆是所有线程共享的一片公用内存区域。对象都在堆里创建，为了提升效率线程会从堆中弄一个缓存到自己的栈，如果多个线程使用该变量就可能引发问题，这时volatile变量就可以发挥作用了，它要求线程从主存中读取变量的值。 生产者消费者模型的作用是什么？ 通过平衡生产者的生产能力和消费者的消费能力来提升整个系统的运行效率，这是生产者消费者模型最重要的作用解耦，这是生产者消费者模型附带的作用，解耦意味着生产者和消费者之间的联系少，联系越少越可以独自发展而不需要收到相互的制约 如何写代码来解决生产者消费者问题？ 在现实中你解决的许多线程问题都属于生产者消费者模型，就是一个线程生产任务供其它线程进行消费，你必须知道怎么进行线程间通信来解决这个问题。比较低级的办法是用wait和notify来解决这个问题，比较赞的办法是用Semaphore 或者 BlockingQueue来实现生产者消费者模型。 什么是死锁？ Java多线程中的死锁是指两个或两个以上的进程在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。这是一个严重的问题，因为死锁会让你的程序挂起无法完成任务，死锁的发生必须满足以下四个条件： 互斥条件：一个资源每次只能被一个进程使用。 请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。 不剥夺条件：进程已获得的资源，在末使用完之前，不能强行剥夺。 循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。 如何避免死锁？ 避免死锁最简单的方法就是阻止循环等待条件，将系统中所有的资源设置标志位、排序，规定所有的进程申请资源必须以一定的顺序（升序或降序）做操作来避免死锁 让程序每次至多只能获得一个锁。当然，在多线程环境下，这种情况通常并不现实 设计时考虑清楚锁的顺序，尽量减少嵌在的加锁交互数量 既然死锁的产生是两个线程无限等待对方持有的锁，那么只要等待时间有个上限不就好了。当然synchronized不具备这个功能，但是我们可以使用Lock类中的tryLock方法去尝试获取锁，这个方法可以指定一个超时时限，在等待超过该时限之后变回返回一个失败信息 Java中活锁和死锁有什么区别？ 这是上题的扩展，活锁和死锁类似，不同之处在于处于活锁的线程或进程的状态是不断改变的，活锁可以认为是一种特殊的饥饿。一个现实的活锁例子是两个人在狭小的走廊碰到，两个人都试着避让对方好让彼此通过，但是因为避让的方向都一样导致最后谁都不能通过走廊。简单的说就是，活锁和死锁的主要区别是前者进程的状态可以改变但是却不能继续执行。 怎么检测一个线程是否拥有锁？ 我一直不知道我们竟然可以检测一个线程是否拥有锁，直到我看到一本书上写到。在java.lang.Thread中有一个方法叫holdsLock()，它返回true如果当且仅当当前线程拥有某个具体对象的锁。 你如何在Java中获取线程堆栈？ 对于不同的操作系统，有多种方法来获得Java进程的线程堆栈。当你获取线程堆栈时，JVM会把所有线程的状态存到日志文件或者输出到控制台。在Windows你可以使用Ctrl + Break组合键来获取线程堆栈，Linux下用kill -3命令。你也可以用jstack这个工具来获取，它对线程id进行操作，你可以用jps这个工具找到id。 JVM中哪个参数是用来控制线程的栈堆栈小的？ 这个问题很简单， -Xss参数用来控制线程的堆栈大小。 Java中synchronized 和 ReentrantLock 有什么不同？ synchronized是和if、else、for、while一样的关键字，ReentrantLock是类，这是二者的本质区别。Java在过去很长一段时间只能通过synchronized关键字来实现互斥，它有一些缺点，比如你不能扩展锁之外的方法或者块边界，尝试获取锁时不能中途取消等。Java 5 通过Lock接口提供了更复杂的控制来解决这些问题。ReentrantLock 类实现了 Lock接口，提供了比synchronized更多更灵活的特性，可以被继承、可以有方法、可以有各种各样的类变量扩展性。 ReentrantLock比synchronized的扩展性体现在几点上： ReentrantLock可以对获取锁的等待时间进行设置，这样就避免了死锁 ReentrantLock可以获取各种锁的信息 ReentrantLock可以灵活地实现多路通知另外，二者的锁机制其实也是不一样的。ReentrantLock底层调用的是Unsafe的park方法加锁，synchronized操作的应该是对象头中mark word，这点我不能确定。从性能上来说ReentrantLock比synchronized略有胜出（JDK6起），在JDK5中是远远胜出，为嘛不放弃synchronized呢？ReentrantLock的危险性要比同步机制高，如果忘记在finnally块中调用unlock，那么虽然代码表面上能正常运行，但实际上已经埋下了一颗定时炸弹，并很可能伤及其他代码。仅当内置锁不能满足需求时，才可以考虑使用ReentrantLock. 有三个线程T1，T2，T3，怎么确保它们按顺序执行？ 在多线程中有多种方法让线程按特定顺序执行，你可以用线程类的join()方法在一个线程中启动另一个线程，另外一个线程完成该线程继续执行。为了确保三个线程的顺序你应该先启动最后一个(T3调用T2，T2调用T1.，这样T1就会先完成而T3最后完成。 Thread类中的yield方法有什么作用？ 可以暂停当前正在执行的线程对象，让其它有相同优先级的线程执行。它是一个静态方法而且只保证当前线程放弃CPU占用而不能保证使其它线程一定能占用CPU，执行yield()的线程有可能在进入到暂停状态后马上又被执行。yield()方法的作用是放弃当前的CPU资源，将它让给其他的任务去占用CPU执行时间。但放弃时间不确定，有可能刚刚放弃，马上又获得CPU时间片。这里需要注意的是yield()方法和sleep方法一样，线程并不会让出锁，和wait不同。 Java中Semaphore是什么？ Java中的Semaphore是一种新的同步类，它是一个计数信号。从概念上讲，从概念上讲，信号量维护了一个许可集合。如有必要，在许可可用前会阻塞每一个 acquire()，然后再获取该许可。每个 release()添加一个许可，从而可能释放一个正在阻塞的获取者。但是，不使用实际的许可对象，Semaphore只对可用许可的号码进行计数，并采取相应的行动。信号量常常用于多线程的代码中，比如数据库连接池。 Semaphore有什么作用？ Semaphore就是一个信号量，它的作用是限制某段代码块的并发数。Semaphore有一个构造函数，可以传入一个int型整数n，表示某段代码最多只有n个线程可以访问，如果超出了n，那么请等待，等到某个线程执行完毕这段代码块，下一个线程再进入。由此可以看出如果Semaphore构造函数中传入的int型整数n=1，相当于变成了一个synchronized了。 Swing是线程安全的吗？ 为什么？ 你可以很肯定的给出回答，Swing不是线程安全的，但是你应该解释这么回答的原因即便面试官没有问你为什么。当我们说swing不是线程安全的常常提到它的组件，这些组件不能在多线程中进行修改，所有对GUI组件的更新都要在AWT线程中完成，而Swing提供了同步和异步两种回调方法来进行更新。 Swing API中那些方法是线程安全的？ 这个问题又提到了swing和线程安全，虽然组件不是线程安全的但是有一些方法是可以被多线程安全调用的，比如repaint(), revalidate()。 JTextComponent的setText()方法和JTextArea的insert() 和 append() 方法也是线程安全的。 Java中invokeAndWait 和 invokeLater有什么区别？ 这两个方法是Swing API 提供给Java开发者用来从当前线程而不是事件派发线程更新GUI组件用的。InvokeAndWait()同步更新GUI组件，比如一个进度条，一旦进度更新了，进度条也要做出相应改变。如果进度被多个线程跟踪，那么就调用invokeAndWait()方法请求事件派发线程对组件进行相应更新。而invokeLater()方法是异步调用更新组件的。 如何在Java中创建Immutable对象？ 这个问题看起来和多线程没什么关系， 但不变性有助于简化已经很复杂的并发程序。Immutable对象可以在没有同步的情况下共享，降低了对该对象进行并发访问时的同步化开销。可是Java没有@Immutable这个注解符，要创建不可变类，要实现下面几个步骤：通过构造方法初始化所有成员、对变量不要提供setter方法、将所有的成员声明为私有的，这样就不允许直接访问这些成员、在getter方法中，不要直接返回对象本身，而是克隆对象，并返回对象的拷贝。 Java中的ReadWriteLock是什么？ ReadWriteLock是Java 5 中新增的一个接口，一般而言，读写锁是用来提升并发程序性能的锁分离技术的成果。一个ReadWriteLock维护一对关联的锁，一个用于只读操作一个用于写。在没有写线程的情况下一个读锁可能会同时被多个读线程持有。写锁是独占的，你可以使用JDK中的ReentrantReadWriteLock来实现这个规则，它最多支持65535个写锁和65535个读锁。ReentrantReadWriteLock是ReadWriteLock接口的一个具体实现，实现了读写的分离，读锁是共享的，写锁是独占的，读和读之间不会互斥，读和写、写和读、写和写之间才会互斥，提升了读写的性能。 单例模式的双检锁是什么？ 这个问题在Java面试中经常被问到，但是面试官对回答此问题的满意度仅为50%。一半的人写不出双检锁还有一半的人说不出它的隐患和Java1.5是如何对它修正的。它其实是一个用来创建线程安全的单例的老方法，当单例实例第一次被创建时它试图用单个锁进行性能优化，但是由于太过于复杂在JDK1.4中它是失败的，我个人也不喜欢它。无论如何，即便你也不喜欢它但是还是要了解一下，因为它经常被问到。可以参考我的这篇文章： 如何正确的写出单例模式 如何在Java中创建线程安全的Singleton？ 这是上面那个问题的后续，如果你不喜欢双检锁而面试官问了创建Singleton类的替代方法，你可以利用JVM的类加载和静态变量初始化特征来创建Singleton实例，或者是利用枚举类型来创建Singleton，我很喜欢用这种方法。 什么是多线程的上下文切换？ 多线程的上下文切换是指CPU控制权由一个已经正在运行的线程切换到另外一个就绪并等待获取CPU执行权的线程的过程。 写出3条你遵循的多线程最佳实践 这种问题我最喜欢了，我相信你在写并发代码来提升性能的时候也会遵循某些最佳实践。以下三条最佳实践我觉得大多数Java程序员都应该遵循： 给你的线程起个有意义的名字。这样可以方便找bug或追踪。OrderProcessor, QuoteProcessor or TradeProcessor 这种名字比 Thread-1. Thread-2 and Thread-3 好多了，给线程起一个和它要完成的任务相关的名字，所有的主要框架甚至JDK都遵循这个最佳实践。 避免锁定和缩小同步的范围 锁花费的代价高昂且上下文切换更耗费时间空间，试试最低限度的使用同步和锁，缩小临界区。因此相对于同步方法我更喜欢同步块，它给我拥有对锁的绝对控制权。 多用同步类少用wait 和 notify 首先，CountDownLatch, Semaphore, CyclicBarrier 和 Exchanger 这些同步类简化了编码操作，而用wait和notify很难实现对复杂控制流的控制。其次，这些类是由最好的企业编写和维护在后续的JDK中它们还会不断优化和完善，使用这些更高等级的同步工具你的程序可以不费吹灰之力获得优化。 多用并发集合少用同步集合 这是另外一个容易遵循且受益巨大的最佳实践，并发集合比同步集合的可扩展性更好，所以在并发编程时使用并发集合效果更好。如果下一次你需要用到map，你应该首先想到用ConcurrentHashMap。 Java中的fork join框架是什么？ fork join框架是JDK7中出现的一款高效的工具，Java开发人员可以通过它充分利用现代服务器上的多处理器。它是专门为了那些可以递归划分成许多子模块设计的，目的是将所有可用的处理能力用来提升程序的性能。fork join框架一个巨大的优势是它使用了工作窃取算法，可以完成更多任务的工作线程可以从其它线程中窃取任务来执行。 Java中如何获取到线程dump文件？ 死循环、死锁、阻塞、页面打开慢等问题，打线程dump是最好的解决问题的途径。所谓线程dump也就是线程堆栈，获取到线程堆栈有两步： 获取到线程的pid，可以通过使用jps命令，在Linux环境下还可以使用ps -ef | grep java 打印线程堆栈，可以通过使用jstack pid命令，在Linux环境下还可以使用kill -3 pid 另外提一点，Thread类提供了一个getStackTrace()方法也可以用于获取线程堆栈。这是一个实例方法，因此此方法是和具体线程实例绑定的，每次获取获取到的是具体某个线程当前运行的堆栈。 Linux环境下如何查找哪个线程使用CPU最长? 这是一个比较偏实践的问题，可以这么做： 获取项目的pid，jps或者ps -ef | grep java top -H -p pid，顺序不能改变 这样就可以打印出当前的项目，每条线程占用CPU时间的百分比。注意这里打出的是LWP，也就是操作系统原生线程的线程号。使用”top -H -p pid”+”jps pid”可以很容易地找到某条占用CPU高的线程的线程堆栈，从而定位占用CPU高的原因，一般是因为不当的代码操作导致了死循环。最后提一点，”top -H -p pid”打出来的LWP是十进制的，”jps pid”打出来的本地线程号是十六进制的，转换一下，就能定位到占用CPU高的线程的当前线程堆栈了。 Hashtable的size()方法中明明只有一条语句”return count”，为什么还要做同步？ 同一时间只能有一条线程执行固定类的同步方法，但是对于类的非同步方法，可以多条线程同时访问。所以，这样就有问题了，可能线程A在执行Hashtable的put方法添加数据，线程B则可以正常调用size()方法读取Hashtable中当前元素的个数，那读取到的值可能不是最新的，可能线程A添加了完了数据，但是没有对size++，线程B就已经读取size了，那么对于线程B来说读取到的size一定是不准确的。而给size()方法加了同步之后，意味着线程B调用size()方法只有在线程A调用put方法完毕之后才可以调用，这样就保证了线程安全性 CPU执行代码，执行的不是Java代码，这点很关键，一定得记住。Java代码最终是被翻译成汇编代码执行的，汇编代码才是真正可以和硬件电路交互的代码。即使你看到Java代码只有一行，甚至你看到Java代码编译之后生成的字节码也只有一行，也不意味着对于底层来说这句语句的操作只有一个。一句”return count”假设被翻译成了三句汇编语句执行，完全可能执行完第一句，线程就切换了。 同步方法和同步块，哪个是更好的选择? 同步块，这意味着同步块之外的代码是异步执行的，这比同步整个方法更提升代码的效率。请知道一条原则：同步的范围越小越好。借着这一条，我额外提一点，虽说同步的范围越少越好，但是在Java虚拟机中还是存在着一种叫做锁粗化的优化方法，这种方法就是把同步范围变大。这是有用的，比方说StringBuffer，它是一个线程安全的类，自然最常用的append()方法是一个同步方法，我们写代码的时候会反复append字符串，这意味着要进行反复的加锁-&gt;解锁，这对性能不利，因为这意味着Java虚拟机在这条线程上要反复地在内核态和用户态之间进行切换，因此Java虚拟机会将多次append方法调用的代码进行一个锁粗化的操作，将多次的append的操作扩展到append方法的头尾，变成一个大的同步块，这样就减少了加锁–&gt;解锁的次数，有效地提升了代码执行的效率。 终止正在运行的线程的三种方法 使用退出标志，是线程正常退出，也就是当run方法完成后线程终止； 使用stop方法强行终止线程，但是不推荐使用这个方法，因为stop和suspend及resume一样都是作废过期的方法，使用它们可能产生不可预料的结果； 使用interrupt方法中断线程；（推荐） 线程的优先级 Java中线程的优先级分为1-10这10个等级，如果小于1或大于10则JDK抛出IllegalArgumentException()的异常，默认优先级是5。在Java中线程的优先级具有继承性，比如A线程启动B线程，则B线程的优先级与A是一样的。注意程序正确性不能依赖线程的优先级高低，因为操作系统可以完全不理会Java线程对于优先级的决定。 守护线程 Java中有两种线程，一种是用户线程，另一种是守护线程。当进程中不存在非守护线程了，则守护线程自动销毁。通过setDaemon(true)设置线程为后台线程。注意thread.setDaemon(true)必须在thread.start()之前设置，否则会报IllegalThreadStateException异常；在Daemon线程中产生的新线程也是Daemon的；在使用ExecutorSerice等多线程框架时，会把守护线程转换为用户线程，并且也会把优先级设置为Thread.NORM_PRIORITY。在构建Daemon线程时，不能依靠finally块中的内容来确保执行关闭或清理资源的逻辑。 synchronized的类锁与对象锁 类锁：在方法上加上static synchronized的锁，或者synchronized(xxx.class)的锁。如下代码中的method1和method2： 对象锁：参考method4, method5,method6. 12345678910111213141516171819&gt; public class LockStrategy &#123;&gt; &gt; public Object object1 = new Object();&gt; &gt; public static synchronized void method1()&#123;&#125;&gt; public void method2() &#123;&gt; synchronized(LockStrategy.class)&#123;&#125;&gt; &#125;&gt; &gt; public synchronized void method4()&#123;&#125;;&gt; public void method5() &#123;&gt; synchronized(this)&#123;&#125;&gt; &#125;&gt; public void method6() &#123;&gt; synchronized(object1)&#123;&#125;&gt; &#125;&gt;&gt; &#125;&gt; 注意方法method4和method5中的同步块也是互斥的。 同步不具备继承性 当一个线程执行的代码出现异常时，其所持有的锁会自动释放。同步不具有继承性（声明为synchronized的父类方法A，在子类中重写之后并不具备synchronized的特性）。 不同线程间的通信问题 在Java中提供了各种各样的输入/输出流Stream，使我们能够很方便地对数据进行操作，其中管道流（pipeStream)是一种特殊的流，用于在不同线程间直接传送数据。一个线程发送数据到输出管道，另一个线程从输入管道中读数据，通过使用管道，实现不同线程间的通信，而无须借助类似临时文件之类的东西。在JDK中使用4个类来使线程间可以进行通信：PipedInputStream, PipedOutputStream, PipedReader, PipedWriter。使用代码类似inputStream.connect(outputStream)或outputStream.connect(inputStream)使两个Stream之间产生通信连接。 几种进程间的通信方式 管道( pipe )：管道是一种半双工的通信方式，数据只能单向流动，而且只能在具有亲缘关系的进程间使用。进程的亲缘关系通常是指父子进程关系。 有名管道 (named pipe) ： 有名管道也是半双工的通信方式，但是它允许无亲缘关系进程间的通信。 信号量( semophore ) ： 信号量是一个计数器，可以用来控制多个进程对共享资源的访问。它常作为一种锁机制，防止某进程正在访问共享资源时，其他进程也访问该资源。因此，主要作为进程间以及同一进程内不同线程之间的同步手段。 消息队列( message queue ) ： 消息队列是由消息的链表，存放在内核中并由消息队列标识符标识。消息队列克服了信号传递信息少、管道只能承载无格式字节流以及缓冲区大小受限等缺点。 信号 ( sinal ) ： 信号是一种比较复杂的通信方式，用于通知接收进程某个事件已经发生。 共享内存( shared memory ) ：共享内存就是映射一段能被其他进程所访问的内存，这段共享内存由一个进程创建，但多个进程都可以访问。共享内存是最快的 IPC 方式，它是针对其他进程间通信方式运行效率低而专门设计的。它往往与其他通信机制，如信号两，配合使用，来实现进程间的同步和通信。 套接字( socket ) ： 套解口也是一种进程间通信机制，与其他通信机制不同的是，它可用于不同及其间的进程通信。 join方法 如果一个线程A执行了thread.join()语句，其含义是：当前线程A等待thread线程终止之后才从thread.join()返回。join与synchronized的区别是：join在内部使用wait()方法进行等待，而synchronized关键字使用的是“对象监视器”做为同步。join提供了另外两种实现方法：join(long millis)和join(long millis, int nanos)，至多等待多长时间而退出等待(释放锁)，退出等待之后还可以继续运行。内部是通过wait方法来实现的。 ThreadLocal ThreadLocal可以实现每个线程绑定自己的值，即每个线程有各自独立的副本而互相不受影响。一共有四个方法：get, set, remove, initialValue。可以重写initialValue()方法来为ThreadLocal赋初值。ThreadLocal建议设置为static类型的。使用类InheritableThreadLocal可以在子线程中取得父线程继承下来的值。可以采用重写childValue（Object parentValue）方法来更改继承的值。注意：在线程池的情况下，在ThreadLocal业务周期处理完成时，最好显式的调用remove()方法，清空”线程局部变量”中的值。正常情况下使用ThreadLocal不会造成内存溢出，弱引用的只是threadLocal，保存的值依然是强引用的，如果threadLocal依然被其他对象强引用，”线程局部变量”是无法回收的。 ReentrantLock ReentrantLock提供了tryLock方法，tryLock调用的时候，如果锁被其他线程持有，那么tryLock会立即返回，返回结果为false；如果锁没有被其他线程持有，那么当前调用线程会持有锁，并且tryLock返回的结果为true。可以在构造ReentranLock时使用公平锁，公平锁是指多个线程在等待同一个锁时，必须按照申请锁的先后顺序来一次获得锁。synchronized中的锁时非公平的，默认情况下ReentrantLock也是非公平的，但是可以在构造函数中指定使用公平锁。对于ReentrantLock来说，还有一个十分实用的特性，它可以同时绑定多个Condition条件，以实现更精细化的同步控制。ReentrantLock使用方式如下： 1234567&gt; Lock lock = new ReentrantLock();&gt; lock.lock();&gt; try &#123;&gt; &#125; finally &#123;&gt; lock.unlock();&gt; &#125;&gt; ReentrantLock中的其余方法 int getHoldCount()：查询当前线程保持此锁定的个数，也就是调用lock()方法的次数。 int getQueueLength()：返回正等待获取此锁定的线程估计数。比如有5个线程，1个线程首先执行await()方法，那么在调用getQueueLength方法后返回值是4，说明有4个线程在等待lock的释放。 int getWaitQueueLength(Condition condition)：返回等待此锁定相关的给定条件Condition的线程估计数。比如有5个线程，每个线程都执行了同一个condition对象的await方法，则调用getWaitQueueLength(Condition condition)方法时返回的int值是5。 boolean hasQueuedThread(Thread thread)：查询指定线程是否正在等待获取此锁定。 boolean hasQueuedThreads()：查询是否有线程正在等待获取此锁定。 boolean hasWaiters(Condition condition)：查询是否有线程正在等待与此锁定有关的condition条件。 boolean isFair()：判断是不是公平锁。 boolean isHeldByCurrentThread()：查询当前线程是否保持此锁定。 boolean isLocked()：查询此锁定是否由任意线程保持。 void lockInterruptibly()：如果当前线程未被中断，则获取锁定，如果已经被中断则出现异常。 Condition 一个Condition和一个Lock关联在一起，就想一个条件队列和一个内置锁相关联一样。要创建一个Condition，可以在相关联的Lock上调用Lock.newCondition方法。正如Lock比内置加锁提供了更为丰富的功能，Condition同样比内置条件队列提供了更丰富的功能：在每个锁上可存在多个等待、条件等待可以是可中断的或者不可中断的、基于时限的等待，以及公平的或非公平的队列操作。与内置条件队列不同的是，对于每个Lock，可以有任意数量的Condition对象。Condition对象继承了相关的Lock对象的公平性，对于公平的锁，线程会依照FIFO顺序从Condition.await中释放。注意：在Condition对象中，与wait,notify和notifyAll方法对于的分别是await,signal,signalAll。但是，Condition对Object进行了扩展，因而它也包含wait和notify方法。一定要确保使用的版本——await和signal. 读写锁ReentrantReadWriteLock 读写锁表示也有两个锁，一个是读操作相关的锁，也称为共享锁；另一个是写操作相关的锁，也叫排它锁。也就是多个读锁之间不互斥，读锁与写锁互斥，写锁与写锁互斥。在没有Thread进行写操作时，进行读取操作的多个Thread都可以获取读锁，而进行写入操作的Thread只有在获取写锁后才能进行写入操作。即读写锁在同一时刻可以允许多个读线程访问，但是在写线程访问时，所有的读线程和其他写线程均被阻塞。(lock.readlock.lock(), lock.readlock.unlock, lock.writelock.lock, lock.writelock.unlock)。读写锁维护了一对锁，一个读锁和一个写锁，通过分离读锁和写锁，使得并发性相比一般的排它锁有了很大的提升。Java中使用ReentrantReadWriteLock实现读写锁。 线程安全的单例模式 建议不要采用DCL的写法，建议使用下面这种静态内部类写法： 12345678910&gt; public class Singleton &#123; &gt; private static class SingletonHolder &#123; &gt; private static final Singleton INSTANCE = new Singleton(); &gt; &#125; &gt; private Singleton ()&#123;&#125; &gt; public static final Singleton getInstance() &#123; &gt; return SingletonHolder.INSTANCE; &gt; &#125; &gt; &#125;&gt; 或者这种枚举式： 1234&gt; public enum EasySingleton&#123;&gt; INSTANCE;&gt; &#125;&gt; 线程组ThreadGroup 为了有效地对一些线程进行组织管理，通常的情况下事创建一个线程组，然后再将部分线程归属到该组中，这样可以对零散的线程对象进行有效的组织和规划。 ReentrantLock与synchonized区别 ReentrantLock可以中断地获取锁（void lockInterruptibly() throws InterruptedException） ReentrantLock可以尝试非阻塞地获取锁（boolean tryLock()） ReentrantLock可以超时获取锁。通过tryLock(timeout, unit)，可以尝试获得锁，并且指定等待的时间。 ReentrantLock可以实现公平锁。通过new ReentrantLock(true)实现。 ReentrantLock对象可以同时绑定多个Condition对象，而在synchronized中，锁对象的的wait(), notify(), notifyAll()方法可以实现一个隐含条件，如果要和多于一个的条件关联的对象，就不得不额外地添加一个锁，而ReentrantLock则无需这样做，只需要多次调用newCondition()方法即可。 锁降级 锁降级是指写锁降级成读锁。如果当前线程拥有写锁，然后将其释放，最后获取读锁，这种分段完成的过程不能称之为锁降级。锁降级是指把持住（当前拥有的）写锁，再获取到读锁，最后释放（先前拥有的）写锁的过程。锁降级中的读锁是否有必要呢？答案是必要。主要是为了保证数据的可见性，如果当前线程不获取读锁而是直接释放写锁，假设此刻另一个线程（T）获取了写锁并修改了数据，那么当前线程无法感知线程T的数据更新。如果当前线程获取读锁，即遵循锁降级的步骤，则线程T将会被阻塞，直到当前线程使用数据并释放读锁之后，线程T才能获取写锁进行数据更新。 Java里的阻塞队列 ArrayBlockingQueue:一个由数组结构组成的有界阻塞队列。 LinkedeBlockingQueue:一个有链表结构组成的有界阻塞队列。 PriorityBlockingQueue:一个支持优先级排序的无界阻塞队列 DelayQueue:一个使用优先级队列实现的无界阻塞队列。 SynchronousQueue:一个不存储元素的阻塞队列。 LinkedTransferQueue:一个由链表结构组成的无界阻塞队列。 LinkedBlockingDeque:一个由链表结构组成的双向阻塞队列。 高并发、任务执行时间短的业务怎样使用线程池？并发不高、任务执行时间长的业务怎样使用线程池？并发高、业务执行时间长的业务怎样使用线程池？ 这是我在并发编程网上看到的一个问题，把这个问题放在最后一个，希望每个人都能看到并且思考一下，因为这个问题非常好、非常实际、非常专业。关于这个问题，个人看法是： 高并发、任务执行时间短的业务，线程池线程数可以设置为CPU核数+1，减少线程上下文的切换 并发不高、任务执行时间长的业务要区分开看： 假如是业务时间长集中在IO操作上，也就是IO密集型的任务，因为IO操作并不占用CPU，所以不要让所有的CPU闲下来，可以加大线程池中的线程数目，让CPU处理更多的业务 假如是业务时间长集中在计算操作上，也就是计算密集型任务，这个就没办法了，和（1）一样吧，线程池中的线程数设置得少一些，减少线程上下文的切换 并发高、业务执行时间长，解决这种类型任务的关键不在于线程池而在于整体架构的设计，看看这些业务里面某些数据是否能做缓存是第一步，增加服务器是第二步，至于线程池的设置，设置参考第二条。最后，业务执行时间长的问题，也可能需要分析一下，看看能不能使用中间件对任务进行拆分和解耦。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>多线程</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[九种内部排序算法的Java实现及其性能测试]]></title>
    <url>%2Fblog%2F2015%2F12%2F03%2F2015-12-03-%E4%B9%9D%E7%A7%8D%E5%86%85%E9%83%A8%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95%E7%9A%84Java%E5%AE%9E%E7%8E%B0%E5%8F%8A%E5%85%B6%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95%2F</url>
    <content type="text"><![CDATA[九种内部排序算法的Java实现及其性能测试9种内部排序算法性能比较第九种为java.util.Arrays.sort（改进的快速排序方法） 100000的随机数据集 200000的随机数据集 500000的随机数据集 结论：归并排序和堆排序维持O(nlgn)的复杂度，速率差不多，表现优异。固定基准的快排表现很是优秀。而通过使用一个循环完成按增量分组后的直接插入的希尔排序，测试效果显著。冒泡，选择，直接插入都很慢，而冒泡效率是最低。 1.插入排序[稳定]适用于小数组,数组已排好序或接近于排好序速度将会非常快 复杂度：O(n^2) - O(n) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 1234567891011121314151617public void insertionSort(int[] a) &#123; if (null == a || a.length &lt; 2) &#123; return; &#125; for (int i = 1; i &lt; a.length; i++) &#123; // 暂存当前值 int temp = a[i]; int j = i - 1; while (j &gt;= 0 &amp;&amp; temp &lt; a[j]) &#123; // 后移 a[j + 1] = a[j]; j--; &#125; // 当前值归位 a[j + 1] = temp; &#125;&#125; 2.希尔排序(缩小增量排序)[不稳定]复杂度 平均 O(n^1.3) 最好O(n) 最差O(n^s)[1&lt;s&lt;2] 空间O(1) 内循环通过模拟并行的方式完成分组的内部直接插入排序，而不是一个一个分组分组的排，在10w的随机数据20w的随机数据均表现优异。 123456789101112131415161718public void shellSort(int[] a) &#123; if (null == a || a.length &lt; 2) &#123; return; &#125; for (int d = a.length / 2; d &gt; 0; d /= 2) &#123; // 从1B开始先和1A比较 然后2A与2B...然后再1C向前与同组的比较 for (int i = d; i &lt; a.length; i++) &#123; // 内部直接插入 int temp = a[i]; int j = i - d; while (j &gt;= 0 &amp;&amp; temp &lt; a[j]) &#123; a[j + d] = a[j]; j -= d; &#125; a[j + d] = temp; &#125; &#125;&#125; 3.冒泡排序[稳定]复杂度：O(n^2) - O(n) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 1234567891011121314151617181920public void bubbleSort(int[] a) &#123; if (null == a || a.length &lt; 2) &#123; return; &#125; boolean flag; for (int i = 0; i &lt; a.length - 1; i++) &#123; flag = false; for (int j = 0; j &lt; a.length - 1 - i; j++) &#123; if (a[j] &gt; a[j + 1]) &#123; int temp = a[j]; a[j] = a[j + 1]; a[j + 1] = temp; flag = true; &#125; &#125; if (false == flag) &#123; return; &#125; &#125;&#125; 4.选择排序[不稳定]原理：每次从无序序列选取最小的 复杂度：O(n^2) - O(n^2) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 123456789101112131415161718public void selectSort(int[] a) &#123; if (null == a || a.length &lt; 2) &#123; return; &#125; for (int i = 0; i &lt; a.length; i++) &#123; int k = i; for (int j = i + 1; j &lt; a.length; j++) &#123; if (a[j] &lt; a[k]) &#123; k = j; &#125; &#125; if (k != i) &#123; int temp = a[k]; a[k] = a[i]; a[i] = temp; &#125; &#125;&#125; 5.归并排序[稳定]原理：采用分治法 复杂度：O(nlogn) - O(nlgn) - O(nlgn) - O(n)[平均 - 最好 - 最坏 - 空间复杂度] 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748/** 排序 */public void mergeSort(int[] a, int low, int high) &#123; int mid = (low + high) / 2; if (low &lt; high) &#123; // 左边排序 mergeSort(a, low, mid); // 右边排序 mergeSort(a, mid + 1, high); // 有序序列合并 merge(a, low, mid, high); &#125;&#125;/** 合并 */private void merge(int a[], int low, int mid, int high) &#123; // 临时数组 int[] temp = new int[high - low + 1]; // 左指针 int i = low; // 右指针 int j = mid + 1; // 临时数组索引 int k = 0; while (i &lt;= mid &amp;&amp; j &lt;= high) &#123; if (a[i] &lt; a[j]) &#123; temp[k++] = a[i++]; &#125; else &#123; temp[k++] = a[j++]; &#125; &#125; // 把左边剩余的数移入数组 while (i &lt;= mid) &#123; temp[k++] = a[i++]; &#125; // 把右边剩余的数移入数组 while (j &lt;= high) &#123; temp[k++] = a[j++]; &#125; // 注意这里是low + t for (int t = 0; t &lt; temp.length; t++) &#123; a[low + t] = temp[t]; &#125;&#125; 6.快速排序[不稳定]原理：分治+递归 复杂度：O(nlgn) - O(nlgn) - O(n^2) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 栈空间0(lgn) - O(n) 1234567891011121314151617181920212223242526272829/** 固定基准 */public void quickSort(int[] a, int low, int high) &#123; if (null == a || a.length &lt; 2) &#123; return; &#125; if (low &lt; high) &#123; int mid = partition(a, low, high); quickSort(a, low, mid - 1); quickSort(a, mid + 1, high); &#125;&#125;private int partition(int[] a, int low, int high) &#123; int pivot = a[low]; while (low &lt; high) &#123; // 注意等于，否则死循环 while (low &lt; high &amp;&amp; a[high] &gt;= pivot) &#123; high--; &#125; a[low] = a[high]; // 注意等于，否则死循环 while (low &lt; high &amp;&amp; a[low] &lt;= pivot) &#123; low++; &#125; a[high] = a[low]; &#125; a[low] = pivot; return low;&#125; 7.堆排序[不稳定]堆一般指二叉堆。 复杂度：O(nlogn) - O(nlgn) - O(nlgn) - O(1)[平均 - 最好 - 最坏 - 空间复杂度] 大顶堆实现从小到大的升序排列，小顶堆一般用于构造优先队列 123456789101112131415161718192021222324252627282930313233343536373839404142434445public void heapSort(int[] a) &#123; if (null == a || a.length &lt; 2) &#123; return; &#125; buildMaxHeap(a); for (int i = a.length - 1; i &gt;= 0; i--) &#123; int temp = a[0]; a[0] = a[i]; a[i] = temp; adjustHeap(a, i, 0); &#125;&#125;/** 建堆 */private void buildMaxHeap(int[] a) &#123; int mid = a.length / 2; for (int i = mid; i &gt;= 0; i--) &#123; adjustHeap(a, a.length, i); &#125;&#125;/** 递归调整堆 */private void adjustHeap(int[] a, int size, int parent) &#123; int left = 2 * parent + 1; int right = 2 * parent + 2; int largest = parent; if (left &lt; size &amp;&amp; a[left] &gt; a[parent]) &#123; largest = left; &#125; if (right &lt; size &amp;&amp; a[right] &gt; a[largest]) &#123; largest = right; &#125; if (parent != largest) &#123; int temp = a[parent]; a[parent] = a[largest]; a[largest] = temp; adjustHeap(a, size, largest); &#125;&#125; 8.基数排序[稳定]原理：分配加收集 复杂度： O(d(n+r)) r为基数d为位数 空间复杂度O(n+r) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** 基数排序 */public void radixSort(int[] a, int begin, int end, int digit) &#123; // 基数 final int radix = 10; // 桶中的数据统计 int[] count = new int[radix]; int[] bucket = new int[end - begin + 1]; // 按照从低位到高位的顺序执行排序过程 for (int i = 1; i &lt;= digit; i++) &#123; // 清空桶中的数据统计 for (int j = 0; j &lt; radix; j++) &#123; count[j] = 0; &#125; // 统计各个桶将要装入的数据个数 for (int j = begin; j &lt;= end; j++) &#123; int index = getDigit(a[j], i); count[index]++; &#125; // count[i]表示第i个桶的右边界索引 for (int j = 1; j &lt; radix; j++) &#123; count[j] = count[j] + count[j - 1]; &#125; // 将数据依次装入桶中 // 这里要从右向左扫描，保证排序稳定性 for (int j = end; j &gt;= begin; j--) &#123; int index = getDigit(a[j], i); bucket[count[index] - 1] = a[j]; count[index]--; &#125; // 取出，此时已是对应当前位数有序的表 for (int j = 0; j &lt; bucket.length; j++) &#123; a[j] = bucket[j]; &#125; &#125;&#125;/** 获取x的第d位的数字，其中最低位d=1 */private int getDigit(int x, int d) &#123; String div = "1"; while (d &gt;= 2) &#123; div += "0"; d--; &#125; return x / Integer.parseInt(div) % 10;&#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[静态代码块和构造代码块和构造方法执行顺序]]></title>
    <url>%2Fblog%2F2015%2F11%2F12%2F2015-11-12-%E9%9D%99%E6%80%81%E4%BB%A3%E7%A0%81%E5%9D%97%E5%92%8C%E6%9E%84%E9%80%A0%E4%BB%A3%E7%A0%81%E5%9D%97%E5%92%8C%E6%9E%84%E9%80%A0%E6%96%B9%E6%B3%95%E6%89%A7%E8%A1%8C%E9%A1%BA%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[代码块在Java中，使用{}括起来的代码被称为代码块。 分类根据其位置和声明的不同，可以分为 局部代码块：局部位置,用于限定变量的生命周期。 构造代码块：在类中的成员位置,用{}括起来的代码。每次调用构造方法执行前，都会先执行构造代码块。作用：可以把多个构造方法中的共同代码放到一起，对对象进行初始化。 静态代码块：在类中的成员位置,用{}括起来的代码,只不过它用static修饰了。作用：一般是对类进行初始化。 代码执行顺序单个类中: 静态代码块(静态成员变量) -&gt; main方法 -&gt; 构造代码块(成员变量) -&gt; 构造方法静态代码块：只执行一次构造代码块：在每一次创建对象时执行 涉及父类和子类的初始化过程：a.初始化父类中的静态成员变量和静态代码块(按次序)b.初始化子类中的静态成员变量和静态代码块(按次序)c.初始化父类的普通成员变量和构造代码块(按次序)，再执行父类的构造方法(注意父类构造方法中的子类方法覆盖)d.初始化子类的普通成员变量和构造代码块(按次序)，再执行子类的构造方法 看程序写结果12345678910111213141516171819202122232425262728293031323334class Test &#123; static &#123; System.out.println("Test 静态代码块"); &#125; &#123; System.out.println("Test 构造代码块"); &#125; public Test() &#123; System.out.println("Test 构造方法"); &#125; &#125;public class TestDemo &#123; static &#123; System.out.println("TestDemo 静态代码块"); &#125; &#123; System.out.println("TestDemo 构造代码块"); &#125; public static void main(String[] args) &#123; System.out.println("TestDemo main方法"); Test t1 = new Test(); Test t2 = new Test(); &#125; &#125; 其运行结果是： TestDemo 静态代码块TestDemo main方法Test 静态代码块Test 构造代码块Test 构造方法Test 构造代码块Test 构造方法 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class Test extends MyClass&#123; &#123; System.out.println("Test 构造代码块"); &#125; Person person = new Person("Test"); static &#123; System.out.println("Test 静态代码块"); &#125; public Test() &#123; System.out.println("Test 构造方法"); &#125; public static void main(String[] args) &#123; new Test(); &#125;&#125;class Person &#123; static &#123; System.out.println("Person 静态代码块"); &#125; &#123; System.out.println("Person 构造代码块"); &#125; public Person(String str) &#123; System.out.println("Person 构造方法，参数是" + str); &#125; &#125;class MyClass &#123; static Person person = new Person("MyClass"); static &#123; System.out.println("MyClass 静态代码块"); &#125; &#123; System.out.println("MyClass 构造代码块"); &#125; public MyClass() &#123; System.out.println("MyClass 构造方法"); &#125; &#125; 其运行结果是： Person 静态代码块Person 构造代码块Person 构造方法，参数是MyClassMyClass 静态代码块Test 静态代码块MyClass 构造代码块MyClass 构造方法Test 构造代码块Person 构造代码块Person 构造方法，参数是TestTest 构造方法]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>执行顺序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git快速入门和常用命令]]></title>
    <url>%2Fblog%2F2015%2F10%2F03%2F2015-10-03-Git%E5%BF%AB%E9%80%9F%E5%85%A5%E9%97%A8%E5%92%8C%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%2F</url>
    <content type="text"><![CDATA[简易图解git流程概念 Workspace：工作区 Index / Stage：暂存区 Repository：本地仓库区（用HEAD指向最后一次commit的结果） Remote：远程仓库 一、快速入门本地初始化一个项目首先，你需要执行下面两条命令，作为 git 的基础配置，作用是告诉 git 你是谁，你输入的信息将出现在你创建的提交中。 12git config --global user.name "你的名字或昵称"git config --global user.email "你的邮箱" 然后在你的需要初始化版本库的文件夹中执行： 12git init git remote add origin 你的项目地址 # 项目地址形式为:http://git.oschina.net/xxx/xxx.git或者 git@git.oschina.net:xxx/xxx.git 这样就完成了一次版本你的初始化。 如果你想克隆一个项目，只需要执行： 1git clone 项目地址 完成第一次提交进入你已经初始化好的或者克隆项目的目录，然后执行： 1234git pull origin master # 从远程仓库获取最新版本并合并，因为你提交前可能已经有别人提交过了，你需要合并最新版本才能提交到远程仓库git add .git commit -m "这是我第一次提交的说明"git push origin master 然后如果需要账号密码的话就输入账号密码，这样就完成了一次提交。 二、基本命令配置123456789# 显示当前的Git配置git config --list# 编辑Git配置文件git config -e [--global]# 设置提交代码时的用户信息git config --global user.name "[name]"git config --global user.email "[email address]" 获取与创建本地项目12git init # 在当前目录新建一个Git代码库git init 目录 # 新建一个目录，将其初始化为Git代码库 1git clone 项目地址URL # 拷贝一个Git仓库到本地 默认情况下，Git 会按照你提供的 URL 所指示的项目的名称创建你的本地项目目录。 通常就是该 URL 最后一个 / 之后的项目名称。如果你想要一个不一样的名字， 你可以在该命令后加上你想要的名称。URL也可以是你的本地仓库地址，这样就是创建了一个本地仓库的克隆版本。 增加/删除文件到Index你可以计划改动（把它们添加到缓存区），使用如下命令：git add [filename]git add . 这是 git 基本工作流程的第一步； 123456789101112131415161718192021222324252627# 添加指定文件到暂存区git add [file1] [file2] ...# 添加指定目录到暂存区，包括子目录git add [dir]# 添加当前目录的所有文件到暂存区git add .# 添加每个变化前，都会要求确认# 对于同一个文件的多处变化，可以实现分次提交git add -p# 删除工作区文件，并且将这次删除放入暂存区git rm [file1] [file2] ...# 把文件从暂存区域移除，但保留在当前工作目录中git rm --cached [file]# 如果删除之前修改过并且已经放到暂存区域的话，则必须要用强制删除选项 -fgit rm -f [file]# 递归删除整个目录中的所有子目录和文件git rm –r * # 改名文件，并且将这个改名放入暂存区git mv [file-original] [file-renamed] 代码提交使用如下命令以实际提交改动：git commit -m “代码提交信息”现在，你的改动已经提交到了 HEAD，但是还没到你的远端仓库。 123456789101112131415161718# 提交暂存区到仓库区，并填写备注信息git commit -m [message]# 提交暂存区的指定文件到仓库区git commit [file1] [file2] ... -m [message]# 提交工作区自上次commit之后的变化，直接到仓库区git commit -a# 提交时显示所有diff信息git commit -v# 使用一次新的commit，替代上一次提交# 如果代码没有任何新变化，则用来改写上一次commit的提交信息git commit --amend -m [message]# 重做上一次commit，并包括指定文件的新变化git commit --amend [file1] [file2] ... 同步远程仓库你的改动现在已经在本地仓库的 HEAD 中了。执行如下命令以将这些改动提交到远端仓库：git push origin master可以把 master 换成你想要推送的任何分支。 如果你还没有克隆现有仓库，并欲将你的仓库连接到某个远程服务器，你可以使用如下命令添加：git remote add origin [server]如此你就能够将你的改动推送到所添加的服务器上去了。 1234567891011121314151617181920212223# 下载远程仓库的所有变动git fetch [remote]# 查看当前配置有哪些远程仓库git remote -v# 显示某个远程仓库的信息git remote show [remote]# 增加一个新的远程仓库，并命名git remote add [shortname] [url]# 取回远程仓库的变化，并与本地分支合并git pull [remote] [branch]# 上传本地指定分支到远程仓库git push [remote] [branch]# 强行推送当前分支到远程仓库，即使有冲突git push [remote] --force# 推送所有分支到远程仓库git push [remote] --all 要更新你的本地仓库至最新改动，执行：git pull以在你的工作目录中 获取（fetch） 并 合并（merge） 远端的改动。要合并其他分支到你的当前分支（例如 master），执行：git merge [branch]两种情况下，git 都会尝试去自动合并改动。不幸的是，自动合并并非次次都能成功，并可能导致 冲突（conflicts）。 这时候就需要你修改这些文件来人肉合并这些 冲突（conflicts） 了。改完之后，你需要执行如下命令以将它们标记为合并成功：git add [filename]在合并改动之前，也可以使用如下命令查看：git diff {source_branch} {target_branch} 三、查看信息1234567891011121314151617181920212223242526272829303132333435# 列出当前目录所有还没有被git管理的文件和被git管理且被修改但还未提交(git commit)的文件,-s参数可简化显示git status# 显示所有提交过的用户，按提交次数排序git shortlog -sn# 显示指定文件是什么人在什么时间修改过git blame [file]# 显示暂存区和工作区的差异git diff# 显示暂存区和上一个commit的差异git diff --cached [file]# 显示工作区与当前分支最新commit之间的差异git diff HEAD# 显示两次提交之间的差异git diff [first-branch]...[second-branch]# 显示今天你写了多少行代码git diff --shortstat "@&#123;0 day ago&#125;"# 显示某次提交的元数据和内容变化git show [commit]# 显示某次提交发生变化的文件git show --name-only [commit]# 显示某次提交时，某个文件的内容git show [commit]:[filename]# 显示当前分支的最近几次提交git reflog 四、撤销假如你做错事，你可以使用如下命令替换掉本地改动：git checkout –[filename]此命令会使用 HEAD 中的最新内容替换掉你的工作目录中的文件。已添加到缓存区的改动，以及新文件，都不受影响。 假如你想要丢弃你所有的本地改动与提交，可以到服务器上获取最新的版本并将你本地主分支指向到它：git fetch origingit reset –hard origin/master 12345678910111213141516171819202122232425262728293031# 恢复暂存区的指定文件到工作区git checkout [file]# 恢复某个commit的指定文件到暂存区和工作区git checkout [commit] [file]# 恢复暂存区的所有文件到工作区git checkout .# 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变git reset [file]# 重置暂存区与工作区，与上一次commit保持一致git reset --hard# 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变git reset [commit]# 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致git reset --hard [commit]# 重置当前HEAD为指定commit，但保持暂存区和工作区不变git reset --keep [commit]# 新建一个commit，用来撤销指定commit# 后者的所有变化都将被前者抵消，并且应用到当前分支git revert [commit]# 暂时将未提交的变化移除，稍后再移入git stashgit stash pop 五、分支管理分支是用来将特性开发绝缘开来的。在你创建仓库的时候，master 是“默认的”。在其他分支上进行开发，完成后再将它们合并到主分支上。创建一个叫做“feature_x”的分支，并切换过去：git checkout -b feature_x切换回主分支：git checkout master再把新建的分支删掉：git branch -d feature_x除非你将分支推送到远端仓库，不然该分支就是 不为他人所见的：git push origin [branch] 123456789101112131415161718192021222324252627282930313233343536373839404142# 列出所有本地分支git branch# 列出所有远程分支git branch -r# 列出所有本地分支和远程分支git branch -a# 新建一个分支，但依然停留在当前分支git branch [branch-name]# 新建一个分支，并切换到该分支git checkout -b [branch]# 新建一个分支，指向指定commitgit branch [branch] [commit]# 新建一个分支，与指定的远程分支建立追踪关系git branch --track [branch] [remote-branch]# 切换到指定分支，并更新工作区git checkout [branch-name]# 切换到上一个分支git checkout -# 建立追踪关系，在现有分支与指定的远程分支之间git branch --set-upstream [branch] [remote-branch]# 合并指定分支到当前分支git merge [branch]# 选择一个commit，合并进当前分支git cherry-pick [commit]# 删除分支git branch -d [branch-name]# 删除远程分支git push origin --delete [branch-name]git branch -dr [remote/branch] 六、历史记录123456789101112131415161718192021222324252627282930313233343536373839404142# 列出历史提交记录git log# 显示commit历史，以及每次commit发生变更的文件git log --stat# 查看历史记录的简洁的版本git log --oneline # 查看历史中什么时候出现了分支、合并git log --oneline --graph # 逆向显示所有日志git log --reverse --oneline # 查看历史中什么时候出现了分支、合并git log --oneline --graph# 显示指定文件相关的每一次diffgit log -p [file]# 搜索提交历史，根据关键词git log -S [keyword]# 显示某个commit之后的所有变动，每个commit占据一行git log [tag] HEAD --pretty=format:%s# 显示某个commit之后的所有变动，其"提交说明"必须符合搜索条件git log [tag] HEAD --grep feature# 显示某个文件的版本历史，包括文件改名git log --follow [file]git whatchanged [file]# 显示过去5次提交git log -5 --pretty --oneline# 查找指定用户的提交日志，看5条git log --author=用户名 --oneline -5 # 查看3周前且在10月01日之后的所有提交，--no-merges 选项以隐藏合并提交git log --oneline --before=&#123;3.weeks.ago&#125; --after=&#123;2015-10-01&#125; --no-merges 七、标签在软件发布时创建标签，是被推荐的。这是个旧有概念，在 SVN 中也有。可以执行如下命令以创建一个叫做 1.0.0 的标签：git tag 1.0.0 1b2e1d63ff1b2e1d63ff 是你想要标记的提交 ID 的前 10 位字符。使用如下命令获取提交 ID：git log你也可以用该提交 ID 的少一些的前几位，只要它是唯一的。 123456789101112131415161718192021222324252627# 列出所有taggit tag# 新建一个tag在当前commit# -a 选项意为"创建一个带注解的标签"。 不用 -a 选项也可以执行的，但它不会记录这标签是啥时候打的，谁打的，也不会让你添加个标签的注解。 我推荐一直创建带注解的标签git tag -a [tag]# 新建一个tag在指定commitgit tag [tag] [commit]# 删除本地taggit tag -d [tag]# 删除远程taggit push origin :refs/tags/[tagName]# 查看tag信息git show [tag]# 提交指定taggit push [remote] [tag]# 提交所有taggit push [remote] --tags# 新建一个分支，指向某个taggit checkout -b [branch] [tag] Git有commit，为什么还要引入tag？“请把上周一的那个版本打包发布，commit号是6a5819e…”“一串乱七八糟的数字不好找！”如果换一个办法：“请把上周一的那个版本打包发布，版本号是v1.2”“好的，按照tag v1.2查找commit就行！”所以，tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。 八、http(s)方式如何自动记住密码https 方式每次都要输入密码，按照如下设置即可输入一次就不用再手输入密码的困扰而且又享受 https 带来的极速 按照以下设置记住密码十五分钟： 1git config --global credential.helper cache 如果你想自定义记住的时间，可以这样： 1git config credential.helper 'cache --timeout=3600' # 这里记住的是一个小时，如需其他时间，请修改3600为你想修改的时间，单位是秒 你也可以设置长期记住密码： 1git config --global credential.helper store 或修改仓库的地址带上你的账号密码 1http://yourname:password@git.oschina.net/name/project.git # 注意，在码云上使用邮箱时，请对@符号使用%40替换 如果你原本使用的 ssh 地址想更换成 http(s) 地址，可以执行以下命令: 1234# 删除原本的ssh仓库地址git remote rm origin # origin 代表你原本ssh地址的仓库的别名# 新增http地址的仓库git remote add origin http://git.oschina.net/username/project.git 九、版本回退回退远程仓库的版本先在本地切换到远程仓库要回退的分支对应的本地分支，然后本地回退至你需要的版本，然后执行： 1git push &lt;仓库名&gt; &lt;分支名&gt; -f 以当前版本为基础，回退指定个commit首先，确认你当前的版本需要回退多少个版本，然后计算出你要回退的版本数量，执行如下命令 1git reset HEAD~X # X代表你要回退的版本数量，是数字！！！！ 需要注意的是，如果你是合并过分支，那么被合并分支带过来的 commit 并不会被计入回退数量中，而是只计算一个，所以如果需要一次回退多个 commit，不建议使用这种方法 回退到和远程版本一样有时候，当发生错误修改需要放弃全部修改时，可以以远程分支作为回退点退回到与远程分支一样的地方，执行的命令如下 1git reset --hard origin/master # origin代表你远程仓库的名字，master代表分支名]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>命令</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git分支的学习笔记整理]]></title>
    <url>%2Fblog%2F2015%2F09%2F21%2F2015-09-21-Git%E5%88%86%E6%94%AF%E7%9A%84%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E6%95%B4%E7%90%86%2F</url>
    <content type="text"><![CDATA[Git 分支几乎每一种版本控制系统都以某种形式支持分支。使用分支意味着你可以从开发主线上分离开来，然后在不影响主线的同时继续工作。在很多版本控制系统中，这是个昂贵的过程，常常需要创建一个源代码目录的完整副本，对大型项目来说会花费很长时间。有人把 Git 的分支模型称为“必杀技特性”，而正是因为它，将 Git 从版本控制系统家族里区分出来。Git 有何特别之处呢？Git 的分支可谓是难以置信的轻量级，它的新建操作几乎可以在瞬间完成，并且在不同分支间切换起来也差不多一样快。和许多其他版本控制系统不同，Git 鼓励在工作流程中频繁使用分支与合并，哪怕一天之内进行许多次都没有关系。理解分支的概念并熟练运用后，你才会意识到为什么 Git 是一个如此强大而独特的工具，并从此真正改变你的开发方式。 何谓分支Git 保存的不是文件差异或者变化量，而只是一系列文件快照。 在 Git 中提交时，会保存一个提交（commit）对象，该对象包含一个指向暂存内容快照的指针，包含本次提交的作者等相关附属信息，包含零个或多个指向该提交对象的父对象指针：首次提交是没有直接祖先的，普通提交有一个祖先，由两个或多个分支合并产生的提交则有多个祖先。 为直观起见，我们假设在工作目录中有三个文件，准备将它们暂存后提交。暂存操作会对每一个文件计算校验和（SHA-1 哈希字串），然后把当前版本的文件快照保存到 Git 仓库中（Git 使用 blob 类型的对象存储这些快照），并将校验和加入暂存区域： 12$ git add README test.rb LICENSE$ git commit -m 'initial commit of my project' 当使用 git commit 新建一个提交对象前，Git 会先计算每一个子目录（本例中就是项目根目录）的校验和，然后在 Git 仓库中将这些目录保存为树（tree）对象。之后 Git 创建的提交对象，除了包含相关提交信息以外，还包含着指向这个树对象（项目根目录）的指针，如此它就可以在将来需要的时候，重现此次快照的内容了。 现在，Git 仓库中有五个对象：三个表示文件快照内容的 blob 对象；一个记录着目录树内容及其中各个文件对应 blob 对象索引的 tree 对象；以及一个包含指向 tree 对象（根目录）的索引和其他提交信息元数据的 commit 对象。概念上来说，仓库中的各个对象保存的数据和相互关系看起来如图 1 所示： 作些修改后再次提交，那么这次的提交对象会包含一个指向上次提交对象的指针（即下图中的 parent 对象）。两次提交后，仓库历史会变成图 2 的样子： 现在来谈分支。Git 中的分支，其实本质上仅仅是个指向 commit 对象的可变指针。Git 会使用 master 作为分支的默认名字。在若干次提交后，你其实已经有了一个指向最后一次提交对象的 master 分支，它在每次提交的时候都会自动向前移动。 那么，Git 又是如何创建一个新的分支的呢？答案很简单，创建一个新的分支指针。比如新建一个 testing 分支，可以使用 git branch 命令： 1$ git branch testing 这会在当前 commit 对象上新建一个分支指针（见图 4）。 那么，Git 是如何知道你当前在哪个分支上工作的呢？其实答案也很简单，它保存着一个名为 HEAD 的特别指针。请注意它和你熟知的许多其他版本控制系统（比如 Subversion 或 CVS）里的 HEAD 概念大不相同。在 Git 中，它是一个指向你正在工作中的本地分支的指针（将 HEAD 想象为当前分支的别名）。运行 git branch 命令，仅仅是建立了一个新的分支，但不会自动切换到这个分支中去，所以在这个例子中，我们依然还在 master 分支里工作（参考图 5）。 要切换到其他分支，可以执行 git checkout 命令。我们现在转换到新建的 testing 分支： 1$ git checkout testing 这样 HEAD 就指向了 testing 分支（见图 6）。 这样的实现方式会给我们带来什么好处呢？好吧，现在不妨再提交一次： 12$ vim test.rb$ git commit -a -m 'made a change' 图 7 展示了提交后的结果。 非常有趣，现在 testing 分支向前移动了一格，而 master 分支仍然指向原先 git checkout 时所在的 commit 对象。现在我们回到 master 分支看看： 1$ git checkout master 图 8 显示了结果。 这条命令做了两件事。它把 HEAD 指针移回到 master 分支，并把工作目录中的文件换成了 master 分支所指向的快照内容。也就是说，现在开始所做的改动，将始于本项目中一个较老的版本。它的主要作用是将 testing 分支里作出的修改暂时取消，这样你就可以向另一个方向进行开发。 我们作些修改后再次提交： 12$ vim test.rb$ git commit -a -m 'made other changes' 现在我们的项目提交历史产生了分叉（如图 9 所示），因为刚才我们创建了一个分支，转换到其中进行了一些工作，然后又回到原来的主分支进行了另外一些工作。这些改变分别孤立在不同的分支里：我们可以在不同分支里反复切换，并在时机成熟时把它们合并到一起。而所有这些工作，仅仅需要 branch 和 checkout 这两条命令就可以完成。 由于 Git 中的分支实际上仅是一个包含所指对象校验和（40 个字符长度 SHA-1 字串）的文件，所以创建和销毁一个分支就变得非常廉价。说白了，新建一个分支就是向一个文件写入 41 个字节（外加一个换行符）那么简单，当然也就很快了。 这和大多数版本控制系统形成了鲜明对比，它们管理分支大多采取备份所有项目文件到特定目录的方式，所以根据项目文件数量和大小不同，可能花费的时间也会有相当大的差别，快则几秒，慢则数分钟。而 Git 的实现与项目复杂度无关，它永远可以在几毫秒的时间内完成分支的创建和切换。同时，因为每次提交时都记录了祖先信息（即 parent 对象），将来要合并分支时，寻找恰当的合并基础（即共同祖先）的工作其实已经自然而然地摆在那里了，所以实现起来非常容易。Git 鼓励开发者频繁使用分支，正是因为有着这些特性作保障。 分支的新建与合并现在让我们来看一个简单的分支与合并的例子，实际工作中大体也会用到这样的工作流程： 开发某个网站。 为实现某个新的需求，创建一个分支。 在这个分支上开展工作。 假设此时，你突然接到一个电话说有个很严重的问题需要紧急修补，那么可以按照下面的方式处理： 返回到原先已经发布到生产服务器上的分支。 为这次紧急修补建立一个新分支，并在其中修复问题。 通过测试后，回到生产服务器所在的分支，将修补分支合并进来，然后再推送到生产服务器上。 切换到之前实现新需求的分支，继续工作。 分支的新建与切换首先，我们假设你正在项目中愉快地工作，并且已经提交了几次更新（见图 10）。 现在，你决定要修补问题追踪系统上的 #53 问题。顺带说明下，Git 并不同任何特定的问题追踪系统打交道。这里为了说明要解决的问题，才把新建的分支取名为 iss53。要新建并切换到该分支，运行 git checkout 并加上 -b 参数： 12$ git checkout -b iss53Switched to a new branch 'iss53' 这相当于执行下面这两条命令： 12$ git branch iss53$ git checkout iss53 图 11 示意该命令的执行结果。 接着你开始尝试修复问题，在提交了若干次更新后，iss53 分支的指针也会随着向前推进，因为它就是当前分支（换句话说，当前的 HEAD 指针正指向 iss53，见图 12）： 12$ vim index.html$ git commit -a -m 'added a new footer [issue 53]' 现在你就接到了那个网站问题的紧急电话，需要马上修补。有了 Git ，我们就不需要同时发布这个补丁和 iss53 里作出的修改，也不需要在创建和发布该补丁到服务器之前花费大力气来复原这些修改。唯一需要的仅仅是切换回 master 分支。 不过在此之前，留心你的暂存区或者工作目录里，那些还没有提交的修改，它会和你即将检出的分支产生冲突从而阻止 Git 为你切换分支。切换分支的时候最好保持一个清洁的工作区域。稍后会介绍几个绕过这种问题的办法（分别叫做 stashing 和 commit amending）。目前已经提交了所有的修改，所以接下来可以正常转换到 master 分支： 12$ git checkout masterSwitched to branch 'master' 此时工作目录中的内容和你在解决问题 #53 之前一模一样，你可以集中精力进行紧急修补。这一点值得牢记：Git 会把工作目录的内容恢复为检出某分支时它所指向的那个提交对象的快照。它会自动添加、删除和修改文件以确保目录的内容和你当时提交时完全一样。 接下来，你得进行紧急修补。我们创建一个紧急修补分支 hotfix 来开展工作，直到搞定（见图 13）： 123456$ git checkout -b hotfixSwitched to a new branch 'hotfix'$ vim index.html$ git commit -a -m 'fixed the broken email address'[hotfix 3a0874c] fixed the broken email address 1 files changed, 1 deletion(-) 有必要作些测试，确保修补是成功的，然后回到 master 分支并把它合并进来，然后发布到生产服务器。用 git merge 命令来进行合并： 123456$ git checkout master$ git merge hotfixUpdating f42c576..3a0874cFast-forward README | 1 - 1 file changed, 1 deletion(-) 请注意，合并时出现了“Fast forward”的提示。由于当前 master 分支所在的提交对象是要并入的 hotfix 分支的直接上游，Git 只需把 master 分支指针直接右移。换句话说，如果顺着一个分支走下去可以到达另一个分支的话，那么 Git 在合并两者时，只会简单地把指针右移，因为这种单线的历史分支不存在任何需要解决的分歧，所以这种合并过程可以称为快进（Fast forward）。 现在最新的修改已经在当前 master 分支所指向的提交对象中了，可以部署到生产服务器上去了（见图 14）。 在那个超级重要的修补发布以后，你想要回到被打扰之前的工作。由于当前 hotfix 分支和 master 都指向相同的提交对象，所以 hotfix 已经完成了历史使命，可以删掉了。使用 git branch 的 -d 选项执行删除操作： 12$ git branch -d hotfixDeleted branch hotfix (was 3a0874c). 现在回到之前未完成的 #53 问题修复分支上继续工作（图 15）： 123456$ git checkout iss53Switched to branch 'iss53'$ vim index.html$ git commit -a -m 'finished the new footer [issue 53]'[iss53 ad82d7a] finished the new footer [issue 53] 1 file changed, 1 insertion(+) 值得注意的是之前 hotfix 分支的修改内容尚未包含到 iss53 中来。如果需要纳入此次修补，可以用 git merge master 把 master 分支合并到 iss53；或者等 iss53 完成之后，再将 iss53 分支中的更新并入 master。 分支的合并在问题 #53 相关的工作完成之后，可以合并回 master 分支。实际操作同前面合并 hotfix 分支差不多，只需回到 master 分支，运行 git merge 命令指定要合并进来的分支： 123456$ git checkout master$ git merge iss53Auto-merging READMEMerge made by the 'recursive' strategy. README | 1 + 1 file changed, 1 insertion(+) 请注意，这次合并操作的底层实现，并不同于之前 hotfix 的并入方式。因为这次你的开发历史是从更早的地方开始分叉的。由于当前 master 分支所指向的提交对象（C4）并不是 iss53 分支的直接祖先，Git 不得不进行一些额外处理。就此例而言，Git 会用两个分支的末端（C4 和 C5）以及它们的共同祖先（C2）进行一次简单的三方合并计算。图 16 用红框标出了 Git 用于合并的三个提交对象： 这次，Git 没有简单地把分支指针右移，而是对三方合并后的结果重新做一个新的快照，并自动创建一个指向它的提交对象（C6）（见图 17）。这个提交对象比较特殊，它有两个祖先（C4 和 C5）。 值得一提的是 Git 可以自己裁决哪个共同祖先才是最佳合并基础；这和 CVS 或 Subversion（1.5 以后的版本）不同，它们需要开发者手工指定合并基础。所以此特性让 Git 的合并操作比其他系统都要简单不少。 既然之前的工作成果已经合并到 master 了，那么 iss53 也就没用了。你可以就此删除它，并在问题追踪系统里关闭该问题。 1$ git branch -d iss53 遇到冲突时的分支合并有时候合并操作并不会如此顺利。如果在不同的分支中都修改了同一个文件的同一部分，Git 就无法干净地把两者合到一起（逻辑上说，这种问题只能由人来裁决。）。如果你在解决问题 #53 的过程中修改了 hotfix 中修改的部分，将得到类似下面的结果： 1234$ git merge iss53Auto-merging index.htmlCONFLICT (content): Merge conflict in index.htmlAutomatic merge failed; fix conflicts and then commit the result. Git 作了合并，但没有提交，它会停下来等你解决冲突。要看看哪些文件在合并时发生冲突，可以用 git status查阅： 1234567891011$ git statusOn branch masterYou have unmerged paths. (fix conflicts and run "git commit")Unmerged paths: (use "git add &lt;file&gt;..." to mark resolution) both modified: index.htmlno changes added to commit (use "git add" and/or "git commit -a") 任何包含未解决冲突的文件都会以未合并（unmerged）的状态列出。Git 会在有冲突的文件里加入标准的冲突解决标记，可以通过它们来手工定位并解决这些冲突。可以看到此文件包含类似下面这样的部分： 1234567&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD&lt;div id="footer"&gt;contact : email.support@github.com&lt;/div&gt;=======&lt;div id="footer"&gt; please contact us at support@github.com&lt;/div&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt; iss53 可以看到 ======= 隔开的上半部分，是 HEAD（即 master 分支，在运行 merge 命令时所切换到的分支）中的内容，下半部分是在 iss53 分支中的内容。解决冲突的办法无非是二者选其一或者由你亲自整合到一起。比如你可以通过把这段内容替换为下面这样来解决： 123&lt;div id="footer"&gt;please contact us at email.support@github.com&lt;/div&gt; 这个解决方案各采纳了两个分支中的一部分内容，而且我还删除了 &lt;&lt;&lt;&lt;&lt;&lt;&lt;，======= 和 &gt;&gt;&gt;&gt;&gt;&gt;&gt; 这些行。在解决了所有文件里的所有冲突后，运行 git add 将把它们标记为已解决状态（注：实际上就是来一次快照保存到暂存区域。）。因为一旦暂存，就表示冲突已经解决。如果你想用一个有图形界面的工具来解决这些问题，不妨运行 git mergetool，它会调用一个可视化的合并工具并引导你解决所有冲突： 12345678910111213$ git mergetoolThis message is displayed because 'merge.tool' is not configured.See 'git mergetool --tool-help' or 'git help config' for more details.'git mergetool' will now attempt to use one of the following tools:opendiff kdiff3 tkdiff xxdiff meld tortoisemerge gvimdiff diffuse diffmerge ecmerge p4merge araxis bc3 codecompare vimdiff emergeMerging:index.htmlNormal merge conflict for 'index.html': &#123;local&#125;: modified file &#123;remote&#125;: modified fileHit return to start merge resolution tool (opendiff): 如果不想用默认的合并工具（Git 为我默认选择了 opendiff，因为我在 Mac 上运行了该命令），你可以在上方”merge tool candidates”里找到可用的合并工具列表，输入你想用的工具名。 退出合并工具以后，Git 会询问你合并是否成功。如果回答是，它会为你把相关文件暂存起来，以表明状态为已解决。 再运行一次 git status 来确认所有冲突都已解决： 123456$ git statusOn branch masterChanges to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) modified: index.html 如果觉得满意了，并且确认所有冲突都已解决，也就是进入了暂存区，就可以用 git commit 来完成这次合并提交。提交的记录差不多是这样： 12345678910Merge branch 'iss53'Conflicts: index.html## It looks like you may be committing a merge.# If this is not correct, please remove the file# .git/MERGE_HEAD# and try again.# 如果想给将来看这次合并的人一些方便，可以修改该信息，提供更多合并细节。比如你都作了哪些改动，以及这么做的原因。有时候裁决冲突的理由并不直接或明显，有必要略加注解。 分支的管理到目前为止，你已经学会了如何创建、合并和删除分支。除此之外，我们还需要学习如何管理分支，在日后的常规工作中会经常用到下面介绍的管理命令。 git branch 命令不仅仅能创建和删除分支，如果不加任何参数，它会给出当前所有分支的清单： 1234$ git branch iss53* master testing 注意看 master 分支前的 * 字符：它表示当前所在的分支。也就是说，如果现在提交更新，master 分支将随着开发进度前移。若要查看各个分支最后一个提交对象的信息，运行 git branch -v： 1234$ git branch -v iss53 93b412c fix javascript issue* master 7a98805 Merge branch 'iss53' testing 782fd34 add scott to the author list in the readmes 要从该清单中筛选出你已经（或尚未）与当前分支合并的分支，可以用 --merged 和 --no-merged 选项（Git 1.5.6 以上版本）。比如用 git branch --merged 查看哪些分支已被并入当前分支（也就是说哪些分支是当前分支的直接上游。）： 123$ git branch --merged iss53* master 之前我们已经合并了 iss53，所以在这里会看到它。一般来说，列表中没有 * 的分支通常都可以用 git branch -d 来删掉。原因很简单，既然已经把它们所包含的工作整合到了其他分支，删掉也不会损失什么。 另外可以用 git branch --no-merged 查看尚未合并的工作： 12$ git branch --no-merged testing 它会显示还未合并进来的分支。由于这些分支中还包含着尚未合并进来的工作成果，所以简单地用 git branch -d 删除该分支会提示错误，因为那样做会丢失数据： 123$ git branch -d testingerror: The branch 'testing' is not fully merged.If you are sure you want to delete it, run 'git branch -D testing'. 不过，如果你确实想要删除该分支上的改动，可以用大写的删除选项 -D 强制执行，就像上面提示信息中给出的那样。 利用分支进行开发的工作流程现在我们已经学会了新建分支和合并分支，可以（或应该）用它来做点什么呢？我会介绍一些利用分支进行开发的工作流程。而正是由于分支管理的便捷，才衍生出了这类典型的工作模式，你可以根据项目的实际情况选择一种用用看。 长期分支由于 Git 使用简单的三方合并，所以就算在较长一段时间内，反复多次把某个分支合并到另一分支，也不是什么难事。也就是说，你可以同时拥有多个开放的分支，每个分支用于完成特定的任务，随着开发的推进，你可以随时把某个特性分支的成果并到其他分支中。 许多使用 Git 的开发者都喜欢用这种方式来开展工作，比如仅在 master 分支中保留完全稳定的代码，即已经发布或即将发布的代码。与此同时，他们还有一个名为 develop 或 next 的平行分支，专门用于后续的开发，或仅用于稳定性测试 — 当然并不是说一定要绝对稳定，不过一旦进入某种稳定状态，便可以把它合并到 master 里。这样，在确保这些已完成的特性分支（短期分支，比如之前的 iss53 分支）能够通过所有测试，并且不会引入更多错误之后，就可以并到主干分支中，等待下一次的发布。 本质上我们刚才谈论的，是随着提交对象不断右移的指针。稳定分支的指针总是在提交历史中落后一大截，而前沿分支总是比较靠前（见图 18）。 或者把它们想象成工作流水线，或许更好理解一些，经过测试的提交对象集合被遴选到更稳定的流水线（见图 19）。 你可以用这招维护不同层次的稳定性。某些大项目还会有个 proposed（建议）或 pu（proposed updates，建议更新）分支，它包含着那些可能还没有成熟到进入 next 或 master 的内容。这么做的目的是拥有不同层次的稳定性：当这些分支进入到更稳定的水平时，再把它们合并到更高层分支中去。再次说明下，使用多个长期分支的做法并非必需，不过一般来说，对于特大型项目或特复杂的项目，这么做确实更容易管理。 特性分支在任何规模的项目中都可以使用特性（Topic）分支。一个特性分支是指一个短期的，用来实现单一特性或与其相关工作的分支。可能你在以前的版本控制系统里从未做过类似这样的事情，因为通常创建与合并分支消耗太大。然而在 Git 中，一天之内建立、使用、合并再删除多个分支是常见的事。 我们在上节的例子里已经见过这种用法了。我们创建了 iss53 和 hotfix 这两个特性分支，在提交了若干更新后，把它们合并到主干分支，然后删除。该技术允许你迅速且完全的进行语境切换 — 因为你的工作分散在不同的流水线里，每个分支里的改变都和它的目标特性相关，浏览代码之类的事情因而变得更简单了。你可以把作出的改变保持在特性分支中几分钟，几天甚至几个月，等它们成熟以后再合并，而不用在乎它们建立的顺序或者进度。 现在我们来看一个实际的例子。请看图 20，由下往上，起先我们在 master 工作到 C1，然后开始一个新分支 iss91 尝试修复 91 号缺陷，提交到 C6 的时候，又冒出一个解决该问题的新办法，于是从之前 C4 的地方又分出一个分支 iss91v2，干到 C8 的时候，又回到主干 master 中提交了 C9 和 C10，再回到 iss91v2 继续工作，提交 C11，接着，又冒出个不太确定的想法，从 master 的最新提交 C10 处开了个新的分支 dumbidea 做些试验。 现在，假定两件事情：我们最终决定使用第二个解决方案，即 iss91v2 中的办法；另外，我们把 dumbidea 分支拿给同事们看了以后，发现它竟然是个天才之作。所以接下来，我们准备抛弃原来的 iss91 分支（实际上会丢弃 C5 和 C6），直接在主干中并入另外两个分支。最终的提交历史将变成图 21 这样： 请务必牢记这些分支全部都是本地分支，这一点很重要。当你在使用分支及合并的时候，一切都是在你自己的 Git 仓库中进行的 — 完全不涉及与服务器的交互。 远程分支远程分支（remote branch）是对远程仓库中的分支的索引。它们是一些无法移动的本地分支；只有在 Git 进行网络交互时才会更新。远程分支就像是书签，提醒着你上次连接远程仓库时上面各分支的位置。 我们用 (远程仓库名)/(分支名) 这样的形式表示远程分支。比如我们想看看上次同 origin 仓库通讯时 master 分支的样子，就应该查看 origin/master 分支。如果你和同伴一起修复某个问题，但他们先推送了一个 iss53 分支到远程仓库，虽然你可能也有一个本地的 iss53 分支，但指向服务器上最新更新的却应该是 origin/iss53 分支。 可能有点乱，我们不妨举例说明。假设你们团队有个地址为 git.ourcompany.com 的 Git 服务器。如果你从这里克隆，Git 会自动为你将此远程仓库命名为 origin，并下载其中所有的数据，建立一个指向它的 master 分支的指针，在本地命名为 origin/master，但你无法在本地更改其数据。接着，Git 建立一个属于你自己的本地 master分支，始于 origin 上 master 分支相同的位置，你可以就此开始工作（见图 22）： 如果你在本地 master 分支做了些改动，与此同时，其他人向 git.ourcompany.com 推送了他们的更新，那么服务器上的 master 分支就会向前推进，而与此同时，你在本地的提交历史正朝向不同方向发展。不过只要你不和服务器通讯，你的 origin/master 指针仍然保持原位不会移动（见图 23）。 可以运行 git fetch origin 来同步远程服务器上的数据到本地。该命令首先找到 origin 是哪个服务器（本例为 git.ourcompany.com），从上面获取你尚未拥有的数据，更新你本地的数据库，然后把 origin/master 的指针移到它最新的位置上（见图 24）。 为了演示拥有多个远程分支（在不同的远程服务器上）的项目是如何工作的，我们假设你还有另一个仅供你的敏捷开发小组使用的内部服务器 git.team1.ourcompany.com。可以用 git remote add 命令把它加为当前项目的远程分支之一。我们把它命名为 teamone，以便代替完整的 Git URL 以方便使用（见图 25）。 现在你可以用 git fetch teamone 来获取小组服务器上你还没有的数据了。由于当前该服务器上的内容是你 origin 服务器上的子集，Git 不会下载任何数据，而只是简单地创建一个名为 teamone/master 的远程分支，指向 teamone 服务器上 master 分支所在的提交对象 31b8e（见图 26）。 推送本地分支要想和其他人分享某个本地分支，你需要把它推送到一个你拥有写权限的远程仓库。你创建的本地分支不会因为你的写入操作而被自动同步到你引入的远程服务器上，你需要明确地执行推送分支的操作。换句话说，对于无意分享的分支，你尽管保留为私人分支好了，而只推送那些协同工作要用到的特性分支。 如果你有个叫 serverfix 的分支需要和他人一起开发，可以运行 git push (远程仓库名) (分支名)： 1234567$ git push origin serverfixCounting objects: 20, done.Compressing objects: 100% (14/14), done.Writing objects: 100% (15/15), 1.74 KiB, done.Total 15 (delta 5), reused 0 (delta 0)To git@github.com:schacon/simplegit.git * [new branch] serverfix -&gt; serverfix 这里其实走了一点捷径。Git 自动把 serverfix 分支名扩展为 refs/heads/serverfix:refs/heads/serverfix，意为“取出我在本地的 serverfix 分支，推送到远程仓库的 serverfix 分支中去”。以后有机会了再介绍 refs/heads/ 的细节，不过一般使用的时候都可以省略它。也可以运行 git push origin serverfix:serverfix 来实现相同的效果，它的意思是“上传我本地的 serverfix 分支到远程仓库中去，仍旧称它为 serverfix 分支”。通过此语法，你可以把本地分支推送到某个命名不同的远程分支：若想把远程分支叫作 awesomebranch，可以用 git push origin serverfix:awesomebranch 来推送数据。 接下来，当你的协作者再次从服务器上获取数据时，他们将得到一个新的远程分支 origin/serverfix，并指向服务器上 serverfix 所指向的版本： 1234567$ git fetch originremote: Counting objects: 20, done.remote: Compressing objects: 100% (14/14), done.remote: Total 15 (delta 5), reused 0 (delta 0)Unpacking objects: 100% (15/15), done.From git@github.com:schacon/simplegit * [new branch] serverfix -&gt; origin/serverfix 值得注意的是，在 fetch 操作下载好新的远程分支之后，你仍然无法在本地编辑该远程仓库中的分支。换句话说，在本例中，你不会有一个新的 serverfix 分支，有的只是一个你无法移动的 origin/serverfix 指针。 如果要把该远程分支的内容合并到当前分支，可以运行 git merge origin/serverfix。如果想要一份自己的 serverfix 来开发，可以在远程分支的基础上分化出一个新的分支来： 123$ git checkout -b serverfix origin/serverfixBranch serverfix set up to track remote branch serverfix from origin.Switched to a new branch 'serverfix' 这会切换到新建的 serverfix 本地分支，其内容同远程分支 origin/serverfix 一致，这样你就可以在里面继续开发了。 跟踪远程分支从远程分支 checkout 出来的本地分支，称为 跟踪分支 (tracking branch)。跟踪分支是一种和某个远程分支有直接联系的本地分支。在跟踪分支里输入 git push，Git 会自行推断应该向哪个服务器的哪个分支推送数据。同样，在这些分支里运行 git pull 会获取所有远程索引，并把它们的数据都合并到本地分支中来。 在克隆仓库时，Git 通常会自动创建一个名为 master 的分支来跟踪 origin/master。这正是 git push 和 git pull 一开始就能正常工作的原因。当然，你可以随心所欲地设定为其它跟踪分支，比如 origin 上除了 master之外的其它分支。刚才我们已经看到了这样的一个例子：git checkout -b [分支名] [远程名]/[分支名]。如果你有 1.6.2 以上版本的 Git，还可以用 --track 选项简化： 123$ git checkout --track origin/serverfixBranch serverfix set up to track remote branch serverfix from origin.Switched to a new branch 'serverfix' 要为本地分支设定不同于远程分支的名字，只需在第一个版本的命令里换个名字： 123$ git checkout -b sf origin/serverfixBranch sf set up to track remote branch serverfix from origin.Switched to a new branch 'sf' 现在你的本地分支 sf 会自动将推送和抓取数据的位置定位到 origin/serverfix 了。 删除远程分支如果不再需要某个远程分支了，比如搞定了某个特性并把它合并进了远程的 master 分支（或任何其他存放稳定代码的分支），可以用这个非常无厘头的语法来删除它：git push [远程名] :[分支名]。如果想在服务器上删除 serverfix 分支，运行下面的命令： 123$ git push origin :serverfixTo git@github.com:schacon/simplegit.git - [deleted] serverfix 咚！服务器上的分支没了。你最好特别留心这一页，因为你一定会用到那个命令，而且你很可能会忘掉它的语法。有种方便记忆这条命令的方法：记住我们不久前见过的 git push [远程名] [本地分支]:[远程分支] 语法，如果省略 [本地分支]，那就等于是在说“在这里提取空白然后把它变成[远程分支]”。 分支的衍合把一个分支中的修改整合到另一个分支的办法有两种：merge 和 rebase（rebase 的翻译暂定为“衍合”，大家知道就可以了）。 基本的衍合操作请回顾之前有关合并的一节（见图 27），你会看到开发进程分叉到两个不同分支，又各自提交了更新。 之前介绍过，最容易的整合分支的方法是 merge 命令，它会把两个分支最新的快照（C3 和 C4）以及二者最新的共同祖先（C2）进行三方合并，合并的结果是产生一个新的提交对象（C5）。如图 28 所示： 其实，还有另外一个选择：你可以把在 C3 里产生的变化补丁在 C4 的基础上重新打一遍。在 Git 里，这种操作叫做衍合（rebase）。有了 rebase 命令，就可以把在一个分支里提交的改变移到另一个分支里重放一遍。 在上面这个例子中，运行： 1234$ git checkout experiment$ git rebase masterFirst, rewinding head to replay your work on top of it...Applying: added staged command 它的原理是回到两个分支最近的共同祖先，根据当前分支（也就是要进行衍合的分支 experiment）后续的历次提交对象（这里只有一个 C3），生成一系列文件补丁，然后以基底分支（也就是主干分支 master）最后一个提交对象（C4）为新的出发点，逐个应用之前准备好的补丁文件，最后会生成一个新的合并提交对象（C3’），从而改写 experiment 的提交历史，使它成为 master 分支的直接下游，如图 29 所示： 现在回到 master 分支，进行一次快进合并（见图 30）： 现在的 C3’ 对应的快照，其实和普通的三方合并，即上个例子中的 C5 对应的快照内容一模一样了。虽然最后整合得到的结果没有任何区别，但衍合能产生一个更为整洁的提交历史。如果视察一个衍合过的分支的历史记录，看起来会更清楚：仿佛所有修改都是在一根线上先后进行的，尽管实际上它们原本是同时并行发生的。 一般我们使用衍合的目的，是想要得到一个能在远程分支上干净应用的补丁 — 比如某些项目你不是维护者，但想帮点忙的话，最好用衍合：先在自己的一个分支里进行开发，当准备向主项目提交补丁的时候，根据最新的 origin/master 进行一次衍合操作然后再提交，这样维护者就不需要做任何整合工作（实际上是把解决分支补丁同最新主干代码之间冲突的责任，化转为由提交补丁的人来解决），只需根据你提供的仓库地址作一次快进合并，或者直接采纳你提交的补丁。 请注意，合并结果中最后一次提交所指向的快照，无论是通过衍合，还是三方合并，都会得到相同的快照内容，只不过提交历史不同罢了。衍合是按照每行的修改次序重演一遍修改，而合并是把最终结果合在一起。 有趣的衍合衍合也可以放到其他分支进行，并不一定非得根据分化之前的分支。以图 31 的历史为例，我们为了给服务器端代码添加一些功能而创建了特性分支 server，然后提交 C3 和 C4。然后又从 C3 的地方再增加一个 client 分支来对客户端代码进行一些相应修改，所以提交了 C8 和 C9。最后，又回到 server 分支提交了 C10。 假设在接下来的一次软件发布中，我们决定先把客户端的修改并到主线中，而暂缓并入服务端软件的修改（因为还需要进一步测试）。这个时候，我们就可以把基于 client 分支而非 server 分支的改变（即 C8 和 C9），跳过 server 直接放到 master 分支中重演一遍，但这需要用 git rebase 的 --onto 选项指定新的基底分支 master： 1$ git rebase --onto master server client 这好比在说：“取出 client 分支，找出 client 分支和 server 分支的共同祖先之后的变化，然后把它们在 master 上重演一遍”。是不是有点复杂？不过它的结果如图 32 所示，非常酷（虽然 client 里的 C8, C9 在 C3 之后，但这仅表明时间上的先后，而非在 C3 修改的基础上进一步改动，因为 server 和 client 这两个分支对应的代码应该是两套文件，虽然这么说不是很严格，但应理解为在 C3 时间点之后，对另外的文件所做的 C8，C9 修改，放到主干重演。）： 现在可以快进 master 分支了（见图 33）： 12$ git checkout master$ git merge client 现在我们决定把 server 分支的变化也包含进来。我们可以直接把 server 分支衍合到 master，而不用手工切换到 server 分支后再执行衍合操作 — git rebase [主分支] [特性分支] 命令会先取出特性分支 server，然后在主分支 master 上重演： 1$ git rebase master server 于是，server 的进度应用到 master 的基础上，如图 34 所示： 然后就可以快进主干分支 master 了： 12$ git checkout master$ git merge server 现在 client 和 server 分支的变化都已经集成到主干分支来了，可以删掉它们了。最终我们的提交历史会变成图 35 的样子： 12$ git branch -d client$ git branch -d server 衍合的风险呃，奇妙的衍合也并非完美无缺，要用它得遵守一条准则： 一旦分支中的提交对象发布到公共仓库，就千万不要对该分支进行衍合操作。 如果你遵循这条金科玉律，就不会出差错。否则，人民群众会仇恨你，你的朋友和家人也会嘲笑你，唾弃你。 在进行衍合的时候，实际上抛弃了一些现存的提交对象而创造了一些类似但不同的新的提交对象。如果你把原来分支中的提交对象发布出去，并且其他人更新下载后在其基础上开展工作，而稍后你又用 git rebase 抛弃这些提交对象，把新的重演后的提交对象发布出去的话，你的合作者就不得不重新合并他们的工作，这样当你再次从他们那里获取内容时，提交历史就会变得一团糟。 下面我们用一个实际例子来说明为什么公开的衍合会带来问题。假设你从一个中央服务器克隆然后在它的基础上搞了一些开发，提交历史类似图 36 所示： 现在，某人在 C1 的基础上做了些改变，并合并他自己的分支得到结果 C6，推送到中央服务器。当你抓取并合并这些数据到你本地的开发分支中后，会得到合并结果 C7，历史提交会变成图 37 这样： 接下来，那个推送 C6 上来的人决定用衍合取代之前的合并操作；继而又用 git push --force 覆盖了服务器上的历史，得到 C4’。而之后当你再从服务器上下载最新提交后，会得到： 下载更新后需要合并，但此时衍合产生的提交对象 C4’ 的 SHA-1 校验值和之前 C4 完全不同，所以 Git 会把它们当作新的提交对象处理，而实际上此刻你的提交历史 C7 中早已经包含了 C4 的修改内容，于是合并操作会把 C7 和 C4’ 合并为 C8（见图 39）: C8 这一步的合并是迟早会发生的，因为只有这样你才能和其他协作者提交的内容保持同步。而在 C8 之后，你的提交历史里就会同时包含 C4 和 C4’，两者有着不同的 SHA-1 校验值，如果用 git log 查看历史，会看到两个提交拥有相同的作者日期与说明，令人费解。而更糟的是，当你把这样的历史推送到服务器后，会再次把这些衍合后的提交引入到中央服务器，进一步困扰其他人（这个例子中，出问题的责任方是那个发布了 C6 后又用衍合发布 C4’ 的人，其他人会因此反馈双重历史到共享主干，从而混淆大家的视听）。 如果把衍合当成一种在推送之前清理提交历史的手段，而且仅仅衍合那些尚未公开的提交对象，就没问题。如果衍合那些已经公开的提交对象，并且已经有人基于这些提交对象开展了后续开发工作的话，就会出现叫人沮丧的麻烦。 本文参考《Pro Git》 一书]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库学习笔记]]></title>
    <url>%2Fblog%2F2015%2F06%2F22%2F2015-06-22-MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[数据库1.事务 说明 ：事务是恢复和并发控制的基本单位，是用户定义的一个操作序列。这些操作要么都做，要么都不做，是一个不可分割的工作单位。通过事务，逻辑相关的一组操作绑定在一起，以便服务器保持数据的完整性。 事务的特性：ACID A:原子性(Atomicity)事务是数据库的逻辑工作单位，事务中包括的诸操作要么全做，要么全不做。C:一致性(Consistency)事务执行的结果必须是使数据库从一个一致性状态变到另一个一致性状态。一致性与原子性是密切相关的。 I:隔离性(Isolation)一个事务的执行不能被其他事务干扰。 D:持续性/永久性(Durability)一个事务一旦提交，它对数据库中数据的改变就应该是永久性的。 2.多个条件where 1=1 模糊查询%%通配符 3.手写一段sql语句，具体内容忘了，好像和limit有关 4.存储引擎的区别 InnoDB 是支持事务的存储引擎，其设计目标是面向在线事务处理的应用，其特点是行锁设计，支持外键，支持类似Oracle的非锁定读，默认读取操作不会产生锁。MySQL5.5以后是默认的存储引擎。还提供了插入缓存，二次写，预读等高性能和高可用的功能。 MyISAM 引擎不支持事务，表锁设计，支持全文索引，主要是面向OLAP数据库应用。 NDB是集群存储引擎，其数据全部放在内存中，因此主键查找的速度极快。 Memory将表中的数据存放在内存中，如果数据库重启或者发生奔溃，表中的数据都将消失。它使用于存储临时数据的临时表。默认采用哈希索引 5.sql注入原理 就是通过把SQL命令插入到Web 表单 提交或输入域名或页面请求的查询字符串，最终达到欺骗服务器执行恶意的SQL命令 1）猜表名，列名等 2）后台身份验证绕过漏洞 验证绕过漏洞就是’or’=’or’后台绕过漏洞，利用的就是AND和OR的运算规则，从而造成后台脚本逻辑性错误. 防范： 1）永远不要信任用户的输入，要对用户的输入进行校验，可以通过正则表达式，或限制长度，对单引号和双”-“进行转换等。 2）永远不要使用动态拼装SQL，可以使用参数化的SQL或者直接使用存储过程进行数据查询存取。 3）永远不要使用管理员权限的数据库连接，为每个应用使用单独的权限有限的数据库连接。 4）不要把机密信息明文存放，请加密或者hash掉密码和敏感的信息。 5）应用的异常信息应该给出尽可能少的提示，最好使用自定义的错误信息对原始错误信息进行包装，把异常信息存放在独立的表中。 6.数据库范式 第一范式（1NF）：属性不可分。 第二范式（2NF）：符合1NF，并且，非主属性完全依赖于码。 第三范式（3NF）：符合2NF，并且，消除传递依赖 BCNF:符合3NF, 并且,没有任何属性完全函数依赖于非码的任何一组属性. 数据库索引索引是一个单独存储在磁盘上的数据库结构，它们包含着对数据表里所有记录的引用指针，使用索引可以提高数据库特定数据的查询速度.索引时在存储引擎中实现的，因此每种存储引擎的索引不一定完全相同,并且每种存储引擎也不一定支持所有索引类型． 索引的存储类型有两种：BTREE和HASH,具体和表的存储引擎有关．MyISAM和InnoDB存储引擎只支持BTREE;MEMORY/HEAD存储索引可以支持HASH和BTREE索引． 索引的优点： 1.通过创建唯一索引，可以保证数据库表中每行数据的唯一性. 2.可以加快数据的查询速度． 3.在实现数据的参考完整性方面，可以加速表和表之间的连接． 索引的缺点： 1.创建索引和维护索引要耗费时间，并且随着数据量的增加耗费时间也增加． 2.索引需要占空间内存． 3.在对表中数据进行增加,删除和修改的时候，索引也需要动态维护，这样降低了数据维护速度． 索引分类： 1.普通索引 2.唯一索引 数据库锁机制数据库锁定机制简单来说就是数据库为了保证数据的一致性而使各种共享资源在被并发访问，访问变得有序所设计的一种规则。MySQL各存储引擎使用了三种类型（级别）的锁定机制：行级锁定，页级锁定和表级锁定。 表级锁定（table-level）：表级别的锁定是MySQL各存储引擎中最大颗粒度的锁定机制。该锁定机制最大的特点是实现逻辑非常简单，带来的系统负面影响最小。所以获取锁和释放锁的速度很快。由于表级锁一次会将整个表锁定，所以可以很好的避免困扰我们的死锁问题。当然，锁定颗粒度大所带来最大的负面影响就是出现锁定资源争用的概率也会最高，致使并大度大打折扣。表级锁分为读锁和写锁。 页级锁定（page-level）：页级锁定的特点是锁定颗粒度介于行级锁定与表级锁之间，所以获取锁定所需要的资源开销，以及所能提供的并发处理能力也同样是介于上面二者之间。另外，页级锁定和行级锁定一样，会发生死锁。 行级锁定（row-level）：行级锁定最大的特点就是锁定对象的颗粒度很小，也是目前各大数据库管理软件所实现的锁定颗粒度最小的。由于锁定颗粒度很小，所以发生锁定资源争用的概率也最小，能够给予应用程序尽可能大的并发处理能力而提高一些需要高并发应用系统的整体性能。虽然能够在并发处理能力上面有较大的优势，但是行级锁定也因此带来了不少弊端。由于锁定资源的颗粒度很小，所以每次获取锁和释放锁需要做的事情也更多，带来的消耗自然也就更大了。此外，行级锁定也最容易发生死锁。InnoDB的行级锁同样分为两种，共享锁和排他锁，同样InnoDB也引入了意向锁（表级锁）的概念，所以也就有了意向共享锁和意向排他锁，所以InnoDB实际上有四种锁，即共享锁（S）、排他锁（X）、意向共享锁（IS）、意向排他锁（IX）； 在MySQL数据库中，使用表级锁定的主要是MyISAM，Memory，CSV等一些非事务性存储引擎，而使用行级锁定的主要是Innodb存储引擎和NDBCluster存储引擎，页级锁定主要是BerkeleyDB存储引擎的锁定方式。 而意向锁的作用就是当一个事务在需要获取资源锁定的时候，如果遇到自己需要的资源已经被排他锁占用的时候，该事务可以需要锁定行的表上面添加一个合适的意向锁。如果自己需要一个共享锁，那么就在表上面添加一个意向共享锁。而如果自己需要的是某行（或者某些行）上面添加一个排他锁的话，则先在表上面添加一个意向排他锁。意向共享锁可以同时并存多个，但是意向排他锁同时只能有一个存在。 | | 共享锁（S）| 排他锁（X）| 意向共享锁（IS）| 意向排他锁（IX）|共享锁（S） | 兼容 | 冲突 | 兼容 |冲突排他锁（X） | 冲突 | 冲突 | 冲突 |冲突意向共享锁（IS） | 兼容 | 冲突 | 兼容 |兼容意向排他锁（IX） | 冲突 | 冲突 | 兼容 |兼容 参考地址：http://www.cnblogs.com/ggjucheng/archive/2012/11/14/2770445.html MyISAM 表锁优化建议： 1、缩短锁定时间 2、分离能并行的操作 3、合理利用读写优先级 乐观锁，悲观锁悲观锁:它指的是对数据被外界（包括本系统当前的其他事务，以及来自外部系统的事务处理）修改持保守态度，因此，在整个数据处理过程中，将数据处于锁定状态。悲观锁的实现，往往依靠数据库提供的锁机制。悲观的缺陷是不论是页锁还是行锁，加锁的时间可能会很长，这样可能会长时间的限制其他用户的访问，也就是说悲观锁的并发访问性不好。 乐观锁（ Optimistic Locking ） :相对悲观锁而言，乐观锁假设认为数据一般情况下不会造成冲突，所以在数据进行提交更新的时候，才会正式对数据的冲突与否进行检测，如果发现冲突了，则则拒绝更新并返回用户错误的信息，让用户决定如何去做。乐观锁由程序实现，不会存在死锁问题。它适用的场景也相对乐观。但乐观锁不能解决脏读的问题 悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。[1] 乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。[1] 乐观锁不能解决脏读的问题。 事务隔离机制为什么？ 1.更新丢失 两个事务都同时更新一行数据，一个事务对数据的更新把另一个事务对数据的更新覆盖了。这是因为系统没有执行任何的锁操作，因此并发事务并没有被隔离开来。 2.脏读 一个事务读取到了另一个事务未提交的数据操作结果。这是相当危险的，因为很可能所有的操作都被回滚。 3.不可重复读 不可重复读（Non-repeatable Reads）：一个事务对同一行数据重复读取两次，但是却得到了不同的结果。 4、幻读：幻读与不可重复读类似。它发生在一个事务(T1)读取了几行数据，接着另一个并发事务(T2)插入了一些数据时。在随后的查询中，第一个事务(T1)就会发现多了一些原本不存在的记录 事物隔离级别： 未提交读(READ UNCOMMITTED):允许脏读 不允许更新丢失 提交读(READ COMMITTED):允许不可重复读 但不允许脏读 可重复读(REPEATABLE READ):禁止不可重复读和脏读 但可能出现幻读 可串行化(SERIZLIZABLE):它通过强制事务串行执行，不能并发的执行，避免了前面说的幻读的问题。 总结：隔离级别越高，越能保证事务的完整性和一致性，但对并发性能的影响也就越大。对于多数应用可以把隔离级别设置为ReadCommited，也就是授权读取，能够避免脏读，而保持较好的并发性能。 1脏读 不可重复读 幻读可能性 加锁读 未提交读 YES YES YES NO提交读 NO YES YES NO可重复读 NO NO YES NO可串行化 NO NO NO YES 数据库事务属性事务是由一组SQL语句组成的逻辑处理单元，事务具有以下4个属性，通常简称为事务的ACID属性。原子性（Atomicity）：事务是一个原子操作单元，其对数据的修改，要么全都执行，要么全都不执行。一致性（Consistent）：在事务开始和完成时，数据都必须保持一致状态。这意味着所有相关的数据规则都必须应用于事务的修改，以保持数据的完整性；事务结束时，所有的内部数据结构（如B树索引或双向链表）也都必须是正确的。隔离性（Isolation）：数据库系统提供一定的隔离机制，保证事务在不受外部并发操作影响的“独立”环境执行。这意味着事务处理过程中的中间状态对外部是不可见的，反之亦然。持久性（Durable）：事务完成之后，它对于数据的修改是永久性的，即使出现系统故障也能够保持。 数据库事务的几种粒度数据库的索引是如何实现的MyISAM索引实现MyISAM索引使用了B+Tree作为索引结构，叶子结点的data域存放的是数据记录的地址。MyISAM中索引检索的算法为首先按照B+Tree搜索算法搜索索引，如果指定的Key存在，则取出其data域的值，然后以data域的值为地址，读取相应数据记录。主索引和辅助索引的存储结构没有任何区别。 InnoDB索引实现虽然InnoDB也使用B+Tree作为索引结构，但具体实现方式却与MyISAM截然不同。第一个重大区别是InnoDB的数据文件本身就是索引文件。从上文知道，MyISAM索引文件和数据文件是分离的，索引文件仅保存数据记录的地址。而在InnoDB中，表数据文件本身就是按B+Tree组织的一个索引结构，这棵树的叶节点data域保存了完整的数据记录。这种索引叫做聚集索引。这种索引叫做聚集索引。因为InnoDB的数据文件本身要按主键聚集，所以InnoDB要求表必须有主键（MyISAM可以没有），如果没有显式指定，则MySQL系统会自动选择一个可以唯一标识数据。第二个与MyISAM索引的不同是InnoDB的辅助索引data域存储相应记录主键的值而不是地址。换句话说，InnoDB的所有辅助索引都引用主键作为data域。 Memory索引实现Memory索引适用于需要快速访问数据的场景，显示支持哈希索引。内部基于哈希表数据结构实现，只包含哈希值和行指针，对于每一行数据，存储引擎都会对所有的引擎列计算一个哈希码，在哈希表对应位置存放该行数据的指针或地址。为了解决多个hash冲突问题，哈希索引采用了链地址法来解决冲突问题。所以采用链表数组作为存储结构。这种索引结构十分紧凑，且具有很快的查询速度。但也存在一些问题，1.哈希表数据不是按照索引顺序存储的，所以无法用于排序。2.只能支持等值比较查询。3.存在冲突情况下查询速度变慢。https://msdn.microsoft.com/zh-cn/library/dn133190.aspx 数据库连接池原理背景 传统的数据库连接方式是，用户每次请求都要向数据库获取连接，而数据库连接的创建和关闭需要一定的开销。频繁的建立、关闭数据库，会极大的降低系统的性能，增大系统的开销，甚至成为系统的瓶颈。另外使用这种传统的模式，还必须管理数据库的每一个连接，以确保他们能正确关闭，如果出现程序异常而导致某些连接未能关闭。同时无节制的创建连接极易导致数据库服务器内存溢出。 原理 数据库连接池的基本思想就是为数据库连接建立一个“缓冲池”。预先在缓冲池中放入一定数量的连接，当需要建立数据库连接时，只需从“缓冲池”中取出一个，使用完毕之后再放回去。以及一套连接使用、分配、管理策略，使得该连接池中的连接可以得到高效、安全的复用，避免了数据库连接频繁建立、关闭的开销。我们可以通过设定连接池最大连接数来防止系统无尽的与数据库连接。 开源java连接池 现在很多Web服务器(Weblogic, WebSphere, Tomcat)都提供了DataSoruce的实现，即连接池的实现。通常我们把DataSource的实现，按其英文含义称之为数据源，数据源中都包含了数据库连接池的实现。 1.C3P0 :是一个开放源代码的JDBC连接池，它在lib目录中与Hibernate一起发布,包括了实现jdbc3和jdbc2扩展规范说明的Connection 和Statement 池的DataSources 对象。参考网站: http://sourceforge.net/projects/c30/ 2.Proxool :是一个Java SQL Driver驱动程序，提供了对你选择的其它类型的驱动程序的连接池封装。可以非常简单的移植到现存的代码中。完全可配置。快速，成熟，健壮。可以透明地为你现存的JDBC驱动程序增加连接池功能。 参考网站: http://proxool.sourceforge.net 3.Jakarta DBCP :是一个依赖Jakarta commons-pool对象池机制的数据库连接池.DBCP可以直接的在应用程序用使用。参考网站: http://commons.apache.org/proper/commons-dbcp/ 原理: http://www.uml.org.cn/sjjm/201004153.asp实现: http://www.cnblogs.com/lihuiyy/archive/2012/02/14/2351768.html 连接池使用什么数据结构实现（链表） mysql有那些存储引擎，分别有什么特点join操作LEFT JOIN 关键字会从左表 (Persons) 那里返回所有的行，即使在右表 (Orders) 中没有匹配的行。 RIGHT JOIN 关键字会从右表 (Orders) 那里返回所有的行，即使在左表 (Persons) 中没有匹配的行。 FULL JOIN 关键字会从左表 (Persons) 和右表 (Orders) 那里返回所有的行。如果 “Persons” 中的行在表 “Orders” 中没有匹配，或者如果 “Orders” 中的行在表 “Persons” 中没有匹配，这些行同样会列出。 INNER JOIN 关键字在表中存在至少一个匹配时返回行。如果 “Persons” 中的行在 “Orders” 中没有匹配，就不会列出这些行。 三大范式第一范式（无重复的列） 第二范式（属性完全依赖于主键） 定义：满足第一范式前提，当存在多个主键的时候，才会发生不符合第二范式的情况。比如有两个主键，不能存在这样的属性，它只依赖于其中一个主键，这就是不符合。通俗解释：任意一个字段都只依赖表中的同一个字段。 eg:比如不符合第二范式学生证 名称 学生证号 学生证办理时间 借书证名称 借书证号 借书证办理时间 改成2张表如下学生证表学生证 学生证号 学生证办理时间 借书证表借书证 借书证号 借书证办理时间 第三范式（属性不能传递依赖于主属性） 定义：满足第二范式前提，如果某一属性依赖于其他非主键属性，而其他非主键属性又依赖于主键，那么这个属性就是间接依赖于主键，这被称作传递依赖于主属性。 eg:爸爸资料表，不满足第三范式爸爸 儿子 女儿 女儿的小熊 女儿的海绵宝宝 改成 爸爸信息表：爸爸 儿子 女儿女儿信息表女儿 女儿的小熊 女儿的海绵宝宝]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>学习笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL在Linux上修改root密码]]></title>
    <url>%2Fblog%2F2015%2F06%2F10%2F2015-06-10-MySQL%E4%BF%AE%E6%94%B9root%E5%AF%86%E7%A0%81%2F</url>
    <content type="text"><![CDATA[方法一：用set password命令首先，登陆mysql mysql -u root -p 然后执行set password命令set password for root@localhost = password(‘654321’); 上面例子，将root密码更改为654321 方法二：使用mysqladmin格式为：mysqladmin -u用户名 -p旧密码 password 新密码 mysqladmin -uroot -p123456 password “654321” 上面例子，将root密码由123456更改为654321 方法三：更改mysql的user表首先，登陆mysql 12345mysql -uroot -p# 然后操作mysql库的user表，进行updatemysql&gt; use mysql;mysql&gt; update user set password=password(&apos;654321&apos;) where user=&apos;root&apos; and host=&apos;localhost&apos;;mysql&gt; flush privileges; 方法四：忘记密码的情况下首先停止mysql服务 1、service mysqld stop 以跳过授权的方式启动mysql 2、mysqld_safe –skip-grant-tables &amp; 3、mysql -u root 操作mysql库的user表，进行update 12345mysql&gt; use mysql;mysql&gt; update user set password=password(&apos;654321&apos;) where user=&apos;root&apos; and host=&apos;localhost&apos;;mysql&gt; flush privileges;mysql&gt; quit# 重启mysql服务 重启mysql服务service mysqld restart 开启远程mysql连接步骤：1、登入mysql 2、use mysql命令 3、GRANT ALL PRIVILEGES ON . TO ‘root’@’%’ IDENTIFIED BY ‘root’ WITH GRANT OPTION; 4、flush privileges 5、查看select host,user from user]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>修改密码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL多表查询]]></title>
    <url>%2Fblog%2F2015%2F06%2F04%2F2015-06-04-MySQL%E5%A4%9A%E8%A1%A8%E6%9F%A5%E8%AF%A2%2F</url>
    <content type="text"><![CDATA[多表查询的分类1.连接查询交叉连接：cross join（很少用这个） 交叉连接：查询到的是两个表的笛卡尔积。 语法1：select * from 表1 cross join 表2; 语法2：select * from 表1,表2; 内连接：inner join（inner是可以省略的） 显试内连接：在SQL中显示的调用inner join关键字 语法：select * from 表1 inner join 表2 on 关联条件; 隐式内连接：在SQL中没有调用inner join关键字 语法：select * from 表1,表2 where 关联条件; 外连接：outer join（outer是可以省略的） 左外连接： 语法：select * from 表1 left outer join 表2 on 关联条件; 右外连接： 语法：select * from 表1 right outer join 表2 on 关联条件; 2.子查询子查询：一个查询语句条件需要依赖另一个查询语句的结果其实就是sql查询语句的嵌套使用 实例说明1.数据准备这里用班级、学生、课程、和学生选课情况四张表之间的关系来举例说明。 班级表： 有1/2/3/4四个班 学生表： 有9名学生，其中4班没有学生，有2名学生没有班级 课程表： 有语数英三门课程 学生选课表： 每个学生选了几门课，以及得的分数 四张表的外键关系： 2.交叉连接使用cross join关键字 1select * from class cross join student; 不使用cross join关键字（效果和上面的相同） 1select * from class,student; class表有4条数据，student表有9条数据，所以上面两种查询的结果都是36条数据（笛卡尔积） 3.内连接显式内连接12select * from class c inner join student s on c.class_id = s.class_id;-- inner可以省略不写 隐式内连接效果和显式内连接相同 1select * from class c,student s where c.class_id = s.class_id; class表有4条数据，student表有9条数据，但student表的学生没有4班的，还有2个学生没有班级。内连接是取两个表共有的部分，结果是7条数据 4.外连接左外连接12select * from class c left outer join student s on c.class_id = s.class_id;-- outer可以省略不写 左外连接会以左表（class表）为基准，查询出左表的全部内容，即便4班在学生表内没有人，也会将4班的结果列出来 右外连接12select * from class c right outer join student s on c.class_id = s.class_id;-- outer可以省略不写 右外连接会以右表（student表）为基准，查询出右表的全部内容，即便有两名学生没有班级，也会将这两名学生的结果列出来 5.子查询我们可以在where和having子句中使用子查询，将子查询得到的结果作为判断的条件 带in的子查询查询学生生日在1990年3月1日之后的班级的信息 1select * from class where class_id in (select class_id from student where student_birthday &gt; '1990-03-01'); in表示前面的值是否存在子查询结果集中。“in”也可以写成“= any”，”not in”也可以写成”&lt;&gt;all”，子查询结果中有null的时候，null不会用于比较。 带exists的子查询查询学生生日大于1991年1月1日，如果记录存在，前面的SQL语句就执行 1select * from class where exists (SELECT class_id FROM student WHERE student_birthday &gt; '1991-01-01'); exists可以理解为，将主查询的数据，放到子查询中做条件验证，根据验证结果（true 或 false）来决定主查询的数据结果是否得以保留。 带any的子查询查询，班级id大于，任意一个，学生所在班级id，的班级信息 1select * from class where class_id &gt; any (select class_id from student); any可以理解为or，或的意思，只要满足后面结果的其一即可。some和any的效果是相同的，所以也可以写成some。子查询结果中有null的时候，null不会用于比较 &lt;&gt;any是只要不等于其中的任意一个，就成立 带all的子查询查询，班级id大于，任何一个，学生所在班级id，的班级信息 1select * from class where class_id &gt; all (select class_id from student); all可以理解为and，且的意思，要满足后面结果的每一项才可。注意！！all在子查询结果中有null的时候，不会返回数据。我这里演示的时候，将学生表的那两个没有班级的学生，设置了班级id为3，才有的返回结果。&lt;&gt;all的同义词是not in，表示不等于集合中的所有值，这个很容易和&lt;&gt;any搞混，平时多留点心就好了。]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>多表查询</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL基础使用规范]]></title>
    <url>%2Fblog%2F2015%2F05%2F26%2F2015-05-26-MySql%E5%9F%BA%E7%A1%80%E4%BD%BF%E7%94%A8%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[一、基础规范1、使用InnoDB存储引擎 支持事务、行级锁、并发性能更好、CPU及内存缓存页优化使得资源利用率更高 2、推荐使用utf8mb4字符集 无需转码，无乱码风险, 支持emoji表情以及部分不常见汉字 3、表、字段必须加注释 方便他人理解字段意思。4、不在数据库做计算 禁止使用存储过程、视图、触发器、Event。 在并发量大的情况下，这些功能很可能将数据库拖跨，业务逻辑放到服务层具备更好的扩展性，能够轻易实现“增机器就加性能” 5、禁止存储文件 文件存储在文件系统，数据库里存URI 6、控制单表数据量 单表记录控制在千万级 二、命名规范1、库名、表名、字段名：小写，下划线风格 非唯一索引名idx_xxx，唯一索引名uniq_xxx 2、表必须有主键，例如自增主键 a）主键递增，数据行写入可以提高插入性能 b）主键要选择较短的数据类型，Innodb引擎普通索引都会保存主键的值，较短的数据类型可以有效的减少索引的磁盘空间，提高索引的缓存效率 c）保证实体的完整性，唯一性 3、不要使用外键，如果有外键约束，用应用程序控制 外键会导致表与表之间耦合，update与delete操作都会涉及相关联的表，十分影响sql 的性能，甚至会造成死锁。高并发情况下容易造成数据库性能下降，大数据高并发业务场景数据库使用以性能优先 三、字段设计规范1、把字段定义为NOT NULL并且提供默认值 a）null的列使索引/索引统计/值比较都更加复杂，对MySQL来说更难优化 b）null 这种类型MySQL内部需要进行特殊处理，增加数据库处理记录的复杂性；同等条件下，表中有较多空字段的时候，数据库的处理性能会降低很多 c）null值需要更多的存储空间，无论是表还是索引中每行中的null的列都需要额外的空间来标识 d）对null 的处理时候，只能采用is null或is not null，而不能采用=、in、&lt;、&lt;&gt;、!=、not in这些操作符号。如：where name!=’zhangsan’，如果存在name为null值的记录，查询结果就不会包含name为null值的记录 2、不要使用TEXT、BLOB类型 会浪费更多的磁盘和内存空间，非必要的大量的大字段查询会淘汰掉热数据，导致内存命中率急剧降低，影响数据库性能,如果必须要使用则独立出来一张表，用主键来对应，避免影响其它字段索引效率 3、不要使用小数存储货币 建议使用整数，小数容易导致钱对不上 4、必须使用varchar存储手机号 手机号会去做数学运算么？ 5、为提高效率可以牺牲范式设计，冗余数据 a）不是频繁修改的字段 b）不是 varchar 超长字段，更不能是 text 字段 四、索引设计规范1、禁止在更新十分频繁、区分度不高的属性上建立索引 a）更新会变更B+树，更新频繁的字段建立索引会大大降低数据库性能 b）“性别”这种区分度不大的属性，建立索引是没有什么意义的 2、建立组合索引，必须把区分度高的字段放在最左边 如果 where a=? and b=? ， a 列的几乎接近于唯一值，那么只需要单建 idx_a 索引即可 3、 页面搜索严禁左模糊或者全模糊 索引文件具有 B-Tree 的最左前缀匹配特性，如果左边的值未确定，那么无法使用此索引, 如果需要请走搜索引擎来解决 五、SQL使用规范1、禁止使用SELECT *，只获取必要的字段，需要显示说明列属性 a）消耗cpu，io，内存，带宽 b）不能有效的利用覆盖索引 c）使用SELECT *容易在增加或者删除字段后出现程序BUG, 不具有扩展性 2、使用INSERT INTO t_xxx VALUES(xxx)，必须显示指定插入的列属性 容易在增加或者删除字段后出现程序BUG 3、务必请使用“同类型”进行比较，否则可能全表扫面 SELECT name FROM t_user WHERE phone=1333333333 会导致全表扫描. 4、禁止在WHERE条件的上使用函数或者计算 解读：SELECT naem FROM t_user WHERE date(create_datatime)=’2017-12-15’ 会导致全表扫描 推荐的写法是：SELECT name FROM t_user WHERE create_datatime&gt;=’2017-02-15’ and create_datatime &lt; ‘2017-02-16 ‘ 5、禁止负向查询，以及%开头的模糊查询 a）负向查询条件：NOT、!=、&lt;&gt;、!&lt;、!&gt;、NOT IN、NOT LIKE等，会导致全表扫描 b）%开头的模糊查询，会导致全表扫描 6、不要大表使用JOIN查询，禁止大表使用子查询 会产生临时表，消耗较多内存与CPU，极大影响数据库性能 7、OR改写为IN()或者UNION 原因很简单or不会走索引 8、简单的事务 事务就像程序中的锁一样粒度尽可能要小 9、不要一次更新大量数据 数据更新会对行或者表加锁，应该分为多次更新]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>规范</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[如何正确的写出单例模式]]></title>
    <url>%2Fblog%2F2015%2F04%2F18%2F2015-04-18-%E5%A6%82%E4%BD%95%E6%AD%A3%E7%A1%AE%E7%9A%84%E5%86%99%E5%87%BA%E5%8D%95%E4%BE%8B%E6%A8%A1%E5%BC%8F%2F</url>
    <content type="text"><![CDATA[单例模式算是设计模式中最容易理解，也是最容易手写代码的模式了吧。但是其中的坑却不少，所以也常作为面试题来考。本文主要对几种单例写法的整理，并分析其优缺点。很多都是一些老生常谈的问题，但如果你不知道如何创建一个线程安全的单例，不知道什么是双检锁，那这篇文章可能会帮助到你。 懒汉式，线程不安全当被问到要实现一个单例模式时，很多人的第一反应是写出如下的代码，包括教科书上也是这样教我们的。 12345678910public class Singleton &#123; private static Singleton instance; private Singleton ()&#123;&#125; public static Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance; &#125;&#125; 这段代码简单明了，而且使用了懒加载模式，但是却存在致命的问题。当有多个线程并行调用 getInstance() 的时候，就会创建多个实例。也就是说在多线程下不能正常工作。 懒汉式，线程安全为了解决上面的问题，最简单的方法是将整个 getInstance() 方法设为同步（synchronized）。 123456public static synchronized Singleton getInstance() &#123; if (instance == null) &#123; instance = new Singleton(); &#125; return instance;&#125; 虽然做到了线程安全，并且解决了多实例的问题，但是它并不高效。因为在任何时候只能有一个线程调用 getInstance() 方法。但是同步操作只需要在第一次调用时才被需要，即第一次创建单例实例对象时。这就引出了双重检验锁。 懒汉式，双重检验锁双重检验锁模式（double checked locking pattern），是一种使用同步块加锁的方法。程序员称其为双重检查锁，因为会有两次检查 instance == null，一次是在同步块外，一次是在同步块内。为什么在同步块内还要再检验一次？因为可能会有多个线程一起进入同步块外的 if，如果在同步块内不进行二次检验的话就会生成多个实例了。 12345678910public static Singleton getSingleton() &#123; if (instance == null) &#123; //Single Checked synchronized (Singleton.class) &#123; if (instance == null) &#123; //Double Checked instance = new Singleton(); &#125; &#125; &#125; return instance ;&#125; 这段代码看起来很完美，很可惜，它是有问题。主要在于instance = new Singleton()这句，这并非是一个原子操作，事实上在 JVM 中这句话大概做了下面 3 件事情。 给 instance 分配内存 调用 Singleton 的构造函数来初始化成员变量 将instance对象指向分配的内存空间（执行完这步 instance 就为非 null 了） 但是在 JVM 的即时编译器中存在指令重排序的优化。也就是说上面的第二步和第三步的顺序是不能保证的，最终的执行顺序可能是 1-2-3 也可能是 1-3-2。如果是后者，则在 3 执行完毕、2 未执行之前，被线程二抢占了，这时 instance 已经是非 null 了（但却没有初始化），所以线程二会直接返回 instance，然后使用，然后顺理成章地报错。 我们只需要将 instance 变量声明成 volatile 就可以了。 123456789101112131415public class Singleton &#123; private volatile static Singleton instance; //声明成 volatile private Singleton ()&#123;&#125; public static Singleton getSingleton() &#123; if (instance == null) &#123; synchronized (Singleton.class) &#123; if (instance == null) &#123; instance = new Singleton(); &#125; &#125; &#125; return instance; &#125; &#125; 有些人认为使用 volatile 的原因是可见性，也就是可以保证线程在本地不会存有 instance 的副本，每次都是去主内存中读取。但其实是不对的。使用 volatile 的主要原因是其另一个特性：禁止指令重排序优化。也就是说，在 volatile 变量的赋值操作后面会有一个内存屏障（生成的汇编代码上），读操作不会被重排序到内存屏障之前。比如上面的例子，取操作必须在执行完 1-2-3 之后或者 1-3-2 之后，不存在执行到 1-3 然后取到值的情况。从「先行发生原则」的角度理解的话，就是对于一个 volatile 变量的写操作都先行发生于后面对这个变量的读操作（这里的“后面”是时间上的先后顺序）。 但是特别注意在 Java 5 以前的版本使用了 volatile 的双检锁还是有问题的。其原因是 Java 5 以前的 JMM （Java 内存模型）是存在缺陷的，即时将变量声明成 volatile 也不能完全避免重排序，主要是 volatile 变量前后的代码仍然存在重排序问题。这个 volatile 屏蔽重排序的问题在 Java 5 中才得以修复，所以在这之后才可以放心使用 volatile。 相信你不会喜欢这种复杂又隐含问题的方式，当然我们有更好的实现线程安全的单例模式的办法。 懒汉式，静态内部类 static nested class我比较倾向于使用静态内部类的方法，这种方法也是《Effective Java》上所推荐的。 123456789public class Singleton &#123; private static class SingletonHolder &#123; private static final Singleton INSTANCE = new Singleton(); &#125; private Singleton ()&#123;&#125; public static final Singleton getInstance() &#123; return SingletonHolder.INSTANCE; &#125; &#125; 这种写法仍然使用JVM本身机制保证了线程安全问题；由于 SingletonHolder 是私有的，除了 getInstance() 之外没有办法访问它，因此它是懒汉式的；同时读取实例的时候不会进行同步，没有性能缺陷；也不依赖 JDK 版本。 饿汉式 static final field这种方法非常简单，因为单例的实例被声明成 static 和 final 变量了，在第一次加载类到内存中时就会初始化，所以创建实例本身是线程安全的。 123456789public class Singleton&#123; //类加载时就初始化 private static final Singleton instance = new Singleton(); private Singleton()&#123;&#125; public static Singleton getInstance()&#123; return instance; &#125;&#125; 这种写法如果完美的话，就没必要在啰嗦那么多双检锁的问题了。缺点是它不是一种懒加载模式（lazy initialization），单例会在加载类后一开始就被初始化，即使客户端没有调用 getInstance()方法。饿汉式的创建方式在一些场景中将无法使用：譬如 Singleton 实例的创建是依赖参数或者配置文件的，在 getInstance() 之前必须调用某个方法设置参数给它，那样这种单例写法就无法使用了。 枚举 Enum用枚举写单例实在太简单了！这也是它最大的优点。下面这段代码就是声明枚举实例的通常做法。 123public enum EasySingleton&#123; INSTANCE;&#125; 我们可以通过EasySingleton.INSTANCE来访问实例，这比调用getInstance()方法简单多了。创建枚举默认就是线程安全的，所以不需要担心double checked locking，而且还能防止反序列化导致重新创建新的对象。但是还是很少看到有人这样写，可能是因为不太熟悉吧。 总结一般来说，单例模式有五种写法：懒汉、饿汉、双重检验锁、静态内部类、枚举。上述所说都是线程安全的实现，文章开头给出的第一种方法不算正确的写法。 就我个人而言，一般情况下直接使用饿汉式就好了，如果明确要求要懒加载（lazy initialization）会倾向于使用静态内部类，如果涉及到反序列化创建对象时会试着使用枚举的方式来实现单例。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>单例模式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JDK1.7中HashMap底层实现原理]]></title>
    <url>%2Fblog%2F2015%2F03%2F13%2F2015-03-13-JDK1-7%E4%B8%ADHashMap%E5%BA%95%E5%B1%82%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%2F</url>
    <content type="text"><![CDATA[一、数据结构HashMap中的数据结构是数组+单链表的组合，以键值对(key-value)的形式存储元素的，通过put()和get()方法储存和获取对象。 （蓝色方块表示Entry对象，横排红框表示数组table[ ]，纵排绿框表示哈希桶bucket【实际上是一个由Entry组成的链表，新加入的Entry放在链头，最先加入的放在链尾】） 二、实现原理成员变量源码分析： 1234567891011121314151617181920212223242526/** 初始容量，默认16 */static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; // aka 16/** 最大初始容量，2^30 */static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30;/** 负载因子，默认0.75，负载因子越小，hash冲突机率越低 */static final float DEFAULT_LOAD_FACTOR = 0.75f;/** 初始化一个Entry的空数组 */static final Entry&lt;?,?&gt;[] EMPTY_TABLE = &#123;&#125;;/** 将初始化好的空数组赋值给table，table数组是HashMap实际存储数据的地方，并不在EMPTY_TABLE数组中 */transient Entry&lt;K,V&gt;[] table = (Entry&lt;K,V&gt;[]) EMPTY_TABLE;/** HashMap实际存储的元素个数 */transient int size;/** 临界值（HashMap 实际能存储的大小）,公式为(threshold = capacity * loadFactor) */int threshold;/** 负载因子 */final float loadFactor;/** HashMap的结构被修改的次数，用于迭代器 */transient int modCount; 构造方法源码分析： 123456789101112131415161718192021222324252627282930public HashMap(int initialCapacity, float loadFactor) &#123; // 判断设置的容量和负载因子合不合理 if (initialCapacity &lt; 0) throw new IllegalArgumentException("Illegal initial capacity: " + initialCapacity); if (initialCapacity &gt; MAXIMUM_CAPACITY) initialCapacity = MAXIMUM_CAPACITY; if (loadFactor &lt;= 0 || Float.isNaN(loadFactor)) throw new IllegalArgumentException("Illegal load factor: " + loadFactor); // 设置负载因子，临界值此时为容量大小，后面第一次put时由inflateTable(int toSize)方法计算设置 this.loadFactor = loadFactor; threshold = initialCapacity; init();&#125;public HashMap(int initialCapacity) &#123; this(initialCapacity, DEFAULT_LOAD_FACTOR);&#125;public HashMap() &#123; this(DEFAULT_INITIAL_CAPACITY, DEFAULT_LOAD_FACTOR);&#125;public HashMap(Map&lt;? extends K, ? extends V&gt; m) &#123; this(Math.max((int) (m.size() / DEFAULT_LOAD_FACTOR) + 1, DEFAULT_INITIAL_CAPACITY), DEFAULT_LOAD_FACTOR); inflateTable(threshold); putAllForCreate(m);&#125; put方法put()源码分析： 12345678910111213141516171819202122232425262728293031public V put(K key, V value) &#123; // 如果table引用指向成员变量EMPTY_TABLE，那么初始化HashMap（设置容量、临界值，新的Entry数组引用） if (table == EMPTY_TABLE) &#123; inflateTable(threshold); &#125; // 若“key为null”，则将该键值对添加到table[0]处，遍历该链表，如果有key为null，则将value替换。没有就创建新Entry对象放在链表表头 // 所以table[0]的位置上，永远最多存储1个Entry对象，形成不了链表。key为null的Entry存在这里 if (key == null) return putForNullKey(value); // 若“key不为null”，则计算该key的哈希值 int hash = hash(key); // 搜索指定hash值在对应table中的索引 int i = indexFor(hash, table.length); // 循环遍历table数组上的Entry对象，判断该位置上key是否已存在 for (Entry&lt;K,V&gt; e = table[i]; e != null; e = e.next) &#123; Object k; // 哈希值相同并且对象相同 if (e.hash == hash &amp;&amp; ((k = e.key) == key || key.equals(k))) &#123; // 如果这个key对应的键值对已经存在，就用新的value代替老的value，然后退出！ V oldValue = e.value; e.value = value; e.recordAccess(this); return oldValue; &#125; &#125; // 修改次数+1 modCount++; // table数组中没有key对应的键值对，就将key-value添加到table[i]处 addEntry(hash, key, value, i); return null; &#125; 可以看到，当我们给put()方法传递键和值时，HashMap会由key来调用hash()方法，返回键的hash值，计算Index后用于找到bucket（哈希桶）的位置来储存Entry对象。 如果两个对象key的hash值相同，那么它们的bucket位置也相同，但equals()不相同，添加元素时会发生hash碰撞，也叫hash冲突，HashMap使用链表来解决碰撞问题。 分析源码可知，put()时，HashMap会先遍历table数组，用hash值和equals()判断数组中是否存在完全相同的key对象， 如果这个key对象在table数组中已经存在，就用新的value代替老的value。如果不存在，就创建一个新的Entry对象添加到table[ i ]处。 如果该table[ i ]已经存在其他元素，那么新Entry对象将会储存在bucket链表的表头，通过next指向原有的Entry对象，形成链表结构（hash碰撞解决方案）。 Entry数据结构源码如下（HashMap内部类）： 12345678910111213141516171819static class Entry&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; final K key; V value; /** 指向下一个元素的引用 */ Entry&lt;K,V&gt; next; int hash; /** * 构造方法为Entry赋值 */ Entry(int h, K k, V v, Entry&lt;K,V&gt; n) &#123; value = v; next = n; key = k; hash = h; &#125; ... ...&#125; 形成单链表的核心代码如下： 12345678910111213141516171819202122/** * 将Entry添加到数组bucketIndex位置对应的哈希桶中，并判断数组是否需要扩容 */void addEntry(int hash, K key, V value, int bucketIndex) &#123; // 如果数组长度大于等于容量×负载因子，并且要添加的位置为null if ((size &gt;= threshold) &amp;&amp; (null != table[bucketIndex])) &#123; // 长度扩大为原数组的两倍，代码分析见下面扩容机制 resize(2 * table.length); hash = (null != key) ? hash(key) : 0; bucketIndex = indexFor(hash, table.length); &#125; createEntry(hash, key, value, bucketIndex);&#125;/** * 在链表中添加一个新的Entry对象在链表的表头 */void createEntry(int hash, K key, V value, int bucketIndex) &#123; Entry&lt;K,V&gt; e = table[bucketIndex]; table[bucketIndex] = new Entry&lt;&gt;(hash, key, value, e); size++;&#125; get方法如果两个不同的key的hashcode相同，两个值对象储存在同一个bucket位置，要获取value，我们调用get()方法，HashMap会使用key的hashcode找到bucket位置，因为HashMap在链表中存储的是Entry键值对，所以找到bucket位置之后，会调用key的equals()方法，按顺序遍历链表的每个 Entry，直到找到想获取的 Entry 为止——如果恰好要搜索的 Entry 位于该 Entry 链的最末端（该 Entry 是最早放入该 bucket 中），那HashMap必须循环到最后才能找到该元素。 get()方法源码如下： 1234567891011121314151617181920212223242526272829public V get(Object key) &#123; // 若key为null，遍历table[0]处的链表（实际上要么没有元素，要么只有一个Entry对象），取出key为null的value if (key == null) return getForNullKey(); // 若key不为null，用key获取Entry对象 Entry&lt;K,V&gt; entry = getEntry(key); // 若链表中找到的Entry不为null，返回该Entry中的value return null == entry ? null : entry.getValue();&#125;final Entry&lt;K,V&gt; getEntry(Object key) &#123; if (size == 0) &#123; return null; &#125; // 计算key的hash值 int hash = (key == null) ? 0 : hash(key); // 计算key在数组中对应位置，遍历该位置的链表 for (Entry&lt;K,V&gt; e = table[indexFor(hash, table.length)]; e != null; e = e.next) &#123; Object k; // 若key完全相同，返回链表中对应的Entry对象 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) return e; &#125; // 链表中没找到对应的key，返回null return null;&#125; 三、hash算法我们可以看到在HashMap中要找到某个元素，需要根据key的hash值来求得对应数组中的位置。如何计算这个位置就是hash算法。前面说过HashMap的数据结构是数组和链表的结合，所以我们当然希望这个HashMap里面的元素位置尽量的分布均匀些，尽量使得每个位置上的元素数量只有一个，那么当我们用hash算法求得这个位置的时候，马上就可以知道对应位置的元素就是我们要的，而不用再去遍历链表。 源码：1234567/** * Returns index for hash code h. */static int indexFor(int h, int length) &#123; // assert Integer.bitCount(length) == 1 : "length must be a non-zero power of 2"; return h &amp; (length-1);&#125; 四、性能问题HashMap有两个参数影响其性能：初始容量和负载因子。均可以通过构造方法指定大小。 容量capacity是HashMap中bucket哈希桶(Entry的链表)的数量，初始容量只是HashMap在创建时的容量，最大设置初始容量是2^30，默认初始容量是16（必须为2的幂），解释一下，当数组长度为2的n次幂的时候，不同的key通过indexFor()方法算得的数组位置相同的几率较小，那么数据在数组上分布就比较均匀，也就是说碰撞的几率小，相对的，get()的时候就不用遍历某个位置上的链表，这样查询效率也就较高了。 负载因子loadFactor是HashMap在其容量自动增加之前可以达到多满的一种尺度，默认值是0.75。 扩容机制：当HashMapde的长度超出了加载因子与当前容量的乘积（默认16*0.75=12）时，通过调用resize方法重新创建一个原来HashMap大小的两倍的newTable数组，最大扩容到2^30+1，并将原先table的元素全部移到newTable里面，重新计算hash，然后再重新根据hash分配位置。这个过程叫作rehash，因为它调用hash方法找到新的bucket位置。 扩容机制源码分析： 123456789101112131415161718void resize(int newCapacity) &#123; Entry[] oldTable = table; int oldCapacity = oldTable.length; // 如果之前的HashMap已经扩充打最大了，那么就将临界值threshold设置为最大的int值 if (oldCapacity == MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return; &#125; // 根据新传入的newCapacity创建新Entry数组 Entry[] newTable = new Entry[newCapacity]; // 用来将原先table的元素全部移到newTable里面，重新计算hash，然后再重新根据hash分配位置 transfer(newTable, initHashSeedAsNeeded(newCapacity)); // 再将newTable赋值给table table = newTable; // 重新计算临界值，扩容公式在这儿（newCapacity * loadFactor） threshold = (int)Math.min(newCapacity * loadFactor, MAXIMUM_CAPACITY + 1);&#125; 123456789101112131415void transfer(Entry[] newTable, boolean rehash) &#123; int newCapacity = newTable.length; for (Entry&lt;K,V&gt; e : table) &#123; while(null != e) &#123; Entry&lt;K,V&gt; next = e.next; if (rehash) &#123; e.hash = null == e.key ? 0 : hash(e.key); &#125; int i = indexFor(e.hash, newCapacity); e.next = newTable[i]; newTable[i] = e; e = next; &#125; &#125;&#125; 扩容问题：数组扩容之后，最消耗性能的点就出现了：原数组中的数据必须重新计算其在新数组中的位置，并放进去，这个操作是极其消耗性能的。所以如果我们已经预知HashMap中元素的个数，那么预设初始容量能够有效的提高HashMap的性能。 重新调整HashMap大小，当多线程的情况下可能产生条件竞争。因为如果两个线程都发现HashMap需要重新调整大小了，它们会同时试着调整大小。在调整大小的过程中，存储在链表中的元素的次序会反过来，因为移动到新的bucket位置的时候，HashMap并不会将元素放在链表的尾部，而是放在头部，这是为了避免尾部遍历(tail traversing)。如果条件竞争发生了，那么就死循环了。 五、线程安全HashMap是线程不安全的，在多线程情况下直接使用HashMap会出现一些莫名其妙不可预知的问题。在多线程下使用HashMap，有几种方案： A.在外部包装HashMap，实现同步机制 B.使用Map m = Collections.synchronizedMap(new HashMap(…));实现同步（官方参考方案，但不建议使用，使用迭代器遍历的时候修改映射结构容易出错） D.使用java.util.HashTable，效率最低（几乎被淘汰了） E.使用java.util.concurrent.ConcurrentHashMap，相对安全，效率高（建议使用） 注意一个小问题，HashMap所有集合类视图所返回迭代器都是快速失败的(fail-fast)，在迭代器创建之后，如果从结构上对映射进行修改，除非通过迭代器自身的 remove 或 add 方法，其他任何时间任何方式的修改，迭代器都将抛出 ConcurrentModificationException。。因此，面对并发的修改，迭代器很快就会完全失败。 六、关于JDK1.8的问题JDK1.8的HashMap源码实现和1.7是不一样的，有很大不同，其底层数据结构也不一样，引入了红黑树结构。有网友测试过，JDK1.8HashMap的性能要高于JDK1.7 15%以上，在某些size的区域上，甚至高于100%。随着size的变大，JDK1.7的花费时间是增长的趋势，而JDK1.8是明显的降低趋势，并且呈现对数增长稳定。当一个链表长度大于8的时候，HashMap会动态的将它替换成一个红黑树（JDK1.8引入红黑树大程度优化了HashMap的性能），这会将时间复杂度从O(n)降为O(logn)。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java类加载器]]></title>
    <url>%2Fblog%2F2015%2F01%2F07%2F2015-01-07-Java%E7%B1%BB%E5%8A%A0%E8%BD%BD%E5%99%A8%2F</url>
    <content type="text"><![CDATA[类加载器工作机制类加载器就是寻找类的节码文件并构造出类在JVM内部表示对象的组件。负责将.class文件加载到内存中，并为之生成对应的Class对象。在Java中，类加载器把一个类装入JVM中，要经过以下步骤： [1.] 加载：（loading）通过类的全限定名获取二进制字节流（并没有指明要从一个Class文件中获取，可以从其他渠道，譬如：网络、动态生成、数据库等），将二进制字节流转换成方法区中的运行时数据结构，在内存中创建一个代表这个类的java.lang.Class对象，作为方法区这个类的各种数据的访问入口。加载阶段和连接阶段（linking）的部分内容（如一部分字节码文件格式验证动作）是交叉进行的，加载阶段尚未完成，连接阶段可能已经开始，但这些夹在加载阶段之中进行的动作，仍然属于连接阶段的内容，这两个阶段的开始时间仍然保持着固定的先后顺序。 [2.] 连接：（linking）执行下面的验证、准备和解析步骤，其中解析步骤是可以选择的。 &gt; [2.1]验证：检查导入类或接口的二进制数据的正确性，（文件格式验证，元数据验证，字节码验证，符号引用验证）。这一阶段的目的是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身的安全。验证阶段是非常重要的，但不是必须的，它对程序运行期没有影响，如果所引用的类经过反复验证，那么可以考虑采用-Xverifynone参数来关闭大部分的类验证措施，以缩短虚拟机类加载的时间。 [2.2]准备：准备阶段是正式为类的静态成员分配内存并设置默认初始值的阶段，这些变量所使用的内存都将在方法区中进行分配。这时候进行内存分配的仅包括类静态成员变量，而不包括实例变量，实例变量将会在对象实例化时随着对象一起分配在堆中。其次，这里所说的初始值“通常情况”下是数据类型的零值。 [2.3]解析：解析阶段是虚拟机将常量池内的符号引用替换为直接引用的过程。解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行。 [3.] 初始化：（initializing）初始化阶段是类加载过程的最后一步，到了初始化阶段，才真正开始执行类中定义的java程序代码。在准备阶段，变量已经赋过一次系统要求的默认初始值，而在初始化阶段，则根据程序猿通过程序制定的主管计划去初始化类变量和其他资源。 JVM三种预定义类型类加载器类装载工作由ClassLoader及其子类负责，ClassLoader是一个重要的Java运行时系统组件，它负责在运行时查找和装入Class字节码文件。JVM在运行时会产生三个ClassLoader： Bootstrap ClassLoader（根加载器），也被称为引导类加载器，负责Java核心类或-Xbootclasspath选项指定的jar包加载到内存中，在JDK中JRE的lib目录下rt.jar文件中加载文件，比如System,String等。它不是由java实现的，而是由C++编写，并不继承自 java.lang.ClassLoader 所以无法通过程序获取根加载器。 Extension ClassLoader（扩展类加载器），负责JRE的扩展目录中jar包的加载。主要将JDK中JRE的lib/ext目录中的类库或者由系统变量-Djava.ext.dir指定位置中的类库加载到内存中。 System/Application ClassLoader（系统类加载器），也称为应用程序类加载器，负责将JVM启动时来自java命令的class文件，或classpass环境变量所指定的jar包和类路径加载到内存中。可以通过ClassLoader的静态方法getSystemLoader()来获取系统类加载器。 这三个类加载器之间存在父子层级关系，即Bootstrap ClassLoader是Extension ClassLoader的父加载器，Extension ClassLoader是System ClassLoader的父加载器。默认情况下，使用System ClassLoader装载应用程序的类，我们可以做一个实验： 12345678public class ClassLoaderTest &#123; public static void main(String[] args) &#123; ClassLoader loader = Thread.currentThread().getContextClassLoader(); System.out.println("current loader:"+loader); System.out.println("parent loader:"+loader.getParent()); System.out.println("grandparent loader:"+loader.getParent(). getParent()); &#125;&#125; 运行以上代码，在控制台上将打出以下信息： 1234current loader:sun.misc.Launcher$AppClassLoader@7b7035c6parent loader:sun.misc.Launcher$ExtClassLoader@3da997a// 根加载器在Java中访问不到，所以返回null grandparent loader:null 通过以上的输出信息，我们知道当前的ClassLoader是AppClassLoader(应用程序类加载器)，父ClassLoader是ExtClassLoader(扩展类加载器)，祖父ClassLoader是根类加载器，因为在Java中无法获得它的句柄，所以仅返回null。 全盘负责机制JVM装载类时使用“全盘负责委托机制”。“全盘负责”是指当一个ClassLoader装载一个类的时，除非显式地使用另一个ClassLoader，该类所依赖及引用的类也由这个ClassLoader载入。”委托机制”见下面的”双亲委派机制“。 双亲委派机制如果一个类加载器收到了一个类加载请求，它不会自己去尝试加载这个类，而是把这个请求转交给父类加载器去完成。每一个层次的类加载器都是如此。因此所有的类加载请求都应该传递到最顶层的根类加载器中，只有到父类加载器反馈自己无法完成这个加载请求（在它的搜索范围没有找到这个类）时，子类加载器才会尝试自己去加载。这一点是从安全角度考虑的，试想如果有人编写了一个恶意的基础类（如java.lang.String）并装载到JVM中将会引起多么可怕的后果。但是由于有了“双亲委派机制”，java.lang.String永远是由根加载器来装载的，这样就避免了上述事件的发生。 “双亲委派”机制只是Java推荐的机制，并不是强制的机制。我们可以继承java.lang.ClassLoader类，实现自己的类加载器。如果想保持双亲委派模型，就应该重写findClass(name)方法；如果想破坏双亲委派模型，可以重写loadClass(name)方法。 ClassLoader重要方法在Java中，ClassLoader是一个抽象类，位于java.lang包中。下面对该类的一些重要接口方法进行介绍： Class loadClass(String name)name参数指定类加载器需要装载类的名字，必须使用全限定类名，如com.baobaotao. beans.Car。该方法有一个重载方法loadClass(String name ,boolean resolve)，resolve参数告诉类加载器是否需要解析该类。在初始化类之前，应考虑进行类解析的工作，但并不是所有的类都需要解析，如果JVM只需要知道该类是否存在或找出该类的超类，那么就不需要进行解析。 Class defineClass(String name, byte[] b, int off, int len)将类文件的字节数组转换成JVM内部的java.lang.Class对象。字节数组可以从本地文件系统、远程网络获取。name为字节数组对应的全限定类名。 Class findSystemClass(String name)从本地文件系统载入Class文件，如果本地文件系统不存在该Class文件，将抛出ClassNotFoundException异常。该方法是JVM默认使用的装载机制。 Class findLoadedClass(String name)调用该方法来查看ClassLoader是否已装入某个类。如果已装入，那么返回java.lang.Class对象，否则返回null。如果强行装载已存在的类，将会抛出连接错误。 ClassLoader getParent()获取类加载器的父加载器，除根加载器外，所有的类加载器都有且仅有一个父加载器，ExtClassLoader的父加载器是根加载器，因为根加载器非Java编写，所以无法获得，将返回null。 除JVM默认的三个ClassLoader以外，可以编写自己的第三方类加载器，以实现一些特殊的需求。类文件被装载并解析后，在JVM内将拥有一个对应的java.lang.Class类描述对象，该类的实例都拥有指向这个类描述对象的引用，而类描述对象又拥有指向关联ClassLoader的引用，如图所示。 每一个类在JVM中都拥有一个对应的java.lang.Class对象，它提供了类结构信息的描述。数组、枚举、注解以及基本Java类型（如int、double等），甚至void都拥有对应的Class对象。Class没有public的构造方法。Class对象是在装载类时由JVM通过调用类加载器中的defineClass()方法自动构造的。 类的初始化类什么时候才被初始化 创建类的实例，也就是new一个对象 访问某个类或接口的静态变量，或者对该静态变量赋值 调用类的静态方法 反射（Class.forName(“com.dijia478.load”)） 初始化一个类的子类（会首先初始化子类的父类） JVM启动时标明的启动类，即用java.exe命令来运行文件名和类名相同的那个主类 只有这6中情况才会导致类的类的初始化。 类的初始化步骤： 如果这个类还没有被加载和连接，那先进行加载和连接 假如这个类存在直接父类，并且这个类还没有被初始化（注意：在一个类加载器中，类只能初始化一次），那就初始化直接的父类（不适用于接口） 加入类中存在初始化语句（如static变量和static块），那就依次执行这些初始化语句。]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>类加载器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[request和response中文乱码问题后台处理办法]]></title>
    <url>%2Fblog%2F2014%2F12%2F19%2F2014-12-19-request%E5%92%8Cresponse%E4%B8%AD%E6%96%87%E4%B9%B1%E7%A0%81%E9%97%AE%E9%A2%98%E5%90%8E%E5%8F%B0%E5%A4%84%E7%90%86%E5%8A%9E%E6%B3%95%2F</url>
    <content type="text"><![CDATA[request接收参数的中文乱码的处理：GET：方法一：使用String的构造方法： 1new String(request.getParameter("传过来的name").getBytes("ISO-8859-1"), "UTF-8"); 方法二：修改tomcat7的默认编码方式，server.xml中端口号那项添加配置： 1&lt;Connector connectionTimeout="50000" port="8080" protocol="HTTP/1.1" redirectPort="8443" URIEncoding="UTF-8"/&gt; POST：方法一：设置request的缓冲区的编码： 1request.setCharacterEncoding("UTF-8"); 方法二：使用spring的编码过滤器，在web.xml中添加： 12345678910111213141516&lt;filter&gt; &lt;filter-name&gt;CharacterEncoding&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;CharacterEncoding&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; response输出中文的乱码问题：字符流输出中文：方法一： 1234// 设置浏览器字符集编码. response.setHeader("Content-Type","text/html;charset=UTF-8");// 设置response的缓冲区的编码.response.setCharacterEncoding("UTF-8"); 方法二：建议使用： 1response.setContentType("text/html;charset=UTF-8"); 字节流输出中文：（实际中不会用）123456// 使用字节流的方式输出中文：ServletOutputStream outputStream = response.getOutputStream();// 设置浏览器默认打开的时候采用的字符集response.setHeader("Content-Type", "text/html;charset=UTF-8");// 设置中文转成字节数组字符集编码outputStream.write("中文".getBytes("UTF-8"));]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>中文乱码</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[服务器获取浏览器发送请求中的cookies，选取自己需要的cookie]]></title>
    <url>%2Fblog%2F2014%2F12%2F01%2F2014-12-01-%E6%9C%8D%E5%8A%A1%E5%99%A8%E8%8E%B7%E5%8F%96%E6%B5%8F%E8%A7%88%E5%99%A8%E5%8F%91%E9%80%81%E8%AF%B7%E6%B1%82%E4%B8%AD%E7%9A%84cookies%2C%E9%80%89%E5%8F%96%E8%87%AA%E5%B7%B1%E9%9C%80%E8%A6%81%E7%9A%84cookie%2F</url>
    <content type="text"><![CDATA[1234567891011String cookieName = “userID”; // 设置自己需要的cookie名Cookie cookies[] = request.getCookies(); // 获取请求中的所有cookieif (cookies!=null) &#123; for(int i=0;i&lt;cookies.length;i++) // 遍历 &#123; Cookie cookie = cookies[i]; if (cookieName.equals(cookie.getName())) doSomethingWith(cookie.getValue()); // 用找的cookie去做你需要它做的事 &#125; &#125;]]></content>
      <categories>
        <category>Java</category>
      </categories>
      <tags>
        <tag>cookie</tag>
      </tags>
  </entry>
</search>
